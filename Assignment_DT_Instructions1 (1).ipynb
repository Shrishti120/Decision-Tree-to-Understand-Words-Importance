{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKMeoHjWN-3k"
   },
   "source": [
    "# <b>Assignment : DT</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22626,
     "status": "ok",
     "timestamp": 1664903527633,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "tsLAttROXYEw",
    "outputId": "5d9481d8-7953-404f-b12d-157569ddf5c7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0vpJMx2pHYX"
   },
   "source": [
    "<font color='red'><b> Please check below video before attempting this assignment</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1581,
     "status": "ok",
     "timestamp": 1664903473513,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "IjWz-o-IN6G_"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math as m\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1664903481119,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "IrJVk4Chpzjp",
    "outputId": "0d79282c-616e-4ec7-c8f1-8aaf0de3b71f"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"https://www.youtube.com/embed/ZhLXULFjIjQ\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7fc197743790>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('ZhLXULFjIjQ', width=\"1000\",height=\"500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CjA-ZU-TqVK1"
   },
   "source": [
    "<font color='red'><b> TF-IDFW2V</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvBr2z6iqW9V"
   },
   "source": [
    "<b>Tfidf w2v (w1,w2..) = (tfidf(w1) * w2v(w1) + tfidf(w2) * w2v(w2) + …)  /    (tfidf(w1) + tfidf(w2) + …)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRAy5UzOvi_a"
   },
   "source": [
    "<b>(Optional) Please check course video on [AVgw2V and TF-IDFW2V ](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2916/avg-word2vec-tf-idf-weighted-word2vec/3/module-3-foundations-of-natural-language-processing-and-machine-learning)for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IB2uk5LwtBlO"
   },
   "source": [
    "<font color='blue'><b>Glove vectors </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j697XLZGtCnz"
   },
   "source": [
    "<b>In this assignment you will be working with glove vectors , please check  [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) and [this](https://en.wikipedia.org/wiki/GloVe_(machine_learning)) for more details.</b><br>\n",
    "\n",
    "Download glove vectors from this [link ](https://drive.google.com/file/d/1lDca_ge-GYO0iQ6_XDLWePQFMdAA2b8f/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3064,
     "status": "ok",
     "timestamp": 1664903633384,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "_ufHLoACuoHw"
   },
   "outputs": [],
   "source": [
    "#please use below code to load glove vectors \n",
    "import pickle\n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMurhntCAooj"
   },
   "source": [
    "or else , you can use below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1664903639320,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "8-KyNeiQAtLG",
    "outputId": "ead9d98a-097a-4a32-f350-279e220b472b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\\ndef loadGloveModel(gloveFile):\\n    print (\"Loading Glove Model\")\\n    f = open(gloveFile,\\'r\\', encoding=\"utf8\")\\n    model = {}\\n    for line in tqdm(f):\\n        splitLine = line.split()\\n        word = splitLine[0]\\n        embedding = np.array([float(val) for val in splitLine[1:]])\\n        model[word] = embedding\\n    print (\"Done.\",len(model),\" words loaded!\")\\n    return model\\nmodel = loadGloveModel(\\'glove.42B.300d.txt\\')\\n\\n# ============================\\nOutput:\\n    \\nLoading Glove Model\\n1917495it [06:32, 4879.69it/s]\\nDone. 1917495  words loaded!\\n\\n# ============================\\n\\nwords = []\\nfor i in preproced_texts:\\n    words.extend(i.split(\\' \\'))\\n\\nfor i in preproced_titles:\\n    words.extend(i.split(\\' \\'))\\nprint(\"all the words in the coupus\", len(words))\\nwords = set(words)\\nprint(\"the unique words in the coupus\", len(words))\\n\\ninter_words = set(model.keys()).intersection(words)\\nprint(\"The number of words that are present in both glove vectors and our coupus\",       len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\\n\\nwords_courpus = {}\\nwords_glove = set(model.keys())\\nfor i in words:\\n    if i in words_glove:\\n        words_courpus[i] = model[i]\\nprint(\"word 2 vec length\", len(words_courpus))\\n\\n\\n# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\\n\\nimport pickle\\nwith open(\\'glove_vectors\\', \\'wb\\') as f:\\n    pickle.dump(words_courpus, f)\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "model = loadGloveModel('glove.42B.300d.txt')\n",
    "\n",
    "# ============================\n",
    "Output:\n",
    "    \n",
    "Loading Glove Model\n",
    "1917495it [06:32, 4879.69it/s]\n",
    "Done. 1917495  words loaded!\n",
    "\n",
    "# ============================\n",
    "\n",
    "words = []\n",
    "for i in preproced_texts:\n",
    "    words.extend(i.split(' '))\n",
    "\n",
    "for i in preproced_titles:\n",
    "    words.extend(i.split(' '))\n",
    "print(\"all the words in the coupus\", len(words))\n",
    "words = set(words)\n",
    "print(\"the unique words in the coupus\", len(words))\n",
    "\n",
    "inter_words = set(model.keys()).intersection(words)\n",
    "print(\"The number of words that are present in both glove vectors and our coupus\", \\\n",
    "      len(inter_words),\"(\",np.round(len(inter_words)/len(words)*100,3),\"%)\")\n",
    "\n",
    "words_courpus = {}\n",
    "words_glove = set(model.keys())\n",
    "for i in words:\n",
    "    if i in words_glove:\n",
    "        words_courpus[i] = model[i]\n",
    "print(\"word 2 vec length\", len(words_courpus))\n",
    "\n",
    "\n",
    "# stronging variables into pickle files python: http://www.jessicayung.com/how-to-use-pickle-to-save-and-load-variables-in-python/\n",
    "\n",
    "import pickle\n",
    "with open('glove_vectors', 'wb') as f:\n",
    "    pickle.dump(words_courpus, f)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTJ7Et5hxpZS"
   },
   "source": [
    "# <font color='red'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACUkHex3N-3m"
   },
   "source": [
    "<ol>\n",
    "    <li><strong>Apply Decision Tree Classifier(DecisionTreeClassifier) on these feature sets</strong>\n",
    "        <ul>\n",
    "            <li><font color='red'>Set 1</font>: categorical, numerical features +  preprocessed_essay (TFIDF) + Sentiment scores(preprocessed_essay)</li>\n",
    "            <li><font color='red'>Set 2</font>: categorical, numerical features +  preprocessed_essay (TFIDF W2V) + Sentiment scores(preprocessed_essay)</li>        </ul>\n",
    "    </li>\n",
    "    <li><strong>The hyper paramter tuning (best `depth` in range [1, 3, 10, 30], and the best `min_samples_split` in range [5, 10, 100, 500])</strong>\n",
    "        <ul>\n",
    "    <li>Find the best hyper parameter which will give the maximum <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/receiver-operating-characteristic-curve-roc-curve-and-auc-1/'>AUC</a> value</li>\n",
    "    <li>find the best hyper paramter using k-fold cross validation(use gridsearch cv or randomsearch cv)/simple cross validation data(you can write your own for loops refer sample solution)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "    <strong>Representation of results</strong>\n",
    "        <ul>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/Gp2DQmh.jpg' width=500px> with X-axis as <strong>min_sample_split</strong>, Y-axis as <strong>max_depth</strong>, and Z-axis as <strong>AUC Score</strong> , we have given the notebook which explains how to plot this 3d plot, you can find it in the same drive <i>3d_scatter_plot.ipynb</i></li>\n",
    "            <p style=\"text-align:center;font-size:30px;color:red;\"><strong>or</strong></p> <br>\n",
    "    <li>You need to plot the performance of model both on train data and cross validation data for each hyper parameter, like shown in the figure\n",
    "    <img src='https://i.imgur.com/fgN9aUP.jpg' width=300px> <a href='https://seaborn.pydata.org/generated/seaborn.heatmap.html'>seaborn heat maps</a> with rows as <strong>min_sample_split</strong>, columns as <strong>max_depth</strong>, and values inside the cell representing <strong>AUC Score</strong> </li>\n",
    "    <li>You choose either of the plotting techniques out of 3d plot or heat map</li>\n",
    "    <li>Once after you found the best hyper parameter, you need to train your model with it, and find the AUC on test data and plot the ROC curve on both train and test.\n",
    "        Make sure that you are using predict_proba method to calculate AUC curves, because AUC is calcualted on class probabilities and not on class labels.\n",
    "    <img src='https://i.imgur.com/wMQDTFe.jpg' width=300px></li>\n",
    "    <li>Along with plotting ROC curve, you need to print the <a href='https://www.appliedaicourse.com/course/applied-ai-course-online/lessons/confusion-matrix-tpr-fpr-fnr-tnr-1/'>confusion matrix</a> with predicted and original labels of test data points\n",
    "    <img src='https://i.imgur.com/IdN5Ctv.png' width=300px></li>\n",
    "    <li>Once after you plot the confusion matrix with the test data, get all the `false positive data points`\n",
    "        <ul>\n",
    "            <li> Plot the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`</li>\n",
    "            <li> Plot the box plot with the `price` of these `false positive data points`</li>\n",
    "            <li> Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`</li>\n",
    "        </ul>\n",
    "        </ul>\n",
    "    </li>\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqWyfo1Sx8ua"
   },
   "source": [
    "# <font color='red'><b> Task - 2 </b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xddr3kChx-ew"
   },
   "source": [
    "For this task consider **set-1** features.\n",
    "\n",
    "*  Select all the features which are having non-zero feature importance.You can get the feature importance using  'feature_importances_` \n",
    "   (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n",
    "*  You need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3<br>\n",
    "  **Note**: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n",
    "  </li>\n",
    "    <br>\n",
    "You need to summarize the results at the end of the notebook, summarize it in the table format\n",
    "        <img src='http://i.imgur.com/YVpIGGE.jpg' width=400px>\n",
    "    </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZ-qDp6KxNj0"
   },
   "source": [
    "<font color='blue'><b>Hint for calculating Sentiment scores</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1664903665829,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "2IHTExN4xd5p",
    "outputId": "8e2a375d-146e-428e-87c7-e88b0c8fe37a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shrishti/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 425,
     "status": "ok",
     "timestamp": 1664903669877,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "YKZIvFYBxaaD",
    "outputId": "d971258f-030c-41ae-d76c-fd2e0ae8a668"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment score for sentence 1 {'neg': 0.0, 'neu': 0.213, 'pos': 0.787, 'compound': 0.5719}\n",
      "sentiment score for sentence 2 {'neg': 0.756, 'neu': 0.244, 'pos': 0.0, 'compound': -0.4767}\n",
      "sentiment score for sentence 3 {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/shrishti/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sample_sentence_1='I am happy.'\n",
    "ss_1 = sid.polarity_scores(sample_sentence_1)\n",
    "print('sentiment score for sentence 1',ss_1)\n",
    "\n",
    "sample_sentence_2='I am sad.'\n",
    "ss_2 = sid.polarity_scores(sample_sentence_2)\n",
    "print('sentiment score for sentence 2',ss_2)\n",
    "\n",
    "sample_sentence_3='I am going to New Delhi tommorow.'\n",
    "ss_3 = sid.polarity_scores(sample_sentence_3)\n",
    "print('sentiment score for sentence 3',ss_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6FUMj5TN-3y"
   },
   "source": [
    "<h1> Decision Tree </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luEzcFrGiqLa"
   },
   "source": [
    "# <font color='red'> <b>Task - 1</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQmid3VAN-31"
   },
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1664903696630,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "NqY4ES_3N-33"
   },
   "outputs": [],
   "source": [
    "#make sure you are loading atleast 50k datapoints\n",
    "#you can work with features of preprocessed_data.csv for the assignment.\n",
    "import pandas\n",
    "data = pandas.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1664903702648,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "b7L0jGftN6HO",
    "outputId": "7e14c8b1-8b88-4e12-de9a-e80e8cca20d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "\n",
       "    clean_categories                 clean_subcategories  \\\n",
       "0       math_science  appliedsciences health_lifescience   \n",
       "1       specialneeds                        specialneeds   \n",
       "2  literacy_language                            literacy   \n",
       "3    appliedlearning                    earlydevelopment   \n",
       "4  literacy_language                            literacy   \n",
       "\n",
       "                                               essay   price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...  725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...  213.03  \n",
       "2  having class 24 students comes diverse learner...  329.00  \n",
       "3  i recently read article giving students choice...  481.04  \n",
       "4  my students crave challenge eat obstacles brea...   17.74  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1664903707038,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "P7Z6bmHZN6HO"
   },
   "outputs": [],
   "source": [
    "sent = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664903709162,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ndKAXcAVN6HO"
   },
   "outputs": [],
   "source": [
    "# db = data['essay'][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664903709610,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "hUWgRbE3N6HP"
   },
   "outputs": [],
   "source": [
    "# polarity = [round(sent.polarity_scores(i)['compound'], 2) for i in db]\n",
    "# db['sentiment_score'] = polarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664903709611,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "gPjCRSa8N6HP",
    "outputId": "ff019ac9-1bad-40aa-ea32-bcc474470cdf",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science history_civics</td>\n",
       "      <td>mathematics socialsciences</td>\n",
       "      <td>it end school year routines run course student...</td>\n",
       "      <td>102.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language math_science</td>\n",
       "      <td>literacy mathematics</td>\n",
       "      <td>sitting still overrated it makes sense opera m...</td>\n",
       "      <td>1418.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>math_science history_civics</td>\n",
       "      <td>appliedsciences history_geography</td>\n",
       "      <td>it not enough read book write essay connect de...</td>\n",
       "      <td>495.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl literacy</td>\n",
       "      <td>never society rapidly changed technology invad...</td>\n",
       "      <td>299.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hi</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences mathematics</td>\n",
       "      <td>do remember first time saw star wars wall e ro...</td>\n",
       "      <td>479.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "5           ca            mrs             grades_3_5   \n",
       "6           ca            mrs             grades_3_5   \n",
       "7           ca             ms             grades_3_5   \n",
       "8           ca             ms          grades_prek_2   \n",
       "9           hi            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "5                                             6                    1   \n",
       "6                                             0                    1   \n",
       "7                                             0                    0   \n",
       "8                                           127                    1   \n",
       "9                                            41                    1   \n",
       "\n",
       "                 clean_categories                 clean_subcategories  \\\n",
       "0                    math_science  appliedsciences health_lifescience   \n",
       "1                    specialneeds                        specialneeds   \n",
       "2               literacy_language                            literacy   \n",
       "3                 appliedlearning                    earlydevelopment   \n",
       "4               literacy_language                            literacy   \n",
       "5     math_science history_civics          mathematics socialsciences   \n",
       "6  literacy_language math_science                literacy mathematics   \n",
       "7     math_science history_civics   appliedsciences history_geography   \n",
       "8               literacy_language                        esl literacy   \n",
       "9                    math_science         appliedsciences mathematics   \n",
       "\n",
       "                                               essay    price  \n",
       "0  i fortunate enough use fairy tale stem kits cl...   725.05  \n",
       "1  imagine 8 9 years old you third grade classroo...   213.03  \n",
       "2  having class 24 students comes diverse learner...   329.00  \n",
       "3  i recently read article giving students choice...   481.04  \n",
       "4  my students crave challenge eat obstacles brea...    17.74  \n",
       "5  it end school year routines run course student...   102.50  \n",
       "6  sitting still overrated it makes sense opera m...  1418.08  \n",
       "7  it not enough read book write essay connect de...   495.29  \n",
       "8  never society rapidly changed technology invad...   299.99  \n",
       "9  do remember first time saw star wars wall e ro...   479.94  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1664903711304,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "JCyendvPN6HQ",
    "outputId": "e9e71b6b-a69f-485b-84bf-492054b359bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i fortunate enough use fairy tale stem kits classroom well stem journals students really enjoyed i would love implement lakeshore stem kits classroom next school year provide excellent engaging stem lessons my students come variety backgrounds including language socioeconomic status many not lot experience science engineering kits give materials provide exciting opportunities students each month i try several science stem steam projects i would use kits robot help guide science instruction engaging meaningful ways i adapt kits current language arts pacing guide already teach material kits like tall tales paul bunyan johnny appleseed the following units taught next school year i implement kits magnets motion sink vs float robots i often get units not know if i teaching right way using right materials the kits give additional ideas strategies lessons prepare students science it challenging develop high quality science activities these kits give materials i need provide students science activities go along curriculum classroom although i things like magnets classroom i not know use effectively the kits provide right amount materials show use appropriate way'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = data['essay'][0]\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664903713343,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "pzueXXiiN6HQ",
    "outputId": "6e5b9bfc-2a1c-4fdc-a9ff-4e71715b3fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment score for sentence 1 {'neg': 0.013, 'neu': 0.783, 'pos': 0.205, 'compound': 0.9867}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "sample_sentence_1= db\n",
    "ss_1 = sid.polarity_scores(sample_sentence_1)\n",
    "print('sentiment score for sentence 1',ss_1)\n",
    "# sample_sentence_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 505,
     "status": "ok",
     "timestamp": 1664903745398,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "cT7V9hz7iqLm"
   },
   "outputs": [],
   "source": [
    "# write your code in following steps for task 1\n",
    "# 1. calculate sentiment scores for the essay feature \n",
    "# 2. Split your data.\n",
    "# 3. perform tfidf vectorization of text data.\n",
    "# 4. perform tfidf w2v vectorization of text data.\n",
    "# 5. perform encoding of categorical features.\n",
    "# 6. perform encoding of numerical features\n",
    "# 7. For task 1 set 1 stack up all the features\n",
    "# 8. For task 1 set 2 stack up all the features (for stacking dense features you can use np.stack)\n",
    "# 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n",
    "# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n",
    "# 11. Plot confusion matrix based on best threshold value\n",
    "# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects.\n",
    "# 13. Write your observations about the wordcloud and pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664903745971,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "mHvBlI_9N-4X"
   },
   "outputs": [],
   "source": [
    "# please write all the code with proper documentation, and proper titles for each subsection\n",
    "# go through documentations and blogs before you start coding\n",
    "# first figure out what to do, and then think about how to do.\n",
    "# reading and understanding error messages will be very much helpfull in debugging your code\n",
    "# when you plot any graph make sure you use \n",
    "    # a. Title, that describes your plot, this will be very helpful to the reader\n",
    "    # b. Legends if needed\n",
    "    # c. X-axis label\n",
    "    # d. Y-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKKIf4WaN6HS"
   },
   "source": [
    "## 1. Calculate Sentiment Score for eassy feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwJRxRoSN6HS"
   },
   "source": [
    "### When it comes to Sentiment Score , we know that number of positive and negitive words in each document will be counted to determine the document's sentiment score. When it comes to our data we know that we have four kind of sentences positive , negitive , compound and neutral . Lets see how to calculate the sentiment score for one sentence first then we will see how we can implement it in our eassy of data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1664903747274,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "GvyZ7nywN6HS",
    "outputId": "27370995-9be6-4d0a-8c06-44a0d2ace185"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i fortunate enough use fairy tale stem kits classroom well stem journals students really enjoyed i would love implement lakeshore stem kits classroom next school year provide excellent engaging stem lessons my students come variety backgrounds including language socioeconomic status many not lot experience science engineering kits give materials provide exciting opportunities students each month i try several science stem steam projects i would use kits robot help guide science instruction engaging meaningful ways i adapt kits current language arts pacing guide already teach material kits like tall tales paul bunyan johnny appleseed the following units taught next school year i implement kits magnets motion sink vs float robots i often get units not know if i teaching right way using right materials the kits give additional ideas strategies lessons prepare students science it challenging develop high quality science activities these kits give materials i need provide students science activities go along curriculum classroom although i things like magnets classroom i not know use effectively the kits provide right amount materials show use appropriate way'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = data['essay'][0]\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1664903749512,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "qCT9iOq7iqLu",
    "outputId": "f09d0bad-8f37-4a2e-8d69-d057c7758bda",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.013, 'neu': 0.783, 'pos': 0.205, 'compound': 0.9867}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "db \n",
    "ss = sid.polarity_scores(db)\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeSKca6viqLw"
   },
   "source": [
    "## So we can see when we take sentence into account we get the score as negitive = 0.013 , neutral = 0.783 , positive = 0.205 , neutral = 0.9867 this means our sentence has 98% of compound words in sentence , 20.5 % of positive words , 78.3 % of neutral words and a very small amount of negitive words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gIoFdWQN6HV"
   },
   "source": [
    "## Lets see how this works on whole data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84kBcP-jO74N"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1664903753854,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "LH-9iLDqN6HV",
    "outputId": "c560c76a-69c5-40a5-f56e-cfa408a6d50f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i fortunate enough use fairy tale stem kits classroom well stem journals students really enjoyed i would love implement lakeshore stem kits classroom next school year provide excellent engaging stem lessons my students come variety backgrounds including language socioeconomic status many not lot experience science engineering kits give materials provide exciting opportunities students each month i try several science stem steam projects i would use kits robot help guide science instruction engaging meaningful ways i adapt kits current language arts pacing guide already teach material kits like tall tales paul bunyan johnny appleseed the following units taught next school year i implement kits magnets motion sink vs float robots i often get units not know if i teaching right way using right materials the kits give additional ideas strategies lessons prepare students science it challenging develop high quality science activities these kits give materials i need provide students science activities go along curriculum classroom although i things like magnets classroom i not know use effectively the kits provide right amount materials show use appropriate way'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## to get the sentiment score of whole data as \n",
    "db = data['essay'][0]\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1664903755961,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "RjqNppiMN6HW",
    "outputId": "f0248747-b3fe-4aa8-bee3-1937eac75242"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.013, 'neu': 0.783, 'pos': 0.205, 'compound': 0.9867}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "db \n",
    "ss = sid.polarity_scores(db)\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQ4lwGW6N6HX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 109248/109248 [06:26<00:00, 282.43it/s]\n"
     ]
    }
   ],
   "source": [
    "## defining the analyzer \n",
    "import nltk \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "## for whole data eassy we will be creating different list of words wihere we can store negitive , positive , compound and neutral wprds\n",
    "negitive_words = []\n",
    "positive_words = []\n",
    "neutral_words = []\n",
    "compound_words = []\n",
    "\n",
    "\n",
    "## Next we will get the sentiment score of each kind of word differently and append it to our list \n",
    "for words in tqdm(data['essay']):\n",
    "    negitive_words.append(sid.polarity_scores(words)['neg'])\n",
    "    positive_words.append(sid.polarity_scores(words)['pos'])\n",
    "    neutral_words.append(sid.polarity_scores(words)['neu'])\n",
    "    compound_words.append(sid.polarity_scores(words)['compound'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98,
     "status": "ok",
     "timestamp": 1664002899974,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "fRNpbY1hN6HX",
    "outputId": "d33402d0-aff6-4bdb-daa0-586c83eca657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013,\n",
       " 0.072,\n",
       " 0.017,\n",
       " 0.03,\n",
       " 0.029,\n",
       " 0.013,\n",
       " 0.019,\n",
       " 0.067,\n",
       " 0.063,\n",
       " 0.057,\n",
       " 0.012,\n",
       " 0.034,\n",
       " 0.013,\n",
       " 0.041,\n",
       " 0.018,\n",
       " 0.021,\n",
       " 0.007,\n",
       " 0.079,\n",
       " 0.027,\n",
       " 0.044,\n",
       " 0.0,\n",
       " 0.02,\n",
       " 0.064,\n",
       " 0.032,\n",
       " 0.011,\n",
       " 0.037,\n",
       " 0.027,\n",
       " 0.09,\n",
       " 0.0,\n",
       " 0.13,\n",
       " 0.012,\n",
       " 0.065,\n",
       " 0.026,\n",
       " 0.013,\n",
       " 0.034,\n",
       " 0.007,\n",
       " 0.0,\n",
       " 0.042,\n",
       " 0.042,\n",
       " 0.035,\n",
       " 0.091,\n",
       " 0.029,\n",
       " 0.0,\n",
       " 0.051,\n",
       " 0.0,\n",
       " 0.019,\n",
       " 0.073,\n",
       " 0.024,\n",
       " 0.085,\n",
       " 0.01,\n",
       " 0.062,\n",
       " 0.011,\n",
       " 0.024,\n",
       " 0.042,\n",
       " 0.042,\n",
       " 0.025,\n",
       " 0.046,\n",
       " 0.08,\n",
       " 0.05,\n",
       " 0.058,\n",
       " 0.015,\n",
       " 0.081,\n",
       " 0.015,\n",
       " 0.036,\n",
       " 0.016,\n",
       " 0.035,\n",
       " 0.064,\n",
       " 0.026,\n",
       " 0.021,\n",
       " 0.064,\n",
       " 0.0,\n",
       " 0.013,\n",
       " 0.024,\n",
       " 0.085,\n",
       " 0.0,\n",
       " 0.03,\n",
       " 0.038,\n",
       " 0.059,\n",
       " 0.012,\n",
       " 0.093,\n",
       " 0.015,\n",
       " 0.033,\n",
       " 0.008,\n",
       " 0.094,\n",
       " 0.058,\n",
       " 0.011,\n",
       " 0.009,\n",
       " 0.038,\n",
       " 0.027,\n",
       " 0.061,\n",
       " 0.008,\n",
       " 0.032,\n",
       " 0.062,\n",
       " 0.047,\n",
       " 0.036,\n",
       " 0.063,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.036,\n",
       " 0.033,\n",
       " 0.0,\n",
       " 0.033,\n",
       " 0.0,\n",
       " 0.063,\n",
       " 0.035,\n",
       " 0.04,\n",
       " 0.079,\n",
       " 0.02,\n",
       " 0.053,\n",
       " 0.038,\n",
       " 0.028,\n",
       " 0.023,\n",
       " 0.094,\n",
       " 0.048,\n",
       " 0.012,\n",
       " 0.034,\n",
       " 0.073,\n",
       " 0.02,\n",
       " 0.088,\n",
       " 0.057,\n",
       " 0.018,\n",
       " 0.039,\n",
       " 0.044,\n",
       " 0.082,\n",
       " 0.129,\n",
       " 0.0,\n",
       " 0.023,\n",
       " 0.02,\n",
       " 0.044,\n",
       " 0.054,\n",
       " 0.014,\n",
       " 0.067,\n",
       " 0.019,\n",
       " 0.056,\n",
       " 0.011,\n",
       " 0.027,\n",
       " 0.011,\n",
       " 0.0,\n",
       " 0.036,\n",
       " 0.043,\n",
       " 0.0,\n",
       " 0.033,\n",
       " 0.042,\n",
       " 0.018,\n",
       " 0.028,\n",
       " 0.08,\n",
       " 0.075,\n",
       " 0.014,\n",
       " 0.07,\n",
       " 0.07,\n",
       " 0.092,\n",
       " 0.008,\n",
       " 0.038,\n",
       " 0.05,\n",
       " 0.125,\n",
       " 0.023,\n",
       " 0.04,\n",
       " 0.0,\n",
       " 0.023,\n",
       " 0.008,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.048,\n",
       " 0.093,\n",
       " 0.14,\n",
       " 0.02,\n",
       " 0.046,\n",
       " 0.043,\n",
       " 0.029,\n",
       " 0.03,\n",
       " 0.032,\n",
       " 0.062,\n",
       " 0.048,\n",
       " 0.078,\n",
       " 0.077,\n",
       " 0.012,\n",
       " 0.027,\n",
       " 0.026,\n",
       " 0.058,\n",
       " 0.109,\n",
       " 0.064,\n",
       " 0.029,\n",
       " 0.021,\n",
       " 0.0,\n",
       " 0.098,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.032,\n",
       " 0.016,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.028,\n",
       " 0.04,\n",
       " 0.069,\n",
       " 0.051,\n",
       " 0.151,\n",
       " 0.076,\n",
       " 0.018,\n",
       " 0.019,\n",
       " 0.101,\n",
       " 0.019,\n",
       " 0.072,\n",
       " 0.057,\n",
       " 0.029,\n",
       " 0.046,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.038,\n",
       " 0.037,\n",
       " 0.015,\n",
       " 0.047,\n",
       " 0.041,\n",
       " 0.051,\n",
       " 0.017,\n",
       " 0.039,\n",
       " 0.031,\n",
       " 0.024,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.072,\n",
       " 0.08,\n",
       " 0.0,\n",
       " 0.121,\n",
       " 0.019,\n",
       " 0.02,\n",
       " 0.0,\n",
       " 0.063,\n",
       " 0.021,\n",
       " 0.04,\n",
       " 0.098,\n",
       " 0.022,\n",
       " 0.0,\n",
       " 0.124,\n",
       " 0.038,\n",
       " 0.011,\n",
       " 0.018,\n",
       " 0.014,\n",
       " 0.073,\n",
       " 0.095,\n",
       " 0.092,\n",
       " 0.012,\n",
       " 0.007,\n",
       " 0.064,\n",
       " 0.042,\n",
       " 0.012,\n",
       " 0.02,\n",
       " 0.013,\n",
       " 0.05,\n",
       " 0.078,\n",
       " 0.1,\n",
       " 0.048,\n",
       " 0.028,\n",
       " 0.059,\n",
       " 0.021,\n",
       " 0.055,\n",
       " 0.051,\n",
       " 0.101,\n",
       " 0.069,\n",
       " 0.128,\n",
       " 0.015,\n",
       " 0.014,\n",
       " 0.011,\n",
       " 0.061,\n",
       " 0.02,\n",
       " 0.012,\n",
       " 0.118,\n",
       " 0.031,\n",
       " 0.07,\n",
       " 0.024,\n",
       " 0.05,\n",
       " 0.057,\n",
       " 0.011,\n",
       " 0.009,\n",
       " 0.032,\n",
       " 0.062,\n",
       " 0.022,\n",
       " 0.108,\n",
       " 0.023,\n",
       " 0.033,\n",
       " 0.024,\n",
       " 0.028,\n",
       " 0.051,\n",
       " 0.034,\n",
       " 0.007,\n",
       " 0.034,\n",
       " 0.053,\n",
       " 0.0,\n",
       " 0.119,\n",
       " 0.058,\n",
       " 0.019,\n",
       " 0.064,\n",
       " 0.046,\n",
       " 0.055,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.109,\n",
       " 0.044,\n",
       " 0.04,\n",
       " 0.03,\n",
       " 0.063,\n",
       " 0.02,\n",
       " 0.042,\n",
       " 0.041,\n",
       " 0.121,\n",
       " 0.02,\n",
       " 0.014,\n",
       " 0.091,\n",
       " 0.051,\n",
       " 0.044,\n",
       " 0.031,\n",
       " 0.01,\n",
       " 0.014,\n",
       " 0.0,\n",
       " 0.068,\n",
       " 0.072,\n",
       " 0.014,\n",
       " 0.05,\n",
       " 0.049,\n",
       " 0.036,\n",
       " 0.06,\n",
       " 0.018,\n",
       " 0.101,\n",
       " 0.041,\n",
       " 0.116,\n",
       " 0.027,\n",
       " 0.077,\n",
       " 0.023,\n",
       " 0.019,\n",
       " 0.047,\n",
       " 0.008,\n",
       " 0.122,\n",
       " 0.04,\n",
       " 0.07,\n",
       " 0.048,\n",
       " 0.04,\n",
       " 0.143,\n",
       " 0.012,\n",
       " 0.118,\n",
       " 0.175,\n",
       " 0.0,\n",
       " 0.012,\n",
       " 0.028,\n",
       " 0.027,\n",
       " 0.01,\n",
       " 0.054,\n",
       " 0.036,\n",
       " 0.037,\n",
       " 0.018,\n",
       " 0.044,\n",
       " 0.045,\n",
       " 0.0,\n",
       " 0.018,\n",
       " 0.053,\n",
       " 0.082,\n",
       " 0.042,\n",
       " 0.156,\n",
       " 0.102,\n",
       " 0.064,\n",
       " 0.067,\n",
       " 0.068,\n",
       " 0.028,\n",
       " 0.068,\n",
       " 0.059,\n",
       " 0.032,\n",
       " 0.063,\n",
       " 0.094,\n",
       " 0.042,\n",
       " 0.018,\n",
       " 0.181,\n",
       " 0.053,\n",
       " 0.052,\n",
       " 0.0,\n",
       " 0.023,\n",
       " 0.009,\n",
       " 0.133,\n",
       " 0.026,\n",
       " 0.015,\n",
       " 0.028,\n",
       " 0.0,\n",
       " 0.019,\n",
       " 0.017,\n",
       " 0.035,\n",
       " 0.067,\n",
       " 0.013,\n",
       " 0.054,\n",
       " 0.026,\n",
       " 0.022,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.022,\n",
       " 0.093,\n",
       " 0.024,\n",
       " 0.045,\n",
       " 0.01,\n",
       " 0.047,\n",
       " 0.024,\n",
       " 0.019,\n",
       " 0.048,\n",
       " 0.015,\n",
       " 0.014,\n",
       " 0.068,\n",
       " 0.023,\n",
       " 0.025,\n",
       " 0.056,\n",
       " 0.014,\n",
       " 0.058,\n",
       " 0.039,\n",
       " 0.027,\n",
       " 0.044,\n",
       " 0.034,\n",
       " 0.042,\n",
       " 0.056,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.032,\n",
       " 0.019,\n",
       " 0.022,\n",
       " 0.0,\n",
       " 0.097,\n",
       " 0.09,\n",
       " 0.049,\n",
       " 0.034,\n",
       " 0.04,\n",
       " 0.032,\n",
       " 0.0,\n",
       " 0.018,\n",
       " 0.044,\n",
       " 0.034,\n",
       " 0.031,\n",
       " 0.04,\n",
       " 0.038,\n",
       " 0.048,\n",
       " 0.037,\n",
       " 0.029,\n",
       " 0.057,\n",
       " 0.017,\n",
       " 0.0,\n",
       " 0.055,\n",
       " 0.095,\n",
       " 0.071,\n",
       " 0.126,\n",
       " 0.05,\n",
       " 0.0,\n",
       " 0.027,\n",
       " 0.01,\n",
       " 0.009,\n",
       " 0.017,\n",
       " 0.055,\n",
       " 0.069,\n",
       " 0.038,\n",
       " 0.084,\n",
       " 0.011,\n",
       " 0.062,\n",
       " 0.055,\n",
       " 0.017,\n",
       " 0.038,\n",
       " 0.021,\n",
       " 0.014,\n",
       " 0.065,\n",
       " 0.056,\n",
       " 0.05,\n",
       " 0.033,\n",
       " 0.06,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.042,\n",
       " 0.016,\n",
       " 0.011,\n",
       " 0.019,\n",
       " 0.023,\n",
       " 0.048,\n",
       " 0.023,\n",
       " 0.031,\n",
       " 0.076,\n",
       " 0.044,\n",
       " 0.041,\n",
       " 0.021,\n",
       " 0.025,\n",
       " 0.064,\n",
       " 0.024,\n",
       " 0.115,\n",
       " 0.064,\n",
       " 0.035,\n",
       " 0.009,\n",
       " 0.063,\n",
       " 0.066,\n",
       " 0.057,\n",
       " 0.097,\n",
       " 0.028,\n",
       " 0.016,\n",
       " 0.047,\n",
       " 0.086,\n",
       " 0.039,\n",
       " 0.033,\n",
       " 0.031,\n",
       " 0.056,\n",
       " 0.012,\n",
       " 0.047,\n",
       " 0.121,\n",
       " 0.078,\n",
       " 0.093,\n",
       " 0.03,\n",
       " 0.053,\n",
       " 0.026,\n",
       " 0.024,\n",
       " 0.053,\n",
       " 0.077,\n",
       " 0.06,\n",
       " 0.039,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.034,\n",
       " 0.0,\n",
       " 0.041,\n",
       " 0.067,\n",
       " 0.065,\n",
       " 0.075,\n",
       " 0.037,\n",
       " 0.027,\n",
       " 0.095,\n",
       " 0.029,\n",
       " 0.0,\n",
       " 0.026,\n",
       " 0.0,\n",
       " 0.014,\n",
       " 0.063,\n",
       " 0.043,\n",
       " 0.079,\n",
       " 0.026,\n",
       " 0.054,\n",
       " 0.05,\n",
       " 0.056,\n",
       " 0.072,\n",
       " 0.057,\n",
       " 0.095,\n",
       " 0.0,\n",
       " 0.076,\n",
       " 0.0,\n",
       " 0.053,\n",
       " 0.043,\n",
       " 0.055,\n",
       " 0.069,\n",
       " 0.048,\n",
       " 0.027,\n",
       " 0.065,\n",
       " 0.031,\n",
       " 0.086,\n",
       " 0.13,\n",
       " 0.08,\n",
       " 0.058,\n",
       " 0.017,\n",
       " 0.018,\n",
       " 0.023,\n",
       " 0.038,\n",
       " 0.089,\n",
       " 0.042,\n",
       " 0.062,\n",
       " 0.011,\n",
       " 0.041,\n",
       " 0.107,\n",
       " 0.039,\n",
       " 0.151,\n",
       " 0.048,\n",
       " 0.051,\n",
       " 0.062,\n",
       " 0.036,\n",
       " 0.014,\n",
       " 0.065,\n",
       " 0.016,\n",
       " 0.064,\n",
       " 0.0,\n",
       " 0.024,\n",
       " 0.168,\n",
       " 0.084,\n",
       " 0.0,\n",
       " 0.082,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.024,\n",
       " 0.047,\n",
       " 0.0,\n",
       " 0.07,\n",
       " 0.108,\n",
       " 0.044,\n",
       " 0.059,\n",
       " 0.0,\n",
       " 0.096,\n",
       " 0.029,\n",
       " 0.015,\n",
       " 0.07,\n",
       " 0.088,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.027,\n",
       " 0.014,\n",
       " 0.076,\n",
       " 0.101,\n",
       " 0.129,\n",
       " 0.11,\n",
       " 0.065,\n",
       " 0.049,\n",
       " 0.055,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.047,\n",
       " 0.0,\n",
       " 0.062,\n",
       " 0.007,\n",
       " 0.024,\n",
       " 0.067,\n",
       " 0.079,\n",
       " 0.095,\n",
       " 0.017,\n",
       " 0.054,\n",
       " 0.016,\n",
       " 0.032,\n",
       " 0.061,\n",
       " 0.021,\n",
       " 0.041,\n",
       " 0.03,\n",
       " 0.033,\n",
       " 0.102,\n",
       " 0.051,\n",
       " 0.046,\n",
       " 0.105,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.039,\n",
       " 0.022,\n",
       " 0.034,\n",
       " 0.0,\n",
       " 0.09,\n",
       " 0.032,\n",
       " 0.016,\n",
       " 0.028,\n",
       " 0.0,\n",
       " 0.023,\n",
       " 0.007,\n",
       " 0.052,\n",
       " 0.017,\n",
       " 0.03,\n",
       " 0.053,\n",
       " 0.0,\n",
       " 0.022,\n",
       " 0.02,\n",
       " 0.046,\n",
       " 0.031,\n",
       " 0.01,\n",
       " 0.021,\n",
       " 0.033,\n",
       " 0.117,\n",
       " 0.0,\n",
       " 0.078,\n",
       " 0.083,\n",
       " 0.0,\n",
       " 0.073,\n",
       " 0.072,\n",
       " 0.056,\n",
       " 0.02,\n",
       " 0.036,\n",
       " 0.113,\n",
       " 0.094,\n",
       " 0.11,\n",
       " 0.054,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.077,\n",
       " 0.01,\n",
       " 0.075,\n",
       " 0.0,\n",
       " 0.049,\n",
       " 0.059,\n",
       " 0.03,\n",
       " 0.016,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.046,\n",
       " 0.0,\n",
       " 0.121,\n",
       " 0.0,\n",
       " 0.115,\n",
       " 0.013,\n",
       " 0.031,\n",
       " 0.009,\n",
       " 0.023,\n",
       " 0.028,\n",
       " 0.026,\n",
       " 0.0,\n",
       " 0.031,\n",
       " 0.057,\n",
       " 0.0,\n",
       " 0.028,\n",
       " 0.079,\n",
       " 0.048,\n",
       " 0.06,\n",
       " 0.027,\n",
       " 0.0,\n",
       " 0.039,\n",
       " 0.025,\n",
       " 0.0,\n",
       " 0.068,\n",
       " 0.068,\n",
       " 0.037,\n",
       " 0.017,\n",
       " 0.018,\n",
       " 0.075,\n",
       " 0.014,\n",
       " 0.011,\n",
       " 0.047,\n",
       " 0.048,\n",
       " 0.053,\n",
       " 0.025,\n",
       " 0.026,\n",
       " 0.031,\n",
       " 0.027,\n",
       " 0.044,\n",
       " 0.027,\n",
       " 0.031,\n",
       " 0.021,\n",
       " 0.021,\n",
       " 0.06,\n",
       " 0.045,\n",
       " 0.021,\n",
       " 0.028,\n",
       " 0.009,\n",
       " 0.031,\n",
       " 0.035,\n",
       " 0.027,\n",
       " 0.026,\n",
       " 0.08,\n",
       " 0.027,\n",
       " 0.0,\n",
       " 0.076,\n",
       " 0.034,\n",
       " 0.042,\n",
       " 0.024,\n",
       " 0.01,\n",
       " 0.048,\n",
       " 0.039,\n",
       " 0.022,\n",
       " 0.042,\n",
       " 0.019,\n",
       " 0.095,\n",
       " 0.048,\n",
       " 0.088,\n",
       " 0.045,\n",
       " 0.068,\n",
       " 0.046,\n",
       " 0.042,\n",
       " 0.032,\n",
       " 0.056,\n",
       " 0.024,\n",
       " 0.04,\n",
       " 0.051,\n",
       " 0.038,\n",
       " 0.052,\n",
       " 0.056,\n",
       " 0.028,\n",
       " 0.009,\n",
       " 0.03,\n",
       " 0.009,\n",
       " 0.0,\n",
       " 0.092,\n",
       " 0.087,\n",
       " 0.067,\n",
       " 0.094,\n",
       " 0.038,\n",
       " 0.017,\n",
       " 0.037,\n",
       " 0.068,\n",
       " 0.034,\n",
       " 0.082,\n",
       " 0.034,\n",
       " 0.021,\n",
       " 0.01,\n",
       " 0.011,\n",
       " 0.026,\n",
       " 0.189,\n",
       " 0.013,\n",
       " 0.039,\n",
       " 0.021,\n",
       " 0.147,\n",
       " 0.053,\n",
       " 0.049,\n",
       " 0.0,\n",
       " 0.071,\n",
       " 0.035,\n",
       " 0.0,\n",
       " 0.044,\n",
       " 0.03,\n",
       " 0.018,\n",
       " 0.101,\n",
       " 0.0,\n",
       " 0.021,\n",
       " 0.06,\n",
       " 0.037,\n",
       " 0.055,\n",
       " 0.027,\n",
       " 0.016,\n",
       " 0.0,\n",
       " 0.049,\n",
       " 0.03,\n",
       " 0.0,\n",
       " 0.013,\n",
       " 0.0,\n",
       " 0.074,\n",
       " 0.088,\n",
       " 0.029,\n",
       " 0.01,\n",
       " 0.087,\n",
       " 0.012,\n",
       " 0.098,\n",
       " 0.0,\n",
       " 0.022,\n",
       " 0.0,\n",
       " 0.083,\n",
       " 0.0,\n",
       " 0.053,\n",
       " 0.04,\n",
       " 0.025,\n",
       " 0.014,\n",
       " 0.031,\n",
       " 0.143,\n",
       " 0.082,\n",
       " 0.059,\n",
       " 0.039,\n",
       " 0.028,\n",
       " 0.044,\n",
       " 0.017,\n",
       " 0.056,\n",
       " 0.015,\n",
       " 0.049,\n",
       " 0.037,\n",
       " 0.096,\n",
       " 0.033,\n",
       " 0.0,\n",
       " 0.058,\n",
       " 0.056,\n",
       " 0.066,\n",
       " 0.02,\n",
       " 0.033,\n",
       " 0.0,\n",
       " 0.018,\n",
       " 0.055,\n",
       " 0.039,\n",
       " 0.015,\n",
       " 0.011,\n",
       " 0.052,\n",
       " 0.04,\n",
       " 0.063,\n",
       " 0.027,\n",
       " 0.044,\n",
       " 0.03,\n",
       " 0.044,\n",
       " 0.049,\n",
       " 0.031,\n",
       " 0.012,\n",
       " 0.059,\n",
       " 0.0,\n",
       " 0.04,\n",
       " 0.067,\n",
       " 0.022,\n",
       " 0.019,\n",
       " 0.061,\n",
       " 0.063,\n",
       " 0.063,\n",
       " 0.007,\n",
       " 0.028,\n",
       " 0.058,\n",
       " 0.045,\n",
       " 0.046,\n",
       " 0.056,\n",
       " 0.05,\n",
       " 0.032,\n",
       " 0.02,\n",
       " 0.078,\n",
       " 0.044,\n",
       " 0.023,\n",
       " 0.013,\n",
       " 0.025,\n",
       " 0.076,\n",
       " 0.077,\n",
       " 0.079,\n",
       " 0.08,\n",
       " 0.029,\n",
       " 0.012,\n",
       " 0.025,\n",
       " 0.033,\n",
       " 0.033,\n",
       " 0.02,\n",
       " 0.011,\n",
       " 0.027,\n",
       " 0.012,\n",
       " 0.059,\n",
       " 0.073,\n",
       " 0.011,\n",
       " 0.06,\n",
       " 0.054,\n",
       " 0.032,\n",
       " 0.058,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.038,\n",
       " 0.087,\n",
       " 0.046,\n",
       " 0.033,\n",
       " 0.046,\n",
       " 0.027,\n",
       " 0.093,\n",
       " 0.047,\n",
       " 0.042,\n",
       " 0.041,\n",
       " 0.043,\n",
       " 0.017,\n",
       " 0.017,\n",
       " 0.115,\n",
       " 0.064,\n",
       " 0.073,\n",
       " 0.026,\n",
       " 0.031,\n",
       " 0.048,\n",
       " 0.08,\n",
       " 0.014,\n",
       " 0.042,\n",
       " 0.088,\n",
       " 0.034,\n",
       " 0.065,\n",
       " 0.02,\n",
       " 0.058,\n",
       " 0.068,\n",
       " 0.034,\n",
       " 0.015,\n",
       " 0.03,\n",
       " 0.056,\n",
       " 0.013,\n",
       " 0.066,\n",
       " 0.072,\n",
       " 0.03,\n",
       " 0.047,\n",
       " 0.014,\n",
       " 0.123,\n",
       " 0.081,\n",
       " 0.054,\n",
       " 0.112,\n",
       " 0.03,\n",
       " 0.077,\n",
       " 0.0,\n",
       " 0.025,\n",
       " 0.034,\n",
       " 0.0,\n",
       " 0.071,\n",
       " 0.0,\n",
       " 0.037,\n",
       " 0.144,\n",
       " 0.057,\n",
       " 0.029,\n",
       " 0.02,\n",
       " 0.023,\n",
       " 0.0,\n",
       " 0.075,\n",
       " 0.011,\n",
       " 0.017,\n",
       " 0.057,\n",
       " 0.056,\n",
       " 0.068,\n",
       " 0.0,\n",
       " 0.094,\n",
       " 0.032,\n",
       " 0.028,\n",
       " 0.006,\n",
       " 0.011,\n",
       " 0.037,\n",
       " 0.012,\n",
       " 0.068,\n",
       " 0.068,\n",
       " 0.022,\n",
       " 0.057,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.035,\n",
       " 0.028,\n",
       " 0.027,\n",
       " 0.064,\n",
       " 0.034,\n",
       " 0.029,\n",
       " 0.024,\n",
       " 0.031,\n",
       " 0.037,\n",
       " 0.017,\n",
       " 0.042,\n",
       " 0.092,\n",
       " 0.055,\n",
       " 0.024,\n",
       " 0.052,\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negitive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1664002899974,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "mk6lrWWUN6HX",
    "outputId": "10338aa7-3d04-499b-c8df-4d4889f1f1f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.205,\n",
       " 0.248,\n",
       " 0.262,\n",
       " 0.187,\n",
       " 0.288,\n",
       " 0.286,\n",
       " 0.404,\n",
       " 0.302,\n",
       " 0.153,\n",
       " 0.235,\n",
       " 0.41,\n",
       " 0.235,\n",
       " 0.318,\n",
       " 0.353,\n",
       " 0.15,\n",
       " 0.288,\n",
       " 0.224,\n",
       " 0.259,\n",
       " 0.242,\n",
       " 0.23,\n",
       " 0.391,\n",
       " 0.259,\n",
       " 0.276,\n",
       " 0.203,\n",
       " 0.343,\n",
       " 0.311,\n",
       " 0.174,\n",
       " 0.198,\n",
       " 0.281,\n",
       " 0.2,\n",
       " 0.191,\n",
       " 0.239,\n",
       " 0.171,\n",
       " 0.237,\n",
       " 0.358,\n",
       " 0.443,\n",
       " 0.363,\n",
       " 0.29,\n",
       " 0.198,\n",
       " 0.327,\n",
       " 0.307,\n",
       " 0.35,\n",
       " 0.185,\n",
       " 0.181,\n",
       " 0.301,\n",
       " 0.082,\n",
       " 0.207,\n",
       " 0.226,\n",
       " 0.292,\n",
       " 0.194,\n",
       " 0.294,\n",
       " 0.193,\n",
       " 0.245,\n",
       " 0.341,\n",
       " 0.272,\n",
       " 0.166,\n",
       " 0.214,\n",
       " 0.148,\n",
       " 0.248,\n",
       " 0.191,\n",
       " 0.342,\n",
       " 0.316,\n",
       " 0.161,\n",
       " 0.254,\n",
       " 0.206,\n",
       " 0.407,\n",
       " 0.293,\n",
       " 0.214,\n",
       " 0.294,\n",
       " 0.293,\n",
       " 0.136,\n",
       " 0.222,\n",
       " 0.229,\n",
       " 0.279,\n",
       " 0.297,\n",
       " 0.317,\n",
       " 0.239,\n",
       " 0.202,\n",
       " 0.162,\n",
       " 0.169,\n",
       " 0.133,\n",
       " 0.152,\n",
       " 0.293,\n",
       " 0.084,\n",
       " 0.261,\n",
       " 0.253,\n",
       " 0.218,\n",
       " 0.33,\n",
       " 0.316,\n",
       " 0.164,\n",
       " 0.249,\n",
       " 0.12,\n",
       " 0.197,\n",
       " 0.136,\n",
       " 0.246,\n",
       " 0.224,\n",
       " 0.18,\n",
       " 0.21,\n",
       " 0.246,\n",
       " 0.205,\n",
       " 0.354,\n",
       " 0.186,\n",
       " 0.25,\n",
       " 0.188,\n",
       " 0.241,\n",
       " 0.212,\n",
       " 0.276,\n",
       " 0.288,\n",
       " 0.272,\n",
       " 0.201,\n",
       " 0.279,\n",
       " 0.103,\n",
       " 0.239,\n",
       " 0.278,\n",
       " 0.366,\n",
       " 0.277,\n",
       " 0.171,\n",
       " 0.317,\n",
       " 0.266,\n",
       " 0.221,\n",
       " 0.316,\n",
       " 0.186,\n",
       " 0.224,\n",
       " 0.24,\n",
       " 0.35,\n",
       " 0.313,\n",
       " 0.279,\n",
       " 0.254,\n",
       " 0.275,\n",
       " 0.141,\n",
       " 0.056,\n",
       " 0.384,\n",
       " 0.227,\n",
       " 0.284,\n",
       " 0.243,\n",
       " 0.284,\n",
       " 0.168,\n",
       " 0.303,\n",
       " 0.313,\n",
       " 0.251,\n",
       " 0.155,\n",
       " 0.304,\n",
       " 0.248,\n",
       " 0.311,\n",
       " 0.293,\n",
       " 0.38,\n",
       " 0.254,\n",
       " 0.255,\n",
       " 0.15,\n",
       " 0.215,\n",
       " 0.179,\n",
       " 0.267,\n",
       " 0.278,\n",
       " 0.209,\n",
       " 0.237,\n",
       " 0.314,\n",
       " 0.23,\n",
       " 0.187,\n",
       " 0.218,\n",
       " 0.383,\n",
       " 0.114,\n",
       " 0.408,\n",
       " 0.391,\n",
       " 0.372,\n",
       " 0.284,\n",
       " 0.277,\n",
       " 0.167,\n",
       " 0.399,\n",
       " 0.231,\n",
       " 0.306,\n",
       " 0.187,\n",
       " 0.387,\n",
       " 0.223,\n",
       " 0.156,\n",
       " 0.139,\n",
       " 0.168,\n",
       " 0.313,\n",
       " 0.171,\n",
       " 0.354,\n",
       " 0.179,\n",
       " 0.283,\n",
       " 0.231,\n",
       " 0.302,\n",
       " 0.185,\n",
       " 0.391,\n",
       " 0.27,\n",
       " 0.224,\n",
       " 0.199,\n",
       " 0.281,\n",
       " 0.308,\n",
       " 0.232,\n",
       " 0.141,\n",
       " 0.273,\n",
       " 0.286,\n",
       " 0.168,\n",
       " 0.213,\n",
       " 0.337,\n",
       " 0.253,\n",
       " 0.197,\n",
       " 0.213,\n",
       " 0.312,\n",
       " 0.178,\n",
       " 0.285,\n",
       " 0.329,\n",
       " 0.304,\n",
       " 0.336,\n",
       " 0.199,\n",
       " 0.179,\n",
       " 0.192,\n",
       " 0.393,\n",
       " 0.124,\n",
       " 0.301,\n",
       " 0.215,\n",
       " 0.275,\n",
       " 0.142,\n",
       " 0.18,\n",
       " 0.307,\n",
       " 0.284,\n",
       " 0.142,\n",
       " 0.116,\n",
       " 0.24,\n",
       " 0.296,\n",
       " 0.284,\n",
       " 0.275,\n",
       " 0.155,\n",
       " 0.172,\n",
       " 0.26,\n",
       " 0.222,\n",
       " 0.252,\n",
       " 0.371,\n",
       " 0.219,\n",
       " 0.212,\n",
       " 0.437,\n",
       " 0.44,\n",
       " 0.144,\n",
       " 0.348,\n",
       " 0.309,\n",
       " 0.284,\n",
       " 0.392,\n",
       " 0.308,\n",
       " 0.326,\n",
       " 0.276,\n",
       " 0.285,\n",
       " 0.394,\n",
       " 0.2,\n",
       " 0.287,\n",
       " 0.231,\n",
       " 0.185,\n",
       " 0.261,\n",
       " 0.379,\n",
       " 0.232,\n",
       " 0.174,\n",
       " 0.298,\n",
       " 0.336,\n",
       " 0.228,\n",
       " 0.423,\n",
       " 0.302,\n",
       " 0.221,\n",
       " 0.167,\n",
       " 0.19,\n",
       " 0.238,\n",
       " 0.182,\n",
       " 0.181,\n",
       " 0.208,\n",
       " 0.179,\n",
       " 0.298,\n",
       " 0.193,\n",
       " 0.181,\n",
       " 0.384,\n",
       " 0.288,\n",
       " 0.236,\n",
       " 0.29,\n",
       " 0.204,\n",
       " 0.395,\n",
       " 0.299,\n",
       " 0.343,\n",
       " 0.337,\n",
       " 0.326,\n",
       " 0.242,\n",
       " 0.262,\n",
       " 0.163,\n",
       " 0.241,\n",
       " 0.383,\n",
       " 0.339,\n",
       " 0.386,\n",
       " 0.275,\n",
       " 0.24,\n",
       " 0.221,\n",
       " 0.271,\n",
       " 0.302,\n",
       " 0.205,\n",
       " 0.292,\n",
       " 0.27,\n",
       " 0.314,\n",
       " 0.175,\n",
       " 0.281,\n",
       " 0.342,\n",
       " 0.175,\n",
       " 0.174,\n",
       " 0.222,\n",
       " 0.195,\n",
       " 0.15,\n",
       " 0.205,\n",
       " 0.158,\n",
       " 0.331,\n",
       " 0.343,\n",
       " 0.199,\n",
       " 0.246,\n",
       " 0.101,\n",
       " 0.305,\n",
       " 0.283,\n",
       " 0.157,\n",
       " 0.267,\n",
       " 0.453,\n",
       " 0.407,\n",
       " 0.202,\n",
       " 0.298,\n",
       " 0.183,\n",
       " 0.243,\n",
       " 0.3,\n",
       " 0.163,\n",
       " 0.291,\n",
       " 0.123,\n",
       " 0.267,\n",
       " 0.251,\n",
       " 0.247,\n",
       " 0.245,\n",
       " 0.241,\n",
       " 0.149,\n",
       " 0.346,\n",
       " 0.154,\n",
       " 0.26,\n",
       " 0.246,\n",
       " 0.254,\n",
       " 0.217,\n",
       " 0.185,\n",
       " 0.236,\n",
       " 0.186,\n",
       " 0.133,\n",
       " 0.181,\n",
       " 0.205,\n",
       " 0.119,\n",
       " 0.246,\n",
       " 0.246,\n",
       " 0.21,\n",
       " 0.361,\n",
       " 0.271,\n",
       " 0.285,\n",
       " 0.195,\n",
       " 0.29,\n",
       " 0.348,\n",
       " 0.369,\n",
       " 0.266,\n",
       " 0.266,\n",
       " 0.424,\n",
       " 0.212,\n",
       " 0.211,\n",
       " 0.267,\n",
       " 0.279,\n",
       " 0.146,\n",
       " 0.454,\n",
       " 0.179,\n",
       " 0.216,\n",
       " 0.177,\n",
       " 0.094,\n",
       " 0.185,\n",
       " 0.329,\n",
       " 0.286,\n",
       " 0.156,\n",
       " 0.135,\n",
       " 0.201,\n",
       " 0.218,\n",
       " 0.304,\n",
       " 0.279,\n",
       " 0.414,\n",
       " 0.141,\n",
       " 0.224,\n",
       " 0.147,\n",
       " 0.202,\n",
       " 0.392,\n",
       " 0.194,\n",
       " 0.318,\n",
       " 0.189,\n",
       " 0.262,\n",
       " 0.236,\n",
       " 0.157,\n",
       " 0.414,\n",
       " 0.179,\n",
       " 0.245,\n",
       " 0.322,\n",
       " 0.243,\n",
       " 0.31,\n",
       " 0.22,\n",
       " 0.262,\n",
       " 0.221,\n",
       " 0.499,\n",
       " 0.188,\n",
       " 0.225,\n",
       " 0.229,\n",
       " 0.297,\n",
       " 0.184,\n",
       " 0.184,\n",
       " 0.258,\n",
       " 0.101,\n",
       " 0.351,\n",
       " 0.27,\n",
       " 0.333,\n",
       " 0.313,\n",
       " 0.173,\n",
       " 0.287,\n",
       " 0.175,\n",
       " 0.12,\n",
       " 0.27,\n",
       " 0.337,\n",
       " 0.286,\n",
       " 0.241,\n",
       " 0.239,\n",
       " 0.297,\n",
       " 0.313,\n",
       " 0.257,\n",
       " 0.274,\n",
       " 0.386,\n",
       " 0.218,\n",
       " 0.211,\n",
       " 0.289,\n",
       " 0.209,\n",
       " 0.277,\n",
       " 0.215,\n",
       " 0.268,\n",
       " 0.317,\n",
       " 0.195,\n",
       " 0.229,\n",
       " 0.332,\n",
       " 0.333,\n",
       " 0.175,\n",
       " 0.278,\n",
       " 0.212,\n",
       " 0.284,\n",
       " 0.288,\n",
       " 0.229,\n",
       " 0.267,\n",
       " 0.438,\n",
       " 0.149,\n",
       " 0.323,\n",
       " 0.284,\n",
       " 0.26,\n",
       " 0.374,\n",
       " 0.288,\n",
       " 0.245,\n",
       " 0.199,\n",
       " 0.29,\n",
       " 0.297,\n",
       " 0.306,\n",
       " 0.29,\n",
       " 0.259,\n",
       " 0.382,\n",
       " 0.162,\n",
       " 0.32,\n",
       " 0.222,\n",
       " 0.168,\n",
       " 0.23,\n",
       " 0.258,\n",
       " 0.117,\n",
       " 0.297,\n",
       " 0.28,\n",
       " 0.239,\n",
       " 0.246,\n",
       " 0.238,\n",
       " 0.236,\n",
       " 0.244,\n",
       " 0.247,\n",
       " 0.386,\n",
       " 0.417,\n",
       " 0.36,\n",
       " 0.309,\n",
       " 0.444,\n",
       " 0.259,\n",
       " 0.306,\n",
       " 0.246,\n",
       " 0.161,\n",
       " 0.283,\n",
       " 0.269,\n",
       " 0.211,\n",
       " 0.383,\n",
       " 0.32,\n",
       " 0.321,\n",
       " 0.314,\n",
       " 0.276,\n",
       " 0.36,\n",
       " 0.141,\n",
       " 0.381,\n",
       " 0.143,\n",
       " 0.3,\n",
       " 0.335,\n",
       " 0.204,\n",
       " 0.305,\n",
       " 0.36,\n",
       " 0.316,\n",
       " 0.314,\n",
       " 0.235,\n",
       " 0.256,\n",
       " 0.158,\n",
       " 0.273,\n",
       " 0.354,\n",
       " 0.213,\n",
       " 0.255,\n",
       " 0.282,\n",
       " 0.282,\n",
       " 0.258,\n",
       " 0.244,\n",
       " 0.342,\n",
       " 0.243,\n",
       " 0.263,\n",
       " 0.32,\n",
       " 0.176,\n",
       " 0.254,\n",
       " 0.481,\n",
       " 0.318,\n",
       " 0.154,\n",
       " 0.174,\n",
       " 0.157,\n",
       " 0.229,\n",
       " 0.217,\n",
       " 0.312,\n",
       " 0.228,\n",
       " 0.273,\n",
       " 0.189,\n",
       " 0.286,\n",
       " 0.239,\n",
       " 0.215,\n",
       " 0.296,\n",
       " 0.157,\n",
       " 0.45,\n",
       " 0.347,\n",
       " 0.195,\n",
       " 0.3,\n",
       " 0.202,\n",
       " 0.219,\n",
       " 0.213,\n",
       " 0.33,\n",
       " 0.175,\n",
       " 0.154,\n",
       " 0.361,\n",
       " 0.192,\n",
       " 0.11,\n",
       " 0.181,\n",
       " 0.306,\n",
       " 0.227,\n",
       " 0.164,\n",
       " 0.245,\n",
       " 0.293,\n",
       " 0.347,\n",
       " 0.19,\n",
       " 0.155,\n",
       " 0.388,\n",
       " 0.196,\n",
       " 0.288,\n",
       " 0.331,\n",
       " 0.293,\n",
       " 0.262,\n",
       " 0.309,\n",
       " 0.328,\n",
       " 0.225,\n",
       " 0.242,\n",
       " 0.08,\n",
       " 0.251,\n",
       " 0.358,\n",
       " 0.238,\n",
       " 0.381,\n",
       " 0.269,\n",
       " 0.198,\n",
       " 0.3,\n",
       " 0.367,\n",
       " 0.241,\n",
       " 0.299,\n",
       " 0.28,\n",
       " 0.212,\n",
       " 0.228,\n",
       " 0.227,\n",
       " 0.26,\n",
       " 0.331,\n",
       " 0.232,\n",
       " 0.142,\n",
       " 0.283,\n",
       " 0.308,\n",
       " 0.255,\n",
       " 0.204,\n",
       " 0.26,\n",
       " 0.163,\n",
       " 0.247,\n",
       " 0.146,\n",
       " 0.361,\n",
       " 0.223,\n",
       " 0.134,\n",
       " 0.264,\n",
       " 0.317,\n",
       " 0.328,\n",
       " 0.177,\n",
       " 0.211,\n",
       " 0.134,\n",
       " 0.168,\n",
       " 0.226,\n",
       " 0.125,\n",
       " 0.136,\n",
       " 0.234,\n",
       " 0.269,\n",
       " 0.182,\n",
       " 0.316,\n",
       " 0.342,\n",
       " 0.28,\n",
       " 0.166,\n",
       " 0.205,\n",
       " 0.2,\n",
       " 0.423,\n",
       " 0.16,\n",
       " 0.328,\n",
       " 0.309,\n",
       " 0.223,\n",
       " 0.186,\n",
       " 0.376,\n",
       " 0.24,\n",
       " 0.237,\n",
       " 0.245,\n",
       " 0.463,\n",
       " 0.379,\n",
       " 0.231,\n",
       " 0.204,\n",
       " 0.182,\n",
       " 0.243,\n",
       " 0.179,\n",
       " 0.367,\n",
       " 0.318,\n",
       " 0.096,\n",
       " 0.27,\n",
       " 0.304,\n",
       " 0.179,\n",
       " 0.28,\n",
       " 0.278,\n",
       " 0.225,\n",
       " 0.234,\n",
       " 0.399,\n",
       " 0.416,\n",
       " 0.232,\n",
       " 0.194,\n",
       " 0.389,\n",
       " 0.186,\n",
       " 0.265,\n",
       " 0.406,\n",
       " 0.292,\n",
       " 0.243,\n",
       " 0.199,\n",
       " 0.343,\n",
       " 0.244,\n",
       " 0.249,\n",
       " 0.274,\n",
       " 0.085,\n",
       " 0.341,\n",
       " 0.393,\n",
       " 0.335,\n",
       " 0.211,\n",
       " 0.17,\n",
       " 0.162,\n",
       " 0.176,\n",
       " 0.222,\n",
       " 0.293,\n",
       " 0.199,\n",
       " 0.202,\n",
       " 0.192,\n",
       " 0.222,\n",
       " 0.268,\n",
       " 0.228,\n",
       " 0.302,\n",
       " 0.189,\n",
       " 0.242,\n",
       " 0.262,\n",
       " 0.236,\n",
       " 0.378,\n",
       " 0.347,\n",
       " 0.305,\n",
       " 0.293,\n",
       " 0.399,\n",
       " 0.367,\n",
       " 0.353,\n",
       " 0.272,\n",
       " 0.238,\n",
       " 0.204,\n",
       " 0.178,\n",
       " 0.357,\n",
       " 0.197,\n",
       " 0.267,\n",
       " 0.345,\n",
       " 0.318,\n",
       " 0.18,\n",
       " 0.263,\n",
       " 0.282,\n",
       " 0.27,\n",
       " 0.284,\n",
       " 0.227,\n",
       " 0.362,\n",
       " 0.291,\n",
       " 0.22,\n",
       " 0.364,\n",
       " 0.212,\n",
       " 0.254,\n",
       " 0.285,\n",
       " 0.25,\n",
       " 0.293,\n",
       " 0.319,\n",
       " 0.292,\n",
       " 0.197,\n",
       " 0.326,\n",
       " 0.403,\n",
       " 0.092,\n",
       " 0.302,\n",
       " 0.388,\n",
       " 0.371,\n",
       " 0.171,\n",
       " 0.292,\n",
       " 0.29,\n",
       " 0.33,\n",
       " 0.278,\n",
       " 0.413,\n",
       " 0.297,\n",
       " 0.171,\n",
       " 0.333,\n",
       " 0.359,\n",
       " 0.25,\n",
       " 0.233,\n",
       " 0.286,\n",
       " 0.176,\n",
       " 0.264,\n",
       " 0.372,\n",
       " 0.258,\n",
       " 0.286,\n",
       " 0.317,\n",
       " 0.157,\n",
       " 0.408,\n",
       " 0.209,\n",
       " 0.281,\n",
       " 0.234,\n",
       " 0.234,\n",
       " 0.224,\n",
       " 0.161,\n",
       " 0.305,\n",
       " 0.285,\n",
       " 0.184,\n",
       " 0.206,\n",
       " 0.293,\n",
       " 0.241,\n",
       " 0.304,\n",
       " 0.214,\n",
       " 0.329,\n",
       " 0.251,\n",
       " 0.321,\n",
       " 0.331,\n",
       " 0.367,\n",
       " 0.179,\n",
       " 0.309,\n",
       " 0.251,\n",
       " 0.306,\n",
       " 0.264,\n",
       " 0.258,\n",
       " 0.252,\n",
       " 0.191,\n",
       " 0.222,\n",
       " 0.387,\n",
       " 0.365,\n",
       " 0.175,\n",
       " 0.172,\n",
       " 0.245,\n",
       " 0.265,\n",
       " 0.228,\n",
       " 0.23,\n",
       " 0.285,\n",
       " 0.394,\n",
       " 0.073,\n",
       " 0.349,\n",
       " 0.235,\n",
       " 0.213,\n",
       " 0.157,\n",
       " 0.347,\n",
       " 0.121,\n",
       " 0.275,\n",
       " 0.285,\n",
       " 0.235,\n",
       " 0.246,\n",
       " 0.372,\n",
       " 0.247,\n",
       " 0.206,\n",
       " 0.228,\n",
       " 0.352,\n",
       " 0.207,\n",
       " 0.342,\n",
       " 0.175,\n",
       " 0.264,\n",
       " 0.257,\n",
       " 0.251,\n",
       " 0.161,\n",
       " 0.275,\n",
       " 0.239,\n",
       " 0.125,\n",
       " 0.241,\n",
       " 0.325,\n",
       " 0.208,\n",
       " 0.271,\n",
       " 0.231,\n",
       " 0.188,\n",
       " 0.223,\n",
       " 0.269,\n",
       " 0.266,\n",
       " 0.398,\n",
       " 0.124,\n",
       " 0.214,\n",
       " 0.262,\n",
       " 0.233,\n",
       " 0.273,\n",
       " 0.245,\n",
       " 0.239,\n",
       " 0.208,\n",
       " 0.306,\n",
       " 0.278,\n",
       " 0.272,\n",
       " 0.239,\n",
       " 0.253,\n",
       " 0.356,\n",
       " 0.153,\n",
       " 0.193,\n",
       " 0.379,\n",
       " 0.203,\n",
       " 0.239,\n",
       " 0.307,\n",
       " 0.333,\n",
       " 0.295,\n",
       " 0.216,\n",
       " 0.232,\n",
       " 0.231,\n",
       " 0.304,\n",
       " 0.295,\n",
       " 0.352,\n",
       " 0.228,\n",
       " 0.354,\n",
       " 0.143,\n",
       " 0.226,\n",
       " 0.173,\n",
       " 0.194,\n",
       " 0.302,\n",
       " 0.319,\n",
       " 0.284,\n",
       " 0.203,\n",
       " 0.279,\n",
       " 0.221,\n",
       " 0.328,\n",
       " 0.13,\n",
       " 0.339,\n",
       " 0.315,\n",
       " 0.212,\n",
       " 0.298,\n",
       " 0.312,\n",
       " 0.231,\n",
       " 0.172,\n",
       " 0.337,\n",
       " 0.357,\n",
       " 0.429,\n",
       " 0.381,\n",
       " 0.287,\n",
       " 0.279,\n",
       " 0.207,\n",
       " 0.21,\n",
       " 0.278,\n",
       " 0.128,\n",
       " 0.27,\n",
       " 0.307,\n",
       " 0.251,\n",
       " 0.172,\n",
       " 0.298,\n",
       " 0.124,\n",
       " 0.296,\n",
       " 0.365,\n",
       " 0.144,\n",
       " 0.23,\n",
       " 0.254,\n",
       " 0.194,\n",
       " 0.24,\n",
       " 0.385,\n",
       " 0.284,\n",
       " 0.266,\n",
       " 0.217,\n",
       " 0.277,\n",
       " 0.411,\n",
       " 0.227,\n",
       " 0.34,\n",
       " 0.234,\n",
       " 0.227,\n",
       " 0.308,\n",
       " 0.244,\n",
       " 0.32,\n",
       " 0.264,\n",
       " 0.285,\n",
       " 0.294,\n",
       " 0.193,\n",
       " 0.298,\n",
       " 0.298,\n",
       " 0.424,\n",
       " 0.115,\n",
       " 0.17,\n",
       " 0.223,\n",
       " 0.416,\n",
       " 0.302,\n",
       " 0.232,\n",
       " 0.19,\n",
       " 0.325,\n",
       " 0.109,\n",
       " 0.329,\n",
       " 0.254,\n",
       " 0.235,\n",
       " 0.294,\n",
       " 0.201,\n",
       " 0.358,\n",
       " 0.325,\n",
       " 0.348,\n",
       " 0.344,\n",
       " 0.172,\n",
       " 0.281,\n",
       " 0.19,\n",
       " 0.19,\n",
       " 0.142,\n",
       " 0.364,\n",
       " 0.377,\n",
       " 0.313,\n",
       " 0.194,\n",
       " 0.166,\n",
       " 0.267,\n",
       " 0.215,\n",
       " 0.397,\n",
       " 0.256,\n",
       " 0.29,\n",
       " 0.242,\n",
       " 0.299,\n",
       " 0.192,\n",
       " 0.219,\n",
       " 0.212,\n",
       " 0.177,\n",
       " 0.267,\n",
       " 0.185,\n",
       " 0.135,\n",
       " 0.343,\n",
       " 0.244,\n",
       " 0.252,\n",
       " 0.381,\n",
       " 0.22,\n",
       " 0.238,\n",
       " 0.105,\n",
       " 0.175,\n",
       " 0.25,\n",
       " 0.31,\n",
       " 0.193,\n",
       " 0.168,\n",
       " 0.275,\n",
       " 0.25,\n",
       " 0.17,\n",
       " 0.176,\n",
       " 0.325,\n",
       " 0.341,\n",
       " 0.418,\n",
       " 0.292,\n",
       " 0.284,\n",
       " 0.298,\n",
       " 0.356,\n",
       " 0.357,\n",
       " 0.207,\n",
       " 0.183,\n",
       " 0.208,\n",
       " 0.306,\n",
       " 0.301,\n",
       " 0.184,\n",
       " 0.227,\n",
       " 0.335,\n",
       " 0.24,\n",
       " 0.237,\n",
       " 0.257,\n",
       " 0.274,\n",
       " 0.265,\n",
       " 0.202,\n",
       " 0.44,\n",
       " 0.284,\n",
       " 0.293,\n",
       " 0.276,\n",
       " 0.282,\n",
       " 0.184,\n",
       " 0.353,\n",
       " 0.269,\n",
       " 0.233,\n",
       " 0.167,\n",
       " 0.32,\n",
       " 0.226,\n",
       " 0.221,\n",
       " 0.225,\n",
       " 0.296,\n",
       " 0.283,\n",
       " 0.25,\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1664002899975,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "Ezlf2vDQN6HY",
    "outputId": "008b45cd-ccd4-4795-d020-0bdd27284adc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.783,\n",
       " 0.68,\n",
       " 0.721,\n",
       " 0.783,\n",
       " 0.683,\n",
       " 0.701,\n",
       " 0.577,\n",
       " 0.631,\n",
       " 0.784,\n",
       " 0.708,\n",
       " 0.578,\n",
       " 0.731,\n",
       " 0.669,\n",
       " 0.606,\n",
       " 0.832,\n",
       " 0.691,\n",
       " 0.769,\n",
       " 0.661,\n",
       " 0.732,\n",
       " 0.726,\n",
       " 0.609,\n",
       " 0.72,\n",
       " 0.66,\n",
       " 0.764,\n",
       " 0.646,\n",
       " 0.651,\n",
       " 0.8,\n",
       " 0.712,\n",
       " 0.719,\n",
       " 0.67,\n",
       " 0.798,\n",
       " 0.696,\n",
       " 0.803,\n",
       " 0.749,\n",
       " 0.609,\n",
       " 0.55,\n",
       " 0.637,\n",
       " 0.668,\n",
       " 0.761,\n",
       " 0.638,\n",
       " 0.603,\n",
       " 0.62,\n",
       " 0.815,\n",
       " 0.768,\n",
       " 0.699,\n",
       " 0.9,\n",
       " 0.72,\n",
       " 0.75,\n",
       " 0.623,\n",
       " 0.796,\n",
       " 0.644,\n",
       " 0.796,\n",
       " 0.732,\n",
       " 0.617,\n",
       " 0.685,\n",
       " 0.809,\n",
       " 0.74,\n",
       " 0.772,\n",
       " 0.702,\n",
       " 0.751,\n",
       " 0.643,\n",
       " 0.603,\n",
       " 0.824,\n",
       " 0.71,\n",
       " 0.778,\n",
       " 0.558,\n",
       " 0.643,\n",
       " 0.76,\n",
       " 0.685,\n",
       " 0.643,\n",
       " 0.864,\n",
       " 0.765,\n",
       " 0.747,\n",
       " 0.636,\n",
       " 0.703,\n",
       " 0.653,\n",
       " 0.723,\n",
       " 0.739,\n",
       " 0.826,\n",
       " 0.739,\n",
       " 0.852,\n",
       " 0.815,\n",
       " 0.699,\n",
       " 0.822,\n",
       " 0.681,\n",
       " 0.736,\n",
       " 0.773,\n",
       " 0.633,\n",
       " 0.657,\n",
       " 0.775,\n",
       " 0.743,\n",
       " 0.848,\n",
       " 0.741,\n",
       " 0.817,\n",
       " 0.718,\n",
       " 0.714,\n",
       " 0.795,\n",
       " 0.79,\n",
       " 0.714,\n",
       " 0.795,\n",
       " 0.61,\n",
       " 0.781,\n",
       " 0.75,\n",
       " 0.779,\n",
       " 0.759,\n",
       " 0.726,\n",
       " 0.689,\n",
       " 0.672,\n",
       " 0.65,\n",
       " 0.78,\n",
       " 0.667,\n",
       " 0.858,\n",
       " 0.733,\n",
       " 0.699,\n",
       " 0.54,\n",
       " 0.675,\n",
       " 0.816,\n",
       " 0.649,\n",
       " 0.661,\n",
       " 0.76,\n",
       " 0.595,\n",
       " 0.757,\n",
       " 0.758,\n",
       " 0.721,\n",
       " 0.607,\n",
       " 0.605,\n",
       " 0.592,\n",
       " 0.746,\n",
       " 0.702,\n",
       " 0.839,\n",
       " 0.9,\n",
       " 0.563,\n",
       " 0.758,\n",
       " 0.65,\n",
       " 0.738,\n",
       " 0.66,\n",
       " 0.821,\n",
       " 0.67,\n",
       " 0.676,\n",
       " 0.749,\n",
       " 0.808,\n",
       " 0.652,\n",
       " 0.752,\n",
       " 0.656,\n",
       " 0.665,\n",
       " 0.602,\n",
       " 0.718,\n",
       " 0.665,\n",
       " 0.775,\n",
       " 0.771,\n",
       " 0.75,\n",
       " 0.663,\n",
       " 0.629,\n",
       " 0.783,\n",
       " 0.725,\n",
       " 0.636,\n",
       " 0.645,\n",
       " 0.79,\n",
       " 0.743,\n",
       " 0.617,\n",
       " 0.863,\n",
       " 0.584,\n",
       " 0.609,\n",
       " 0.628,\n",
       " 0.669,\n",
       " 0.63,\n",
       " 0.693,\n",
       " 0.582,\n",
       " 0.723,\n",
       " 0.651,\n",
       " 0.783,\n",
       " 0.582,\n",
       " 0.746,\n",
       " 0.781,\n",
       " 0.813,\n",
       " 0.754,\n",
       " 0.611,\n",
       " 0.817,\n",
       " 0.62,\n",
       " 0.795,\n",
       " 0.658,\n",
       " 0.66,\n",
       " 0.634,\n",
       " 0.786,\n",
       " 0.589,\n",
       " 0.73,\n",
       " 0.678,\n",
       " 0.801,\n",
       " 0.679,\n",
       " 0.66,\n",
       " 0.752,\n",
       " 0.859,\n",
       " 0.727,\n",
       " 0.687,\n",
       " 0.792,\n",
       " 0.718,\n",
       " 0.611,\n",
       " 0.595,\n",
       " 0.727,\n",
       " 0.769,\n",
       " 0.668,\n",
       " 0.721,\n",
       " 0.696,\n",
       " 0.599,\n",
       " 0.639,\n",
       " 0.635,\n",
       " 0.755,\n",
       " 0.821,\n",
       " 0.808,\n",
       " 0.569,\n",
       " 0.839,\n",
       " 0.684,\n",
       " 0.738,\n",
       " 0.684,\n",
       " 0.808,\n",
       " 0.803,\n",
       " 0.654,\n",
       " 0.685,\n",
       " 0.834,\n",
       " 0.861,\n",
       " 0.76,\n",
       " 0.632,\n",
       " 0.636,\n",
       " 0.725,\n",
       " 0.725,\n",
       " 0.809,\n",
       " 0.72,\n",
       " 0.778,\n",
       " 0.685,\n",
       " 0.608,\n",
       " 0.741,\n",
       " 0.69,\n",
       " 0.542,\n",
       " 0.56,\n",
       " 0.732,\n",
       " 0.614,\n",
       " 0.681,\n",
       " 0.698,\n",
       " 0.594,\n",
       " 0.618,\n",
       " 0.579,\n",
       " 0.632,\n",
       " 0.703,\n",
       " 0.598,\n",
       " 0.736,\n",
       " 0.67,\n",
       " 0.757,\n",
       " 0.795,\n",
       " 0.726,\n",
       " 0.571,\n",
       " 0.69,\n",
       " 0.726,\n",
       " 0.655,\n",
       " 0.636,\n",
       " 0.713,\n",
       " 0.556,\n",
       " 0.643,\n",
       " 0.728,\n",
       " 0.731,\n",
       " 0.741,\n",
       " 0.634,\n",
       " 0.804,\n",
       " 0.805,\n",
       " 0.781,\n",
       " 0.76,\n",
       " 0.682,\n",
       " 0.796,\n",
       " 0.701,\n",
       " 0.585,\n",
       " 0.642,\n",
       " 0.74,\n",
       " 0.66,\n",
       " 0.739,\n",
       " 0.594,\n",
       " 0.693,\n",
       " 0.624,\n",
       " 0.601,\n",
       " 0.652,\n",
       " 0.651,\n",
       " 0.715,\n",
       " 0.804,\n",
       " 0.735,\n",
       " 0.588,\n",
       " 0.61,\n",
       " 0.58,\n",
       " 0.718,\n",
       " 0.727,\n",
       " 0.726,\n",
       " 0.729,\n",
       " 0.579,\n",
       " 0.737,\n",
       " 0.689,\n",
       " 0.666,\n",
       " 0.64,\n",
       " 0.77,\n",
       " 0.696,\n",
       " 0.658,\n",
       " 0.77,\n",
       " 0.717,\n",
       " 0.734,\n",
       " 0.765,\n",
       " 0.82,\n",
       " 0.732,\n",
       " 0.822,\n",
       " 0.627,\n",
       " 0.616,\n",
       " 0.68,\n",
       " 0.734,\n",
       " 0.885,\n",
       " 0.604,\n",
       " 0.666,\n",
       " 0.799,\n",
       " 0.702,\n",
       " 0.537,\n",
       " 0.58,\n",
       " 0.798,\n",
       " 0.634,\n",
       " 0.746,\n",
       " 0.743,\n",
       " 0.65,\n",
       " 0.788,\n",
       " 0.673,\n",
       " 0.817,\n",
       " 0.715,\n",
       " 0.648,\n",
       " 0.712,\n",
       " 0.639,\n",
       " 0.732,\n",
       " 0.774,\n",
       " 0.631,\n",
       " 0.827,\n",
       " 0.693,\n",
       " 0.745,\n",
       " 0.624,\n",
       " 0.742,\n",
       " 0.745,\n",
       " 0.716,\n",
       " 0.774,\n",
       " 0.724,\n",
       " 0.807,\n",
       " 0.678,\n",
       " 0.706,\n",
       " 0.754,\n",
       " 0.742,\n",
       " 0.762,\n",
       " 0.612,\n",
       " 0.719,\n",
       " 0.661,\n",
       " 0.77,\n",
       " 0.673,\n",
       " 0.634,\n",
       " 0.587,\n",
       " 0.688,\n",
       " 0.734,\n",
       " 0.558,\n",
       " 0.735,\n",
       " 0.707,\n",
       " 0.691,\n",
       " 0.566,\n",
       " 0.752,\n",
       " 0.482,\n",
       " 0.754,\n",
       " 0.717,\n",
       " 0.794,\n",
       " 0.838,\n",
       " 0.757,\n",
       " 0.639,\n",
       " 0.651,\n",
       " 0.75,\n",
       " 0.822,\n",
       " 0.782,\n",
       " 0.601,\n",
       " 0.643,\n",
       " 0.669,\n",
       " 0.586,\n",
       " 0.836,\n",
       " 0.766,\n",
       " 0.719,\n",
       " 0.772,\n",
       " 0.594,\n",
       " 0.778,\n",
       " 0.682,\n",
       " 0.791,\n",
       " 0.72,\n",
       " 0.729,\n",
       " 0.776,\n",
       " 0.573,\n",
       " 0.767,\n",
       " 0.729,\n",
       " 0.656,\n",
       " 0.734,\n",
       " 0.69,\n",
       " 0.78,\n",
       " 0.716,\n",
       " 0.685,\n",
       " 0.476,\n",
       " 0.767,\n",
       " 0.766,\n",
       " 0.724,\n",
       " 0.679,\n",
       " 0.797,\n",
       " 0.768,\n",
       " 0.727,\n",
       " 0.886,\n",
       " 0.581,\n",
       " 0.707,\n",
       " 0.642,\n",
       " 0.631,\n",
       " 0.814,\n",
       " 0.655,\n",
       " 0.786,\n",
       " 0.853,\n",
       " 0.686,\n",
       " 0.629,\n",
       " 0.672,\n",
       " 0.703,\n",
       " 0.736,\n",
       " 0.703,\n",
       " 0.655,\n",
       " 0.724,\n",
       " 0.704,\n",
       " 0.614,\n",
       " 0.685,\n",
       " 0.699,\n",
       " 0.662,\n",
       " 0.756,\n",
       " 0.683,\n",
       " 0.752,\n",
       " 0.732,\n",
       " 0.665,\n",
       " 0.76,\n",
       " 0.738,\n",
       " 0.637,\n",
       " 0.627,\n",
       " 0.787,\n",
       " 0.674,\n",
       " 0.751,\n",
       " 0.687,\n",
       " 0.655,\n",
       " 0.754,\n",
       " 0.733,\n",
       " 0.507,\n",
       " 0.756,\n",
       " 0.606,\n",
       " 0.59,\n",
       " 0.689,\n",
       " 0.626,\n",
       " 0.685,\n",
       " 0.746,\n",
       " 0.792,\n",
       " 0.692,\n",
       " 0.648,\n",
       " 0.625,\n",
       " 0.672,\n",
       " 0.657,\n",
       " 0.607,\n",
       " 0.776,\n",
       " 0.624,\n",
       " 0.761,\n",
       " 0.794,\n",
       " 0.749,\n",
       " 0.728,\n",
       " 0.818,\n",
       " 0.647,\n",
       " 0.67,\n",
       " 0.728,\n",
       " 0.694,\n",
       " 0.739,\n",
       " 0.764,\n",
       " 0.714,\n",
       " 0.737,\n",
       " 0.604,\n",
       " 0.564,\n",
       " 0.617,\n",
       " 0.644,\n",
       " 0.532,\n",
       " 0.71,\n",
       " 0.619,\n",
       " 0.71,\n",
       " 0.798,\n",
       " 0.697,\n",
       " 0.706,\n",
       " 0.725,\n",
       " 0.593,\n",
       " 0.565,\n",
       " 0.615,\n",
       " 0.651,\n",
       " 0.715,\n",
       " 0.576,\n",
       " 0.793,\n",
       " 0.561,\n",
       " 0.76,\n",
       " 0.672,\n",
       " 0.649,\n",
       " 0.749,\n",
       " 0.61,\n",
       " 0.601,\n",
       " 0.651,\n",
       " 0.655,\n",
       " 0.708,\n",
       " 0.732,\n",
       " 0.796,\n",
       " 0.606,\n",
       " 0.567,\n",
       " 0.694,\n",
       " 0.715,\n",
       " 0.665,\n",
       " 0.692,\n",
       " 0.718,\n",
       " 0.703,\n",
       " 0.581,\n",
       " 0.696,\n",
       " 0.698,\n",
       " 0.68,\n",
       " 0.799,\n",
       " 0.712,\n",
       " 0.519,\n",
       " 0.641,\n",
       " 0.779,\n",
       " 0.761,\n",
       " 0.769,\n",
       " 0.734,\n",
       " 0.756,\n",
       " 0.593,\n",
       " 0.743,\n",
       " 0.727,\n",
       " 0.785,\n",
       " 0.714,\n",
       " 0.747,\n",
       " 0.722,\n",
       " 0.661,\n",
       " 0.764,\n",
       " 0.524,\n",
       " 0.599,\n",
       " 0.756,\n",
       " 0.644,\n",
       " 0.726,\n",
       " 0.723,\n",
       " 0.692,\n",
       " 0.67,\n",
       " 0.749,\n",
       " 0.846,\n",
       " 0.587,\n",
       " 0.765,\n",
       " 0.835,\n",
       " 0.75,\n",
       " 0.646,\n",
       " 0.746,\n",
       " 0.771,\n",
       " 0.724,\n",
       " 0.621,\n",
       " 0.523,\n",
       " 0.729,\n",
       " 0.788,\n",
       " 0.595,\n",
       " 0.786,\n",
       " 0.69,\n",
       " 0.631,\n",
       " 0.617,\n",
       " 0.696,\n",
       " 0.629,\n",
       " 0.661,\n",
       " 0.735,\n",
       " 0.651,\n",
       " 0.881,\n",
       " 0.598,\n",
       " 0.593,\n",
       " 0.712,\n",
       " 0.557,\n",
       " 0.695,\n",
       " 0.789,\n",
       " 0.636,\n",
       " 0.617,\n",
       " 0.695,\n",
       " 0.701,\n",
       " 0.696,\n",
       " 0.621,\n",
       " 0.687,\n",
       " 0.773,\n",
       " 0.657,\n",
       " 0.669,\n",
       " 0.733,\n",
       " 0.834,\n",
       " 0.67,\n",
       " 0.692,\n",
       " 0.675,\n",
       " 0.688,\n",
       " 0.695,\n",
       " 0.778,\n",
       " 0.753,\n",
       " 0.758,\n",
       " 0.61,\n",
       " 0.762,\n",
       " 0.796,\n",
       " 0.648,\n",
       " 0.66,\n",
       " 0.672,\n",
       " 0.823,\n",
       " 0.763,\n",
       " 0.853,\n",
       " 0.755,\n",
       " 0.674,\n",
       " 0.746,\n",
       " 0.755,\n",
       " 0.7,\n",
       " 0.681,\n",
       " 0.763,\n",
       " 0.684,\n",
       " 0.567,\n",
       " 0.673,\n",
       " 0.834,\n",
       " 0.733,\n",
       " 0.793,\n",
       " 0.553,\n",
       " 0.773,\n",
       " 0.593,\n",
       " 0.596,\n",
       " 0.76,\n",
       " 0.76,\n",
       " 0.608,\n",
       " 0.728,\n",
       " 0.702,\n",
       " 0.734,\n",
       " 0.496,\n",
       " 0.59,\n",
       " 0.736,\n",
       " 0.694,\n",
       " 0.767,\n",
       " 0.711,\n",
       " 0.717,\n",
       " 0.633,\n",
       " 0.682,\n",
       " 0.865,\n",
       " 0.708,\n",
       " 0.662,\n",
       " 0.821,\n",
       " 0.63,\n",
       " 0.689,\n",
       " 0.759,\n",
       " 0.737,\n",
       " 0.601,\n",
       " 0.561,\n",
       " 0.761,\n",
       " 0.755,\n",
       " 0.594,\n",
       " 0.784,\n",
       " 0.681,\n",
       " 0.594,\n",
       " 0.686,\n",
       " 0.737,\n",
       " 0.756,\n",
       " 0.626,\n",
       " 0.746,\n",
       " 0.73,\n",
       " 0.693,\n",
       " 0.798,\n",
       " 0.659,\n",
       " 0.529,\n",
       " 0.583,\n",
       " 0.789,\n",
       " 0.757,\n",
       " 0.766,\n",
       " 0.768,\n",
       " 0.758,\n",
       " 0.671,\n",
       " 0.688,\n",
       " 0.705,\n",
       " 0.698,\n",
       " 0.725,\n",
       " 0.732,\n",
       " 0.772,\n",
       " 0.698,\n",
       " 0.734,\n",
       " 0.748,\n",
       " 0.663,\n",
       " 0.764,\n",
       " 0.573,\n",
       " 0.593,\n",
       " 0.665,\n",
       " 0.691,\n",
       " 0.601,\n",
       " 0.608,\n",
       " 0.601,\n",
       " 0.728,\n",
       " 0.642,\n",
       " 0.796,\n",
       " 0.707,\n",
       " 0.63,\n",
       " 0.772,\n",
       " 0.723,\n",
       " 0.632,\n",
       " 0.653,\n",
       " 0.793,\n",
       " 0.737,\n",
       " 0.687,\n",
       " 0.673,\n",
       " 0.716,\n",
       " 0.745,\n",
       " 0.559,\n",
       " 0.662,\n",
       " 0.72,\n",
       " 0.609,\n",
       " 0.788,\n",
       " 0.707,\n",
       " 0.689,\n",
       " 0.75,\n",
       " 0.639,\n",
       " 0.613,\n",
       " 0.671,\n",
       " 0.786,\n",
       " 0.656,\n",
       " 0.522,\n",
       " 0.894,\n",
       " 0.687,\n",
       " 0.565,\n",
       " 0.581,\n",
       " 0.776,\n",
       " 0.683,\n",
       " 0.684,\n",
       " 0.638,\n",
       " 0.695,\n",
       " 0.542,\n",
       " 0.676,\n",
       " 0.797,\n",
       " 0.646,\n",
       " 0.62,\n",
       " 0.69,\n",
       " 0.723,\n",
       " 0.693,\n",
       " 0.797,\n",
       " 0.727,\n",
       " 0.596,\n",
       " 0.706,\n",
       " 0.688,\n",
       " 0.657,\n",
       " 0.763,\n",
       " 0.565,\n",
       " 0.791,\n",
       " 0.643,\n",
       " 0.732,\n",
       " 0.724,\n",
       " 0.752,\n",
       " 0.829,\n",
       " 0.647,\n",
       " 0.676,\n",
       " 0.794,\n",
       " 0.752,\n",
       " 0.688,\n",
       " 0.664,\n",
       " 0.648,\n",
       " 0.698,\n",
       " 0.625,\n",
       " 0.681,\n",
       " 0.633,\n",
       " 0.627,\n",
       " 0.601,\n",
       " 0.765,\n",
       " 0.667,\n",
       " 0.709,\n",
       " 0.642,\n",
       " 0.698,\n",
       " 0.69,\n",
       " 0.692,\n",
       " 0.781,\n",
       " 0.769,\n",
       " 0.583,\n",
       " 0.626,\n",
       " 0.825,\n",
       " 0.735,\n",
       " 0.668,\n",
       " 0.668,\n",
       " 0.678,\n",
       " 0.731,\n",
       " 0.698,\n",
       " 0.569,\n",
       " 0.859,\n",
       " 0.618,\n",
       " 0.683,\n",
       " 0.753,\n",
       " 0.821,\n",
       " 0.643,\n",
       " 0.868,\n",
       " 0.699,\n",
       " 0.525,\n",
       " 0.751,\n",
       " 0.715,\n",
       " 0.607,\n",
       " 0.606,\n",
       " 0.742,\n",
       " 0.723,\n",
       " 0.648,\n",
       " 0.721,\n",
       " 0.623,\n",
       " 0.825,\n",
       " 0.692,\n",
       " 0.714,\n",
       " 0.731,\n",
       " 0.739,\n",
       " 0.725,\n",
       " 0.739,\n",
       " 0.815,\n",
       " 0.722,\n",
       " 0.62,\n",
       " 0.765,\n",
       " 0.714,\n",
       " 0.769,\n",
       " 0.763,\n",
       " 0.747,\n",
       " 0.731,\n",
       " 0.721,\n",
       " 0.602,\n",
       " 0.802,\n",
       " 0.697,\n",
       " 0.709,\n",
       " 0.757,\n",
       " 0.639,\n",
       " 0.743,\n",
       " 0.663,\n",
       " 0.792,\n",
       " 0.672,\n",
       " 0.722,\n",
       " 0.645,\n",
       " 0.761,\n",
       " 0.694,\n",
       " 0.604,\n",
       " 0.822,\n",
       " 0.793,\n",
       " 0.591,\n",
       " 0.654,\n",
       " 0.679,\n",
       " 0.634,\n",
       " 0.628,\n",
       " 0.677,\n",
       " 0.74,\n",
       " 0.752,\n",
       " 0.713,\n",
       " 0.681,\n",
       " 0.655,\n",
       " 0.611,\n",
       " 0.676,\n",
       " 0.613,\n",
       " 0.857,\n",
       " 0.716,\n",
       " 0.771,\n",
       " 0.74,\n",
       " 0.679,\n",
       " 0.648,\n",
       " 0.716,\n",
       " 0.779,\n",
       " 0.665,\n",
       " 0.74,\n",
       " 0.657,\n",
       " 0.859,\n",
       " 0.609,\n",
       " 0.645,\n",
       " 0.724,\n",
       " 0.675,\n",
       " 0.645,\n",
       " 0.739,\n",
       " 0.784,\n",
       " 0.615,\n",
       " 0.613,\n",
       " 0.559,\n",
       " 0.56,\n",
       " 0.713,\n",
       " 0.68,\n",
       " 0.726,\n",
       " 0.768,\n",
       " 0.703,\n",
       " 0.812,\n",
       " 0.667,\n",
       " 0.631,\n",
       " 0.742,\n",
       " 0.8,\n",
       " 0.644,\n",
       " 0.831,\n",
       " 0.658,\n",
       " 0.579,\n",
       " 0.805,\n",
       " 0.739,\n",
       " 0.726,\n",
       " 0.728,\n",
       " 0.715,\n",
       " 0.592,\n",
       " 0.702,\n",
       " 0.709,\n",
       " 0.707,\n",
       " 0.646,\n",
       " 0.51,\n",
       " 0.693,\n",
       " 0.631,\n",
       " 0.754,\n",
       " 0.748,\n",
       " 0.659,\n",
       " 0.723,\n",
       " 0.66,\n",
       " 0.726,\n",
       " 0.688,\n",
       " 0.693,\n",
       " 0.748,\n",
       " 0.629,\n",
       " 0.69,\n",
       " 0.516,\n",
       " 0.831,\n",
       " 0.798,\n",
       " 0.719,\n",
       " 0.584,\n",
       " 0.663,\n",
       " 0.73,\n",
       " 0.723,\n",
       " 0.629,\n",
       " 0.858,\n",
       " 0.625,\n",
       " 0.719,\n",
       " 0.673,\n",
       " 0.659,\n",
       " 0.757,\n",
       " 0.6,\n",
       " 0.632,\n",
       " 0.635,\n",
       " 0.639,\n",
       " 0.713,\n",
       " 0.654,\n",
       " 0.737,\n",
       " 0.784,\n",
       " 0.828,\n",
       " 0.587,\n",
       " 0.543,\n",
       " 0.673,\n",
       " 0.764,\n",
       " 0.746,\n",
       " 0.698,\n",
       " 0.72,\n",
       " 0.583,\n",
       " 0.686,\n",
       " 0.642,\n",
       " 0.724,\n",
       " 0.686,\n",
       " 0.779,\n",
       " 0.725,\n",
       " 0.775,\n",
       " 0.757,\n",
       " 0.661,\n",
       " 0.786,\n",
       " 0.818,\n",
       " 0.643,\n",
       " 0.633,\n",
       " 0.667,\n",
       " 0.565,\n",
       " 0.668,\n",
       " 0.732,\n",
       " 0.817,\n",
       " 0.825,\n",
       " 0.724,\n",
       " 0.656,\n",
       " 0.807,\n",
       " 0.762,\n",
       " 0.725,\n",
       " 0.713,\n",
       " 0.686,\n",
       " 0.767,\n",
       " 0.646,\n",
       " 0.639,\n",
       " 0.559,\n",
       " 0.708,\n",
       " 0.641,\n",
       " 0.691,\n",
       " 0.627,\n",
       " 0.586,\n",
       " 0.737,\n",
       " 0.749,\n",
       " 0.792,\n",
       " 0.601,\n",
       " 0.667,\n",
       " 0.787,\n",
       " 0.767,\n",
       " 0.655,\n",
       " 0.723,\n",
       " 0.752,\n",
       " 0.675,\n",
       " 0.658,\n",
       " 0.713,\n",
       " 0.74,\n",
       " 0.56,\n",
       " 0.716,\n",
       " 0.672,\n",
       " 0.697,\n",
       " 0.691,\n",
       " 0.752,\n",
       " 0.613,\n",
       " 0.703,\n",
       " 0.743,\n",
       " 0.802,\n",
       " 0.643,\n",
       " 0.756,\n",
       " 0.737,\n",
       " 0.683,\n",
       " 0.648,\n",
       " 0.694,\n",
       " 0.698,\n",
       " ...]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neutral_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1664002900807,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "xXblLGQzN6HY",
    "outputId": "c63d0ea7-9af1-4b08-9cd2-18425fb14f8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9867,\n",
       " 0.9897,\n",
       " 0.986,\n",
       " 0.9524,\n",
       " 0.9873,\n",
       " 0.9935,\n",
       " 0.9977,\n",
       " 0.9964,\n",
       " 0.9484,\n",
       " 0.9861,\n",
       " 0.9976,\n",
       " 0.9952,\n",
       " 0.9875,\n",
       " 0.9955,\n",
       " 0.9834,\n",
       " 0.9959,\n",
       " 0.9899,\n",
       " 0.995,\n",
       " 0.9946,\n",
       " 0.9859,\n",
       " 0.9954,\n",
       " 0.9931,\n",
       " 0.99,\n",
       " 0.9841,\n",
       " 0.9968,\n",
       " 0.9862,\n",
       " 0.9877,\n",
       " 0.9686,\n",
       " 0.9916,\n",
       " 0.9299,\n",
       " 0.994,\n",
       " 0.9938,\n",
       " 0.9853,\n",
       " 0.9896,\n",
       " 0.9967,\n",
       " 0.9977,\n",
       " 0.9975,\n",
       " 0.9941,\n",
       " 0.9888,\n",
       " 0.9938,\n",
       " 0.987,\n",
       " 0.9901,\n",
       " 0.9776,\n",
       " 0.9771,\n",
       " 0.9904,\n",
       " 0.7269,\n",
       " 0.9731,\n",
       " 0.9861,\n",
       " 0.9837,\n",
       " 0.9866,\n",
       " 0.9958,\n",
       " 0.9873,\n",
       " 0.9885,\n",
       " 0.9938,\n",
       " 0.9936,\n",
       " 0.9821,\n",
       " 0.9756,\n",
       " 0.8316,\n",
       " 0.9916,\n",
       " 0.9675,\n",
       " 0.9958,\n",
       " 0.9856,\n",
       " 0.9694,\n",
       " 0.9917,\n",
       " 0.9894,\n",
       " 0.9935,\n",
       " 0.9958,\n",
       " 0.9794,\n",
       " 0.998,\n",
       " 0.9958,\n",
       " 0.9694,\n",
       " 0.9922,\n",
       " 0.9951,\n",
       " 0.9904,\n",
       " 0.9873,\n",
       " 0.9937,\n",
       " 0.9953,\n",
       " 0.9901,\n",
       " 0.9399,\n",
       " 0.8913,\n",
       " 0.9586,\n",
       " 0.9423,\n",
       " 0.9971,\n",
       " -0.0299,\n",
       " 0.9931,\n",
       " 0.9912,\n",
       " 0.9955,\n",
       " 0.9962,\n",
       " 0.9862,\n",
       " 0.9079,\n",
       " 0.9964,\n",
       " 0.9643,\n",
       " 0.9793,\n",
       " 0.9802,\n",
       " 0.9959,\n",
       " 0.9858,\n",
       " 0.9172,\n",
       " 0.967,\n",
       " 0.9889,\n",
       " 0.9774,\n",
       " 0.9954,\n",
       " 0.9871,\n",
       " 0.9917,\n",
       " 0.9709,\n",
       " 0.9908,\n",
       " 0.9517,\n",
       " 0.996,\n",
       " 0.9909,\n",
       " 0.9646,\n",
       " 0.9888,\n",
       " 0.9952,\n",
       " 0.7103,\n",
       " 0.9896,\n",
       " 0.996,\n",
       " 0.9951,\n",
       " 0.9929,\n",
       " 0.9729,\n",
       " 0.9981,\n",
       " 0.9903,\n",
       " 0.9831,\n",
       " 0.9888,\n",
       " 0.981,\n",
       " 0.9898,\n",
       " 0.9892,\n",
       " 0.995,\n",
       " 0.9842,\n",
       " 0.9889,\n",
       " 0.9922,\n",
       " 0.9959,\n",
       " 0.9169,\n",
       " 0.1603,\n",
       " 0.9952,\n",
       " 0.9788,\n",
       " 0.9753,\n",
       " 0.9877,\n",
       " 0.9894,\n",
       " 0.9819,\n",
       " 0.9977,\n",
       " 0.9938,\n",
       " 0.9852,\n",
       " 0.8934,\n",
       " 0.9952,\n",
       " 0.9753,\n",
       " 0.9885,\n",
       " 0.9888,\n",
       " 0.994,\n",
       " 0.9937,\n",
       " 0.9794,\n",
       " 0.9263,\n",
       " 0.9837,\n",
       " 0.9029,\n",
       " 0.9832,\n",
       " 0.987,\n",
       " 0.9842,\n",
       " 0.9538,\n",
       " 0.9953,\n",
       " 0.9579,\n",
       " 0.9878,\n",
       " 0.9643,\n",
       " 0.9952,\n",
       " 0.9616,\n",
       " 0.9959,\n",
       " 0.9966,\n",
       " 0.9961,\n",
       " 0.9968,\n",
       " 0.977,\n",
       " 0.4738,\n",
       " 0.9943,\n",
       " 0.9853,\n",
       " 0.9898,\n",
       " 0.9726,\n",
       " 0.9977,\n",
       " 0.9876,\n",
       " 0.9584,\n",
       " 0.836,\n",
       " 0.8839,\n",
       " 0.9918,\n",
       " 0.9716,\n",
       " 0.9928,\n",
       " 0.9517,\n",
       " 0.9958,\n",
       " 0.9796,\n",
       " 0.9959,\n",
       " 0.9753,\n",
       " 0.9961,\n",
       " 0.9864,\n",
       " 0.9582,\n",
       " 0.9828,\n",
       " 0.9948,\n",
       " 0.997,\n",
       " 0.9792,\n",
       " 0.9866,\n",
       " 0.9879,\n",
       " 0.9965,\n",
       " 0.9559,\n",
       " 0.9485,\n",
       " 0.9967,\n",
       " 0.9493,\n",
       " 0.9855,\n",
       " 0.9925,\n",
       " 0.9939,\n",
       " 0.9339,\n",
       " 0.9961,\n",
       " 0.9927,\n",
       " 0.9972,\n",
       " 0.9946,\n",
       " 0.9888,\n",
       " 0.9831,\n",
       " 0.9601,\n",
       " 0.9985,\n",
       " 0.9133,\n",
       " 0.9956,\n",
       " 0.9733,\n",
       " 0.9901,\n",
       " 0.8803,\n",
       " 0.985,\n",
       " 0.9957,\n",
       " 0.9953,\n",
       " 0.9031,\n",
       " 0.9535,\n",
       " 0.9798,\n",
       " 0.9909,\n",
       " 0.99,\n",
       " 0.9928,\n",
       " 0.765,\n",
       " 0.9838,\n",
       " 0.9899,\n",
       " 0.9787,\n",
       " 0.9716,\n",
       " 0.9964,\n",
       " 0.9628,\n",
       " 0.9855,\n",
       " 0.9974,\n",
       " 0.9977,\n",
       " 0.4791,\n",
       " 0.9948,\n",
       " 0.9971,\n",
       " 0.9974,\n",
       " 0.9919,\n",
       " 0.9905,\n",
       " 0.9912,\n",
       " 0.9825,\n",
       " 0.9955,\n",
       " 0.9967,\n",
       " 0.9571,\n",
       " 0.9915,\n",
       " 0.9939,\n",
       " 0.9856,\n",
       " 0.9889,\n",
       " 0.9983,\n",
       " 0.982,\n",
       " 0.8565,\n",
       " 0.995,\n",
       " 0.9973,\n",
       " 0.9896,\n",
       " 0.9971,\n",
       " 0.9902,\n",
       " 0.959,\n",
       " 0.8626,\n",
       " 0.9611,\n",
       " 0.9239,\n",
       " 0.9716,\n",
       " 0.9803,\n",
       " 0.9816,\n",
       " 0.985,\n",
       " 0.9966,\n",
       " 0.9785,\n",
       " 0.8413,\n",
       " 0.9954,\n",
       " 0.9952,\n",
       " 0.9869,\n",
       " 0.9941,\n",
       " 0.962,\n",
       " 0.9968,\n",
       " 0.9895,\n",
       " 0.9974,\n",
       " 0.9961,\n",
       " 0.9969,\n",
       " 0.9823,\n",
       " 0.9874,\n",
       " 0.9825,\n",
       " 0.9781,\n",
       " 0.9982,\n",
       " 0.9929,\n",
       " 0.9985,\n",
       " 0.9931,\n",
       " 0.9938,\n",
       " 0.9909,\n",
       " 0.9954,\n",
       " 0.9837,\n",
       " 0.9879,\n",
       " 0.9814,\n",
       " 0.994,\n",
       " 0.9925,\n",
       " 0.9645,\n",
       " 0.9935,\n",
       " 0.9922,\n",
       " 0.9645,\n",
       " 0.5267,\n",
       " 0.9941,\n",
       " 0.9779,\n",
       " 0.8901,\n",
       " 0.9925,\n",
       " 0.9659,\n",
       " 0.9935,\n",
       " 0.9925,\n",
       " 0.8282,\n",
       " 0.9947,\n",
       " 0.8999,\n",
       " 0.9884,\n",
       " 0.9954,\n",
       " 0.9814,\n",
       " 0.9939,\n",
       " 0.9961,\n",
       " 0.9934,\n",
       " 0.9915,\n",
       " 0.9895,\n",
       " 0.9512,\n",
       " 0.9843,\n",
       " 0.9948,\n",
       " 0.9231,\n",
       " 0.9945,\n",
       " 0.8456,\n",
       " 0.995,\n",
       " 0.9732,\n",
       " 0.9906,\n",
       " 0.9791,\n",
       " 0.9917,\n",
       " 0.9442,\n",
       " 0.9961,\n",
       " 0.9823,\n",
       " 0.9902,\n",
       " 0.9858,\n",
       " 0.9578,\n",
       " 0.9879,\n",
       " 0.9423,\n",
       " 0.9871,\n",
       " 0.969,\n",
       " -0.4434,\n",
       " 0.9839,\n",
       " 0.8136,\n",
       " -0.8481,\n",
       " 0.9873,\n",
       " 0.9918,\n",
       " 0.9858,\n",
       " 0.9955,\n",
       " 0.9955,\n",
       " 0.9931,\n",
       " 0.9652,\n",
       " 0.9922,\n",
       " 0.9967,\n",
       " 0.9968,\n",
       " 0.9771,\n",
       " 0.9908,\n",
       " 0.9976,\n",
       " 0.9847,\n",
       " 0.9867,\n",
       " 0.9943,\n",
       " 0.9728,\n",
       " 0.8225,\n",
       " 0.9964,\n",
       " 0.9708,\n",
       " 0.97,\n",
       " 0.9814,\n",
       " 0.743,\n",
       " 0.9707,\n",
       " 0.9899,\n",
       " 0.9791,\n",
       " 0.9056,\n",
       " 0.951,\n",
       " 0.9861,\n",
       " 0.7986,\n",
       " 0.9951,\n",
       " 0.9955,\n",
       " 0.9948,\n",
       " 0.9753,\n",
       " 0.9951,\n",
       " 0.1027,\n",
       " 0.9916,\n",
       " 0.9944,\n",
       " 0.9826,\n",
       " 0.9926,\n",
       " 0.9804,\n",
       " 0.9953,\n",
       " 0.9876,\n",
       " 0.8121,\n",
       " 0.9962,\n",
       " 0.9136,\n",
       " 0.9701,\n",
       " 0.9884,\n",
       " 0.9826,\n",
       " 0.9964,\n",
       " 0.9783,\n",
       " 0.9902,\n",
       " 0.9731,\n",
       " 0.9991,\n",
       " 0.9628,\n",
       " 0.982,\n",
       " 0.9699,\n",
       " 0.9938,\n",
       " 0.9337,\n",
       " 0.9849,\n",
       " 0.9966,\n",
       " 0.9246,\n",
       " 0.9979,\n",
       " 0.9918,\n",
       " 0.9897,\n",
       " 0.9883,\n",
       " 0.9451,\n",
       " 0.9879,\n",
       " 0.9795,\n",
       " 0.9217,\n",
       " 0.9956,\n",
       " 0.9977,\n",
       " 0.9933,\n",
       " 0.9895,\n",
       " 0.9781,\n",
       " 0.9956,\n",
       " 0.9977,\n",
       " 0.9963,\n",
       " 0.9975,\n",
       " 0.9957,\n",
       " 0.9759,\n",
       " 0.9142,\n",
       " 0.9875,\n",
       " 0.9894,\n",
       " 0.9914,\n",
       " 0.978,\n",
       " 0.9926,\n",
       " 0.9939,\n",
       " 0.9667,\n",
       " 0.9818,\n",
       " 0.9929,\n",
       " 0.9959,\n",
       " 0.9853,\n",
       " 0.9967,\n",
       " 0.9853,\n",
       " 0.9964,\n",
       " 0.9893,\n",
       " 0.9856,\n",
       " 0.9859,\n",
       " 0.9961,\n",
       " 0.8718,\n",
       " 0.9968,\n",
       " 0.9939,\n",
       " 0.9933,\n",
       " 0.9961,\n",
       " 0.9905,\n",
       " 0.9945,\n",
       " 0.9943,\n",
       " 0.9836,\n",
       " 0.9854,\n",
       " 0.9929,\n",
       " 0.9948,\n",
       " 0.9853,\n",
       " 0.9984,\n",
       " 0.9485,\n",
       " 0.9978,\n",
       " 0.982,\n",
       " 0.9496,\n",
       " 0.9822,\n",
       " 0.9891,\n",
       " 0.7875,\n",
       " 0.9854,\n",
       " 0.9958,\n",
       " 0.9887,\n",
       " 0.9934,\n",
       " 0.9891,\n",
       " 0.9902,\n",
       " 0.9879,\n",
       " 0.9771,\n",
       " 0.997,\n",
       " 0.9957,\n",
       " 0.9983,\n",
       " 0.9881,\n",
       " 0.9963,\n",
       " 0.9844,\n",
       " 0.9917,\n",
       " 0.9803,\n",
       " 0.9756,\n",
       " 0.9915,\n",
       " 0.991,\n",
       " 0.9835,\n",
       " 0.9947,\n",
       " 0.9895,\n",
       " 0.9952,\n",
       " 0.9979,\n",
       " 0.9891,\n",
       " 0.9938,\n",
       " 0.8747,\n",
       " 0.9984,\n",
       " 0.7721,\n",
       " 0.9872,\n",
       " 0.9911,\n",
       " 0.9578,\n",
       " 0.9803,\n",
       " 0.9914,\n",
       " 0.9976,\n",
       " 0.9965,\n",
       " 0.9908,\n",
       " 0.99,\n",
       " 0.9639,\n",
       " 0.9719,\n",
       " 0.9898,\n",
       " 0.9542,\n",
       " 0.99,\n",
       " 0.9942,\n",
       " 0.9953,\n",
       " 0.9964,\n",
       " 0.9806,\n",
       " 0.9925,\n",
       " 0.9733,\n",
       " 0.9879,\n",
       " 0.9888,\n",
       " 0.9882,\n",
       " 0.9879,\n",
       " 0.9968,\n",
       " 0.9971,\n",
       " 0.8491,\n",
       " 0.9206,\n",
       " 0.9839,\n",
       " 0.9719,\n",
       " 0.9786,\n",
       " 0.9816,\n",
       " 0.9889,\n",
       " 0.9842,\n",
       " 0.9732,\n",
       " 0.9954,\n",
       " 0.9953,\n",
       " 0.9458,\n",
       " 0.9939,\n",
       " 0.908,\n",
       " 0.9972,\n",
       " 0.9929,\n",
       " 0.9924,\n",
       " 0.9826,\n",
       " 0.9828,\n",
       " 0.9932,\n",
       " 0.9864,\n",
       " 0.9976,\n",
       " 0.9816,\n",
       " 0.9522,\n",
       " 0.9975,\n",
       " 0.9692,\n",
       " 0.7938,\n",
       " 0.9798,\n",
       " 0.9952,\n",
       " 0.9953,\n",
       " 0.9429,\n",
       " 0.9889,\n",
       " 0.9941,\n",
       " 0.9822,\n",
       " 0.9501,\n",
       " 0.9246,\n",
       " 0.999,\n",
       " 0.9919,\n",
       " 0.9946,\n",
       " 0.9956,\n",
       " 0.9926,\n",
       " 0.9943,\n",
       " 0.9926,\n",
       " 0.9971,\n",
       " 0.9927,\n",
       " 0.9892,\n",
       " 0.5994,\n",
       " 0.9889,\n",
       " 0.9973,\n",
       " 0.9911,\n",
       " 0.9957,\n",
       " 0.9889,\n",
       " 0.9911,\n",
       " 0.9963,\n",
       " 0.996,\n",
       " 0.994,\n",
       " 0.9927,\n",
       " 0.9916,\n",
       " 0.9138,\n",
       " 0.9886,\n",
       " 0.9897,\n",
       " 0.9875,\n",
       " 0.9923,\n",
       " 0.9889,\n",
       " 0.9628,\n",
       " 0.9909,\n",
       " 0.9947,\n",
       " 0.9894,\n",
       " 0.974,\n",
       " 0.9965,\n",
       " 0.9423,\n",
       " 0.9931,\n",
       " 0.9237,\n",
       " 0.9957,\n",
       " 0.9781,\n",
       " 0.8555,\n",
       " 0.9909,\n",
       " 0.9965,\n",
       " 0.9925,\n",
       " 0.9571,\n",
       " 0.9769,\n",
       " 0.9652,\n",
       " 0.8934,\n",
       " 0.972,\n",
       " 0.0067,\n",
       " 0.6621,\n",
       " 0.9918,\n",
       " 0.9953,\n",
       " 0.9774,\n",
       " 0.9915,\n",
       " 0.9944,\n",
       " 0.9952,\n",
       " 0.9862,\n",
       " 0.9887,\n",
       " 0.9864,\n",
       " 0.9944,\n",
       " 0.9448,\n",
       " 0.9929,\n",
       " 0.9846,\n",
       " 0.975,\n",
       " 0.9743,\n",
       " 0.9962,\n",
       " 0.9923,\n",
       " 0.9686,\n",
       " 0.991,\n",
       " 0.9979,\n",
       " 0.9975,\n",
       " 0.9881,\n",
       " 0.9836,\n",
       " 0.969,\n",
       " 0.9886,\n",
       " 0.958,\n",
       " 0.9957,\n",
       " 0.9842,\n",
       " 0.8555,\n",
       " 0.994,\n",
       " 0.9948,\n",
       " 0.9552,\n",
       " 0.9826,\n",
       " 0.9946,\n",
       " 0.9847,\n",
       " 0.9939,\n",
       " 0.9955,\n",
       " 0.9979,\n",
       " 0.9873,\n",
       " 0.9485,\n",
       " 0.9952,\n",
       " 0.968,\n",
       " 0.9855,\n",
       " 0.9983,\n",
       " 0.9933,\n",
       " 0.9911,\n",
       " 0.9886,\n",
       " 0.996,\n",
       " 0.9917,\n",
       " 0.9825,\n",
       " 0.9939,\n",
       " -0.2263,\n",
       " 0.9953,\n",
       " 0.9941,\n",
       " 0.9972,\n",
       " 0.9892,\n",
       " 0.9557,\n",
       " 0.9349,\n",
       " 0.9231,\n",
       " 0.9764,\n",
       " 0.9909,\n",
       " 0.9161,\n",
       " 0.9778,\n",
       " 0.9668,\n",
       " 0.9753,\n",
       " 0.9928,\n",
       " 0.9897,\n",
       " 0.9963,\n",
       " 0.9324,\n",
       " 0.9842,\n",
       " 0.9819,\n",
       " 0.9879,\n",
       " 0.9933,\n",
       " 0.9931,\n",
       " 0.998,\n",
       " 0.9851,\n",
       " 0.9953,\n",
       " 0.9928,\n",
       " 0.994,\n",
       " 0.9899,\n",
       " 0.9809,\n",
       " 0.9877,\n",
       " 0.9468,\n",
       " 0.993,\n",
       " 0.9726,\n",
       " 0.9959,\n",
       " 0.9967,\n",
       " 0.9979,\n",
       " 0.946,\n",
       " 0.9803,\n",
       " 0.9961,\n",
       " 0.9834,\n",
       " 0.9969,\n",
       " 0.994,\n",
       " 0.9927,\n",
       " 0.9949,\n",
       " 0.9694,\n",
       " 0.9934,\n",
       " 0.9862,\n",
       " 0.9931,\n",
       " 0.9924,\n",
       " 0.9785,\n",
       " 0.989,\n",
       " 0.9922,\n",
       " 0.9946,\n",
       " 0.9617,\n",
       " 0.9881,\n",
       " 0.9951,\n",
       " 0.8481,\n",
       " 0.987,\n",
       " 0.997,\n",
       " 0.9944,\n",
       " 0.9674,\n",
       " 0.9867,\n",
       " 0.9975,\n",
       " 0.9957,\n",
       " 0.9951,\n",
       " 0.9968,\n",
       " 0.9959,\n",
       " 0.9673,\n",
       " 0.9975,\n",
       " 0.9872,\n",
       " 0.987,\n",
       " 0.9876,\n",
       " 0.9952,\n",
       " 0.9834,\n",
       " 0.9858,\n",
       " 0.9971,\n",
       " 0.995,\n",
       " 0.9851,\n",
       " 0.9928,\n",
       " 0.9595,\n",
       " 0.9994,\n",
       " 0.9919,\n",
       " 0.9853,\n",
       " 0.9882,\n",
       " 0.9871,\n",
       " 0.9925,\n",
       " 0.9719,\n",
       " 0.9976,\n",
       " 0.9898,\n",
       " 0.9779,\n",
       " 0.9829,\n",
       " 0.9967,\n",
       " 0.9546,\n",
       " 0.9966,\n",
       " 0.9825,\n",
       " 0.9974,\n",
       " 0.9948,\n",
       " 0.9826,\n",
       " 0.9956,\n",
       " 0.9976,\n",
       " 0.9036,\n",
       " 0.9953,\n",
       " 0.9827,\n",
       " 0.9897,\n",
       " 0.9935,\n",
       " 0.9916,\n",
       " 0.9753,\n",
       " 0.9793,\n",
       " 0.9829,\n",
       " 0.9968,\n",
       " 0.9979,\n",
       " 0.9451,\n",
       " 0.922,\n",
       " 0.9773,\n",
       " 0.9932,\n",
       " 0.9693,\n",
       " 0.9769,\n",
       " 0.9956,\n",
       " 0.9954,\n",
       " 0.3641,\n",
       " 0.9953,\n",
       " 0.9867,\n",
       " 0.9921,\n",
       " 0.9772,\n",
       " 0.9981,\n",
       " 0.9633,\n",
       " 0.9972,\n",
       " 0.9583,\n",
       " 0.9916,\n",
       " 0.9949,\n",
       " 0.9984,\n",
       " 0.9509,\n",
       " 0.981,\n",
       " 0.9949,\n",
       " 0.9883,\n",
       " 0.9771,\n",
       " 0.9951,\n",
       " 0.9655,\n",
       " 0.9949,\n",
       " 0.9805,\n",
       " 0.9965,\n",
       " 0.8481,\n",
       " 0.9843,\n",
       " 0.9866,\n",
       " 0.9081,\n",
       " 0.9755,\n",
       " 0.9862,\n",
       " 0.9913,\n",
       " 0.9972,\n",
       " 0.9938,\n",
       " 0.9702,\n",
       " 0.9871,\n",
       " 0.9952,\n",
       " 0.9968,\n",
       " 0.9943,\n",
       " 0.8692,\n",
       " 0.9712,\n",
       " 0.9961,\n",
       " 0.9931,\n",
       " 0.9828,\n",
       " 0.9813,\n",
       " 0.985,\n",
       " 0.9747,\n",
       " 0.9896,\n",
       " 0.9956,\n",
       " 0.994,\n",
       " 0.9939,\n",
       " 0.9956,\n",
       " 0.9963,\n",
       " 0.9607,\n",
       " 0.9931,\n",
       " 0.9936,\n",
       " 0.9535,\n",
       " 0.9923,\n",
       " 0.9957,\n",
       " 0.9868,\n",
       " 0.9974,\n",
       " 0.9798,\n",
       " 0.9925,\n",
       " 0.9744,\n",
       " 0.9978,\n",
       " 0.9936,\n",
       " 0.9954,\n",
       " 0.9678,\n",
       " 0.9917,\n",
       " 0.9217,\n",
       " 0.9754,\n",
       " 0.9776,\n",
       " 0.9653,\n",
       " 0.9972,\n",
       " 0.9958,\n",
       " 0.984,\n",
       " 0.9921,\n",
       " 0.9829,\n",
       " 0.9859,\n",
       " 0.9976,\n",
       " 0.9657,\n",
       " 0.9899,\n",
       " 0.9934,\n",
       " 0.9686,\n",
       " 0.994,\n",
       " 0.9966,\n",
       " 0.9925,\n",
       " 0.9535,\n",
       " 0.9969,\n",
       " 0.995,\n",
       " 0.9981,\n",
       " 0.9953,\n",
       " 0.9818,\n",
       " 0.9889,\n",
       " 0.9595,\n",
       " 0.9802,\n",
       " 0.9876,\n",
       " 0.9186,\n",
       " 0.994,\n",
       " 0.9869,\n",
       " 0.991,\n",
       " 0.9584,\n",
       " 0.9904,\n",
       " 0.9494,\n",
       " 0.996,\n",
       " 0.9938,\n",
       " 0.9716,\n",
       " 0.9927,\n",
       " 0.993,\n",
       " 0.9706,\n",
       " 0.9941,\n",
       " 0.9915,\n",
       " 0.9839,\n",
       " 0.9905,\n",
       " 0.9568,\n",
       " 0.9908,\n",
       " 0.9977,\n",
       " 0.9898,\n",
       " 0.9963,\n",
       " 0.9891,\n",
       " 0.9905,\n",
       " 0.998,\n",
       " 0.9766,\n",
       " 0.9956,\n",
       " 0.9925,\n",
       " 0.9967,\n",
       " 0.9926,\n",
       " 0.9849,\n",
       " 0.9968,\n",
       " 0.9959,\n",
       " 0.9965,\n",
       " 0.8676,\n",
       " 0.9901,\n",
       " 0.9643,\n",
       " 0.9965,\n",
       " 0.9978,\n",
       " 0.9891,\n",
       " 0.9394,\n",
       " 0.9921,\n",
       " 0.9526,\n",
       " 0.9981,\n",
       " 0.9813,\n",
       " 0.9493,\n",
       " 0.9907,\n",
       " 0.9854,\n",
       " 0.9981,\n",
       " 0.9819,\n",
       " 0.9984,\n",
       " 0.9984,\n",
       " 0.9017,\n",
       " 0.9883,\n",
       " 0.9725,\n",
       " 0.9686,\n",
       " 0.9437,\n",
       " 0.9924,\n",
       " 0.9897,\n",
       " 0.994,\n",
       " 0.9806,\n",
       " 0.9393,\n",
       " 0.9886,\n",
       " 0.9751,\n",
       " 0.9989,\n",
       " 0.9914,\n",
       " 0.9726,\n",
       " 0.9939,\n",
       " 0.991,\n",
       " 0.966,\n",
       " 0.9806,\n",
       " 0.9878,\n",
       " 0.9559,\n",
       " 0.9677,\n",
       " 0.9919,\n",
       " 0.9201,\n",
       " 0.9924,\n",
       " 0.9874,\n",
       " 0.9942,\n",
       " 0.9965,\n",
       " 0.9729,\n",
       " 0.9842,\n",
       " 0.2782,\n",
       " 0.9638,\n",
       " 0.9899,\n",
       " 0.9961,\n",
       " 0.9771,\n",
       " 0.9611,\n",
       " 0.9958,\n",
       " 0.9925,\n",
       " 0.7452,\n",
       " 0.9866,\n",
       " 0.9956,\n",
       " 0.9964,\n",
       " 0.9955,\n",
       " 0.9867,\n",
       " 0.995,\n",
       " 0.995,\n",
       " 0.9929,\n",
       " 0.9984,\n",
       " 0.9808,\n",
       " 0.9477,\n",
       " 0.9783,\n",
       " 0.9719,\n",
       " 0.9954,\n",
       " 0.9666,\n",
       " 0.9913,\n",
       " 0.9967,\n",
       " 0.9928,\n",
       " 0.9846,\n",
       " 0.9883,\n",
       " 0.9896,\n",
       " 0.981,\n",
       " 0.957,\n",
       " 0.9965,\n",
       " 0.9938,\n",
       " 0.9976,\n",
       " 0.9957,\n",
       " 0.9872,\n",
       " 0.9754,\n",
       " 0.991,\n",
       " 0.9769,\n",
       " 0.9897,\n",
       " 0.969,\n",
       " 0.9972,\n",
       " 0.9826,\n",
       " 0.9934,\n",
       " 0.9773,\n",
       " 0.9976,\n",
       " 0.9788,\n",
       " 0.9915,\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "sHuNJdEqN6HY"
   },
   "outputs": [],
   "source": [
    "## adding these new features which were bassed on sentiment intensity analyzer \n",
    "data['negitiveWords'] = negitive_words\n",
    "data['positiveWords'] = positive_words\n",
    "data['compoundWords'] = compound_words\n",
    "data['NeutralWords'] = neutral_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1664002900808,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "lSWXva_lN6HZ",
    "outputId": "e0ee0a93-5af3-4d38-ba20-3755de10eb0d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school_state</th>\n",
       "      <th>teacher_prefix</th>\n",
       "      <th>project_grade_category</th>\n",
       "      <th>teacher_number_of_previously_posted_projects</th>\n",
       "      <th>project_is_approved</th>\n",
       "      <th>clean_categories</th>\n",
       "      <th>clean_subcategories</th>\n",
       "      <th>essay</th>\n",
       "      <th>price</th>\n",
       "      <th>negitiveWords</th>\n",
       "      <th>positiveWords</th>\n",
       "      <th>compoundWords</th>\n",
       "      <th>NeutralWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences health_lifescience</td>\n",
       "      <td>i fortunate enough use fairy tale stem kits cl...</td>\n",
       "      <td>725.05</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.9867</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ut</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>specialneeds</td>\n",
       "      <td>imagine 8 9 years old you third grade classroo...</td>\n",
       "      <td>213.03</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.9897</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>having class 24 students comes diverse learner...</td>\n",
       "      <td>329.00</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.9860</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ga</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>appliedlearning</td>\n",
       "      <td>earlydevelopment</td>\n",
       "      <td>i recently read article giving students choice...</td>\n",
       "      <td>481.04</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.9524</td>\n",
       "      <td>0.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wa</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>literacy</td>\n",
       "      <td>my students crave challenge eat obstacles brea...</td>\n",
       "      <td>17.74</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.9873</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science history_civics</td>\n",
       "      <td>mathematics socialsciences</td>\n",
       "      <td>it end school year routines run course student...</td>\n",
       "      <td>102.50</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ca</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language math_science</td>\n",
       "      <td>literacy mathematics</td>\n",
       "      <td>sitting still overrated it makes sense opera m...</td>\n",
       "      <td>1418.08</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.9977</td>\n",
       "      <td>0.577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ca</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>math_science history_civics</td>\n",
       "      <td>appliedsciences history_geography</td>\n",
       "      <td>it not enough read book write essay connect de...</td>\n",
       "      <td>495.29</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ca</td>\n",
       "      <td>ms</td>\n",
       "      <td>grades_prek_2</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "      <td>literacy_language</td>\n",
       "      <td>esl literacy</td>\n",
       "      <td>never society rapidly changed technology invad...</td>\n",
       "      <td>299.99</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hi</td>\n",
       "      <td>mrs</td>\n",
       "      <td>grades_3_5</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>math_science</td>\n",
       "      <td>appliedsciences mathematics</td>\n",
       "      <td>do remember first time saw star wars wall e ro...</td>\n",
       "      <td>479.94</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  school_state teacher_prefix project_grade_category  \\\n",
       "0           ca            mrs          grades_prek_2   \n",
       "1           ut             ms             grades_3_5   \n",
       "2           ca            mrs          grades_prek_2   \n",
       "3           ga            mrs          grades_prek_2   \n",
       "4           wa            mrs             grades_3_5   \n",
       "5           ca            mrs             grades_3_5   \n",
       "6           ca            mrs             grades_3_5   \n",
       "7           ca             ms             grades_3_5   \n",
       "8           ca             ms          grades_prek_2   \n",
       "9           hi            mrs             grades_3_5   \n",
       "\n",
       "   teacher_number_of_previously_posted_projects  project_is_approved  \\\n",
       "0                                            53                    1   \n",
       "1                                             4                    1   \n",
       "2                                            10                    1   \n",
       "3                                             2                    1   \n",
       "4                                             2                    1   \n",
       "5                                             6                    1   \n",
       "6                                             0                    1   \n",
       "7                                             0                    0   \n",
       "8                                           127                    1   \n",
       "9                                            41                    1   \n",
       "\n",
       "                 clean_categories                 clean_subcategories  \\\n",
       "0                    math_science  appliedsciences health_lifescience   \n",
       "1                    specialneeds                        specialneeds   \n",
       "2               literacy_language                            literacy   \n",
       "3                 appliedlearning                    earlydevelopment   \n",
       "4               literacy_language                            literacy   \n",
       "5     math_science history_civics          mathematics socialsciences   \n",
       "6  literacy_language math_science                literacy mathematics   \n",
       "7     math_science history_civics   appliedsciences history_geography   \n",
       "8               literacy_language                        esl literacy   \n",
       "9                    math_science         appliedsciences mathematics   \n",
       "\n",
       "                                               essay    price  negitiveWords  \\\n",
       "0  i fortunate enough use fairy tale stem kits cl...   725.05          0.013   \n",
       "1  imagine 8 9 years old you third grade classroo...   213.03          0.072   \n",
       "2  having class 24 students comes diverse learner...   329.00          0.017   \n",
       "3  i recently read article giving students choice...   481.04          0.030   \n",
       "4  my students crave challenge eat obstacles brea...    17.74          0.029   \n",
       "5  it end school year routines run course student...   102.50          0.013   \n",
       "6  sitting still overrated it makes sense opera m...  1418.08          0.019   \n",
       "7  it not enough read book write essay connect de...   495.29          0.067   \n",
       "8  never society rapidly changed technology invad...   299.99          0.063   \n",
       "9  do remember first time saw star wars wall e ro...   479.94          0.057   \n",
       "\n",
       "   positiveWords  compoundWords  NeutralWords  \n",
       "0          0.205         0.9867         0.783  \n",
       "1          0.248         0.9897         0.680  \n",
       "2          0.262         0.9860         0.721  \n",
       "3          0.187         0.9524         0.783  \n",
       "4          0.288         0.9873         0.683  \n",
       "5          0.286         0.9935         0.701  \n",
       "6          0.404         0.9977         0.577  \n",
       "7          0.302         0.9964         0.631  \n",
       "8          0.153         0.9484         0.784  \n",
       "9          0.235         0.9861         0.708  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AnioW4iQ811"
   },
   "source": [
    "## 2. Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UOaeTdIaN6HZ"
   },
   "outputs": [],
   "source": [
    "## Here we are dividing our data into two parts 1. Where our project is approved 2. Where our project is not approved\n",
    "y = data['project_is_approved'].values\n",
    "x = data.drop(['project_is_approved'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "SdcaLrNjN6Ha"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Use train test split to stratifically divide the data into two datasets - train , test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, stratify=y,random_state=42) # as we are using grid search we dont have to use cross validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1664002900810,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "yTWBoqYGN6Ha",
    "outputId": "35cf3cfc-3e63-4066-e057-40ce3f485e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73196, 12)\n",
      "(36052, 12)\n",
      "(73196,)\n",
      "(36052,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuNVPXhjRGIA"
   },
   "source": [
    "# 3. Perform TFIDF vectorization of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22408,
     "status": "ok",
     "timestamp": 1664002923209,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ao1Vy_njN6Ha",
    "outputId": "eeda2ec0-69a4-4902-a3cc-1e63f400a79d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after one hot encodig  (73196, 5000) (73196,)\n",
      "Shape of matrix after one hot encodig  (36052, 5000) (36052,)\n"
     ]
    }
   ],
   "source": [
    "# Apply TFIDF vectorization on 'Preprocessed_Essay'\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "## We will be doing TFIDF vectorization in two steps :\n",
    "## 1. We will fit the data \n",
    "## 2. We will transform the data\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "vectorizer_eassy_tfidf = TfidfVectorizer(min_df = 10 , max_features= 5000)\n",
    "vectorizer_eassy_tfidf.fit(x_train['essay'].values)\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_tfidf = vectorizer_eassy_tfidf.transform(x_train['essay'].values)\n",
    "x_test_tfidf= vectorizer_eassy_tfidf.transform(x_test['essay'].values)\n",
    "\n",
    "print(\"Shape of matrix after one hot encodig \",x_train_tfidf.shape , y_train.shape)\n",
    "print(\"Shape of matrix after one hot encodig \",x_test_tfidf.shape , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2XVFVxjRNm8"
   },
   "source": [
    "## After applying TFIDF vectorization in text data which is preprocessed essay we saw that the shape of train data is (73196 , 5000) and test data is (36052 , 5000) , I have set the max features as 5000 thats why we got 5000 values ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_KHQQ_wSHid"
   },
   "source": [
    "## 4. Perform TFIDF w2v vectorization of text data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQkyPSaHN6Hb"
   },
   "source": [
    "## Before performing tfidf w2v vectorisation of text data , we will perform average w2v vectorization on text data \n",
    "## we have done average w2v vectorization in 2 parts in genreal ,\n",
    "## In first part first we go through e very word in the sentence , then we check if that perticular word is present in list of globe vectors , if the word is present in globe vectors we will get a dense vector of the word aand assign count word to it .\n",
    "## For every word we will repeat this step and get the value of vector and count vector \n",
    "## Then we will check ether the value of count vector is zero or not , if it is not zero divide the vector to the count_word and assign this as the final vector , hence this vector is considerd as the final average w2v value ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25864,
     "status": "ok",
     "timestamp": 1664002949061,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "lIsRkEGuN6Hb",
    "outputId": "46aa91b4-b335-4fc5-d601-e8f90678e684"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 73196/73196 [00:09<00:00, 7968.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73196\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## calculate average word2vec\n",
    "## for computing average w2v for each review \n",
    "avg_w2v_vectors_train = []   ## list to store the final value of vectors as average_word_2_vector value\n",
    "for sen in tqdm(x_train['essay']):\n",
    "    vector = np.zeros(300)  ##initializing vector as zero length\n",
    "    count_word = 0 ## number of words with a valid vector in the review \n",
    "    for word in sen.split():     ## for every word in sentence \n",
    "        if word in glove_words:\n",
    "            vector = vector+model[word]\n",
    "            count_word = count_word+1\n",
    "    if count_word != 0:\n",
    "        vector = vector/count_word\n",
    "    avg_w2v_vectors_train.append(vector)\n",
    "        \n",
    "print(len(avg_w2v_vectors_train))\n",
    "print(len(avg_w2v_vectors_train[0]))\n",
    "# print(avg_w2v_vectors_train)\n",
    "# print(avg_w2v_vectors_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13228,
     "status": "ok",
     "timestamp": 1664002962261,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "GfhvGkuTN6Hc",
    "outputId": "69ea0212-60b4-4538-dff9-d302e2b0437c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 36052/36052 [00:04<00:00, 8193.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36052\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## calculate average word2vec\n",
    "## for computing average w2v for each review \n",
    "avg_w2v_vectors_test = []   ## list to store the final value of vectors as average_word_2_vector value\n",
    "for sen in tqdm(x_test['essay']):\n",
    "    vector = np.zeros(300)  ##initializing vector as zero length\n",
    "    count_word = 0 ## number of words with a valid vector in the review \n",
    "    for word in sen.split():     ## for every word in sentence \n",
    "        if word in glove_words:\n",
    "            vector = vector+model[word]\n",
    "            count_word = count_word+1\n",
    "    if count_word != 0:\n",
    "        vector = vector/count_word\n",
    "    avg_w2v_vectors_test.append(vector)\n",
    "        \n",
    "print(len(avg_w2v_vectors_test))\n",
    "print(len(avg_w2v_vectors_test[0]))\n",
    "# print(avg_w2v_vectors_test)\n",
    "# print(avg_w2v_vectors_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvtIAWndN6Hc"
   },
   "source": [
    "# 4.1 Performing tfidf w2v vectorization of text data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQzpIyBbOZDs"
   },
   "source": [
    "TFIDF w2v vectorization works in exact same way as average w2v only in case of TFIDF vectorization first we will create a set of tfidf words which will have the features name of tfidf vectorization and now we will check if our word is present i  globe vectors and in this tfidf word set , if it is present we will calculate the tfidf value for each word and get the vector value as vector value * tfidf value and we will keep on appending this value ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TLyehvHN6Hc"
   },
   "source": [
    "## 4.1.1 - Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xs7mdLh-N6Hd"
   },
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(x_train['essay'])\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 187288,
     "status": "ok",
     "timestamp": 1664003158717,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "9FivbfS7N6Hd",
    "outputId": "6cf968a1-3cf2-4eb9-ae53-13570594b071"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 73196/73196 [01:21<00:00, 900.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73196\n",
      "300\n",
      "Shape of matrix after tfidf w2v vectors (73196, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "tfidf_w2v_vectors_train = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(x_train['essay']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    tfidf_w2v_vectors_train.append(vector)\n",
    "tfidf_w2v_vectors_train = np.array( tfidf_w2v_vectors_train)\n",
    "\n",
    "print(len( tfidf_w2v_vectors_train))\n",
    "print(len( tfidf_w2v_vectors_train[0]))\n",
    "print(\"Shape of matrix after tfidf w2v vectors\" ,  tfidf_w2v_vectors_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qiR9WZEOZhaR"
   },
   "source": [
    "**After TFIDF W2V vectorization we can see the shape of our vectors are (73196 , 300) and this is for train data .**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH9Pb4eNZ5lx"
   },
   "source": [
    "## As we calulated the TFIDF word2vec value for train data same we will calculate for test data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdUUFJNvN6Hd"
   },
   "source": [
    "## 4.1.2 - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NyitSNgqN6He"
   },
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "tfidf_model = TfidfVectorizer()\n",
    "tfidf_model.fit(x_test['essay'])\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n",
    "tfidf_words = set(tfidf_model.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89738,
     "status": "ok",
     "timestamp": 1664003253575,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "0Xs9PB27N6He",
    "outputId": "cec04020-4ba2-49fd-f40e-38e504a7fd98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 36052/36052 [00:40<00:00, 898.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36052\n",
      "300\n",
      "Shape of matrix after tfidf w2v vectors (36052, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "tfidf_w2v_vectors_test = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sentence in tqdm(x_test['essay']): # for each review/sentence\n",
    "    vector = np.zeros(300) # as word vectors are of zero length\n",
    "    tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sentence.split(): # for each word in a review/sentence\n",
    "        if (word in glove_words) and (word in tfidf_words):\n",
    "            vec = model[word] # getting the vector for each word\n",
    "            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "            tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "            tf_idf_weight += tf_idf\n",
    "    if tf_idf_weight != 0:\n",
    "        vector /= tf_idf_weight\n",
    "    tfidf_w2v_vectors_test.append(vector)\n",
    "tfidf_w2v_vectors_test = np.array( tfidf_w2v_vectors_test)\n",
    "\n",
    "print(len( tfidf_w2v_vectors_test))\n",
    "print(len( tfidf_w2v_vectors_test[0]))\n",
    "print(\"Shape of matrix after tfidf w2v vectors\" ,  tfidf_w2v_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IU6PICXaH30"
   },
   "source": [
    "**We can see the shape of matrix after tfidf word2vec vectorization on test data is (36052 , 300)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jg_lF_NOaVju"
   },
   "source": [
    "# Next comes the encoding of variable which means converting the variables into numerical forms\n",
    "# The first part is Encoding Categorical features , for encoding text variables we will use Count vectorizer .\n",
    "# We have in total 5 categorical features ; School_state , Teacher_Prefix , Clean_Category , Clean_Sub_Category , project_grade_Category . we will use count vectorization in all of them and and get the values after vectorization ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4y_xobrN6Hf"
   },
   "source": [
    "# 5. perform encoding of categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refrence https://stackoverflow.com/questions/66122577/response-coding-for-categorical-data\n",
    "def response_code(feature , dataframe):\n",
    "    ## getting all the values together which means appending wether the project is approved or not in x_train as label feature \n",
    "    x_train['label'] = y_train\n",
    "    ##printing x_train and checking wether label is added or not \n",
    "#     print (x_train.head(10))\n",
    "    \n",
    "    ##counting all the values present in a feature \n",
    "    ## this count will be used in final probability calculation \n",
    "    count = x_train[feature].value_counts()\n",
    "#     print(count)\n",
    "    \n",
    "    ## differentiating values of class 0 and class 1 , count the value and get the probability \n",
    "    \n",
    "    # create a dictonary to save all the values and get its value \n",
    "    save_val = {}\n",
    "    ## for every value in total dictonary \n",
    "    for dif_val , total_count in count.items():\n",
    "        ## create a list to sore the probability \n",
    "        probability = []\n",
    "        ## calculating the probability \n",
    "        ## for calculating probabilty I am using two values val 1 which will store 0 and val 2 which will store 1 \n",
    "        for j in range(2):\n",
    "            val_1 = x_train.loc[(x_train['label'] == j-1) & (x_train[feature] == dif_val)]\n",
    "            val_2 = x_train.loc[(x_train['label'] == j) & (x_train[feature] == dif_val)]\n",
    "            ## we will use only those values which have 1 as there label value as 1 \n",
    "            prob = len(val_2)/total_count\n",
    "            ## Got the value of probability\n",
    "            probability.append(prob)\n",
    "        ## storing the probabilities of individual values in save_val dictonary\n",
    "        save_val[dif_val]=probability\n",
    "        ## printing save_val and observing the the probabilities of individual value\n",
    "#     print(save_val)\n",
    "    \n",
    "    ## creating a list to store the final response_variable \n",
    "    response_variable = [] \n",
    "    \n",
    "    ## checking if a value is present in the dataframe or not \n",
    "    for rs in dataframe[feature]:\n",
    "        if rs in dict(count).keys():\n",
    "            ## if the value is present in dataframe append it to the final response variable list \n",
    "            response_variable.append(save_val[rs])\n",
    "        else:\n",
    "            ## else append a different list \n",
    "            response_variable.append([0.3,0.03])\n",
    "    res = np.array(response_variable)\n",
    "    return res\n",
    "\n",
    "                        \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNEvrS6_N6Hf"
   },
   "source": [
    "School_State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1664003254496,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "y5vVEM1UN6Hf",
    "outputId": "fb0a814d-3360-4992-d55d-dd6ce06228d5"
   },
   "outputs": [],
   "source": [
    "# # Apply One-Hot Encoding on the categorical features either using OneHotEncoder() (or) CountVectorizer(binary=True)\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# ## We will be doing vectorization in two steps :\n",
    "# ## 1. We will fit the data \n",
    "# ## 2. We will transform the data\n",
    "\n",
    "\n",
    "\n",
    "# ## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "# vectorizer_school_state = CountVectorizer() \n",
    "# vectorizer_school_state.fit(x_train['school_state'].values)\n",
    "    \n",
    "# ## 2. Transforming the data to its vectors form \n",
    "# x_train_school_state = vectorizer_school_state.transform(x_train['school_state'].values)\n",
    "# x_test_school_state = vectorizer_school_state.transform(x_test['school_state'].values)\n",
    "\n",
    "# print(\"Shape of matrix after one hot encodig on Train Data\",x_train_school_state.shape)\n",
    "# print(\"Shape of matrix after one hot encodig on Test Data\",x_test_school_state.shape)\n",
    "\n",
    "# ## Lets get the feature names present in school state https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer#:~:text=You%20can%20use%20tfidf_vectorizer.,selected)%20from%20the%20raw%20documents.\n",
    "# print(\"Features Names are : \", vectorizer_school_state.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFNDWF77bZKL"
   },
   "source": [
    "## As we can see we have used Count_Vectorizer to fit our values and then we have transformed our data , and then we got the shape of the atrix after one hot encoding , the shape of train data  is (73196,51) and that of test data is (36052 , 51) also we printed the features name ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MY7FXbzhN6Hf"
   },
   "source": [
    "Teacher Prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1664003254497,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "rAgCIWYqN6Hg",
    "outputId": "1bbeb334-0203-48f6-9769-a77b30750574"
   },
   "outputs": [],
   "source": [
    "# # Apply One-Hot Encoding on the categorical features either using OneHotEncoder() (or) CountVectorizer(binary=True)\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# ## We will be doing vectorization in two steps :\n",
    "# ## 1. We will fit the data \n",
    "# ## 2. We will transform the data\n",
    "\n",
    "\n",
    "\n",
    "# ## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "# vectorizer_teacher_prefix = CountVectorizer() \n",
    "# vectorizer_teacher_prefix.fit(x_train['teacher_prefix'].values)\n",
    "    \n",
    "# ## 2. Transforming the data to its vectors form \n",
    "# x_train_teacher_prefix = vectorizer_teacher_prefix.transform(x_train['teacher_prefix'].values)\n",
    "# x_test_teacher_prefix = vectorizer_teacher_prefix.transform(x_test['teacher_prefix'].values)\n",
    "\n",
    "# print(\"Shape of matrix after one hot encodig on Train Data\",x_train_teacher_prefix.shape)\n",
    "# print(\"Shape of matrix after one hot encodig on Test Data\",x_test_teacher_prefix.shape)\n",
    "\n",
    "# ## Lets get the feature names present in school state https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer#:~:text=You%20can%20use%20tfidf_vectorizer.,selected)%20from%20the%20raw%20documents.\n",
    "# print(\"Features Names are : \", vectorizer_teacher_prefix.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLcxtNg7N6Hg"
   },
   "source": [
    "Clean Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1664003255229,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "17MNaAEsN6Hg",
    "outputId": "57874d34-16e9-4c1a-af42-6daa96885b12"
   },
   "outputs": [],
   "source": [
    "# # Apply One-Hot Encoding on the categorical features either using OneHotEncoder() (or) CountVectorizer(binary=True)\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# ## We will be doing vectorization in two steps :\n",
    "# ## 1. We will fit the data \n",
    "# ## 2. We will transform the data\n",
    "\n",
    "\n",
    "\n",
    "# ## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "# vectorizer_clean_categories = CountVectorizer() \n",
    "# vectorizer_clean_categories.fit(x_train['clean_categories'].values)\n",
    "    \n",
    "# ## 2. Transforming the data to its vectors form \n",
    "# x_train_clean_categories = vectorizer_clean_categories.transform(x_train['clean_categories'].values)\n",
    "# x_test_clean_categories = vectorizer_clean_categories.transform(x_test['clean_categories'].values)\n",
    "\n",
    "# print(\"Shape of matrix after one hot encodig on Train Data\",x_train_clean_categories.shape)\n",
    "# print(\"Shape of matrix after one hot encodig on Test Data\",x_test_clean_categories.shape)\n",
    "\n",
    "# ## Lets get the feature names present in school state https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer#:~:text=You%20can%20use%20tfidf_vectorizer.,selected)%20from%20the%20raw%20documents.\n",
    "# print(\"Features Names are : \", vectorizer_clean_categories.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3baD9-rN6Hh"
   },
   "source": [
    "Clean Sub Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1664003256037,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "WaFunzknN6Hh",
    "outputId": "c11d2204-dabd-4125-9c2d-92d06edcbb6a"
   },
   "outputs": [],
   "source": [
    "# # Apply One-Hot Encoding on the categorical features either using OneHotEncoder() (or) CountVectorizer(binary=True)\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# ## We will be doing vectorization in two steps :\n",
    "# ## 1. We will fit the data \n",
    "# ## 2. We will transform the data\n",
    "\n",
    "\n",
    "\n",
    "# ## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "# vectorizer_clean_subcategories = CountVectorizer() \n",
    "# vectorizer_clean_subcategories.fit(x_train['clean_subcategories'].values)\n",
    "    \n",
    "# ## 2. Transforming the data to its vectors form \n",
    "# x_train_clean_subcategories = vectorizer_clean_subcategories.transform(x_train['clean_subcategories'].values)\n",
    "# x_test_clean_subcategories = vectorizer_clean_subcategories.transform(x_test['clean_subcategories'].values)\n",
    "\n",
    "# print(\"Shape of matrix after one hot encodig on Train Data\",x_train_clean_subcategories.shape)\n",
    "# print(\"Shape of matrix after one hot encodig on Test Data\",x_test_clean_subcategories.shape)\n",
    "# print('\\n')\n",
    "# ## Lets get the feature names present in school state https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer#:~:text=You%20can%20use%20tfidf_vectorizer.,selected)%20from%20the%20raw%20documents.\n",
    "# print(\"Features Names are : \", vectorizer_clean_subcategories.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV1fOsuwN6Hi"
   },
   "source": [
    "project_grade_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706,
     "status": "ok",
     "timestamp": 1664003256740,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "qlhIHh5lN6Hi",
    "outputId": "dd4dd88e-25ac-4dbe-8bb0-806be0c26137"
   },
   "outputs": [],
   "source": [
    "# # Apply One-Hot Encoding on the categorical features either using OneHotEncoder() (or) CountVectorizer(binary=True)\n",
    "# # https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# ## We will be doing vectorization in two steps :\n",
    "# ## 1. We will fit the data \n",
    "# ## 2. We will transform the data\n",
    "\n",
    "\n",
    "\n",
    "# ## 1. Fitting the data : For fitting the data we will use count vectorizer which we have already imported\n",
    "# vectorizer_project_grade_category = CountVectorizer() \n",
    "# vectorizer_project_grade_category.fit(x_train['project_grade_category'].values)\n",
    "    \n",
    "# ## 2. Transforming the data to its vectors form \n",
    "# x_train_project_grade_category = vectorizer_project_grade_category.transform(x_train['project_grade_category'].values)\n",
    "# x_test_project_grade_category = vectorizer_project_grade_category.transform(x_test['project_grade_category'].values)\n",
    "\n",
    "# print(\"Shape of matrix after one hot encodig on Train Data\",x_train_project_grade_category.shape)\n",
    "# print(\"Shape of matrix after one hot encodig on Test Data\",x_test_project_grade_category.shape)\n",
    "\n",
    "# ## Lets get the feature names present in school state https://stackoverflow.com/questions/42525072/get-selected-feature-names-tfidf-vectorizer#:~:text=You%20can%20use%20tfidf_vectorizer.,selected)%20from%20the%20raw%20documents.\n",
    "# print(\"Features Names are : \", vectorizer_project_grade_category.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nottmr_Jq2NR"
   },
   "source": [
    "# 6. Perform Encoding on Numerical Features ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14XmDnrKrM59"
   },
   "source": [
    "## For performing Encoding on Numerical Features we will be using Normalizer , again there are basic two steps whenever we perform encoding 1. Fitting the model in our data and next is transforming our data using the model .\n",
    "\n",
    "## We have total 6 numerical features which are Price , Teacher_No_Of_Previously submitted_projects , Positive_Words , Negitive_Words , Compound_Words , Neutral_Words \n",
    "\n",
    "## For each of the numerical feature we will do the same thing that is using normalizer for fitting and then transforming our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebfJvxvxN6Hj"
   },
   "source": [
    "6.1 Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1664003256741,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ShwrYYaKN6Hj",
    "outputId": "79cc07cd-96ef-4838-a6c9-b06c62ab431b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of t0est set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_price = Normalizer() \n",
    "vectorizer_price.fit(x_train['price'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_price  = vectorizer_price.transform(x_train['price'].values.reshape(1,-1))\n",
    "x_train_price = x_train_price.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_price.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_price1 = Normalizer() \n",
    "vectorizer_price1.fit(x_test['price'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_price  = vectorizer_price1.transform(x_test['price'].values.reshape(1,-1))\n",
    "x_test_price = x_test_price.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_price.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYtYI9b5N6Hk"
   },
   "source": [
    "6.2 Teacher_Number_of_previously_posted_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1664003256742,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "g88BCCXwN6Hk",
    "outputId": "4cfdea8d-0db0-4012-d220-0464bb86b787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of test set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_teacher_number_of_previously_posted_projects = Normalizer() \n",
    "vectorizer_teacher_number_of_previously_posted_projects.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_teacher_number_of_previously_posted_projects  = vectorizer_teacher_number_of_previously_posted_projects.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "x_train_teacher_number_of_previously_posted_projects = x_train_teacher_number_of_previously_posted_projects.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_teacher_number_of_previously_posted_projects.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_teacher_number_of_previously_posted_projects1 = Normalizer() \n",
    "vectorizer_teacher_number_of_previously_posted_projects1.fit(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_teacher_number_of_previously_posted_projects  = vectorizer_teacher_number_of_previously_posted_projects1.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,-1))\n",
    "x_test_teacher_number_of_previously_posted_projects = x_test_teacher_number_of_previously_posted_projects.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_teacher_number_of_previously_posted_projects.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ykb7Vz8N6Hk"
   },
   "source": [
    "6.3 Positive_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1664003256742,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ZvYZ3dAkN6Hl",
    "outputId": "6f48bf90-fe3c-44d7-9760-7597d656ec78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of test set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_Positive_words = Normalizer() \n",
    "vectorizer_Positive_words.fit(x_train['positiveWords'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_positive_words  = vectorizer_Positive_words.transform(x_train['positiveWords'].values.reshape(1,-1))\n",
    "x_train_positive_words = x_train_positive_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_positive_words.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_positive_words = Normalizer()  \n",
    "vectorizer_positive_words.fit(x_test['positiveWords'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_positive_words  = vectorizer_positive_words.transform(x_test['positiveWords'].values.reshape(1,-1))\n",
    "x_test_positive_words = x_test_positive_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_positive_words.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-vINvDWsHk8"
   },
   "source": [
    "6.4 Negitive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1664003256743,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "deLm1AtGN6Hl",
    "outputId": "4522aadb-e0e2-4a2e-eb74-f34cd564b67d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of test set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_negitive_words = Normalizer() \n",
    "vectorizer_negitive_words.fit(x_train['negitiveWords'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_negitive_words  = vectorizer_negitive_words.transform(x_train['negitiveWords'].values.reshape(1,-1))\n",
    "x_train_negitive_words = x_train_negitive_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_negitive_words.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_negitive_words = Normalizer()  \n",
    "vectorizer_negitive_words.fit(x_test['negitiveWords'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_negitive_words  = vectorizer_negitive_words.transform(x_test['negitiveWords'].values.reshape(1,-1))\n",
    "x_test_negitive_words = x_test_negitive_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_negitive_words.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzKD0KLMsLYV"
   },
   "source": [
    "6.5 Compound Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1664003256743,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "gO0NBuvzN6Hl",
    "outputId": "a8b07f08-3cd2-4c5f-ead3-0fbb83fbfd84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of test set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_compound_words = Normalizer() \n",
    "vectorizer_compound_words.fit(x_train['compoundWords'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_compound_words  = vectorizer_compound_words.transform(x_train['compoundWords'].values.reshape(1,-1))\n",
    "x_train_compound_words = x_train_compound_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_compound_words.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_compound_words = Normalizer()  \n",
    "vectorizer_compound_words.fit(x_test['compoundWords'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_compound_words  = vectorizer_compound_words.transform(x_test['compoundWords'].values.reshape(1,-1))\n",
    "x_test_compound_words = x_test_compound_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_compound_words.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwyR8QupsSeP"
   },
   "source": [
    "6.6 Neutral Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1664003256744,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "zrP1Yox8N6Hm",
    "outputId": "9a248e4e-a080-4f19-ccec-63d814f75b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix after Normalizing data on Train Data (73196, 1)\n",
      "Shape of matrix after Normalizing data on Test Data (36052, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply Normalization on the numerical features using Normalizer().\n",
    "## Using Normalization : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html\n",
    "\n",
    "## We will be doing vectorization in two steps :\n",
    "## 1. We will fit the data on train set\n",
    "## 2. We will transform the data of train set\n",
    "## 3. We will fit the data of test set\n",
    "## 4. We will transform the data of test set\n",
    "\n",
    "\n",
    "\n",
    "## 1. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_neutral_words = Normalizer() \n",
    "vectorizer_neutral_words.fit(x_train['NeutralWords'].values.reshape(1,-1))\n",
    "\n",
    "## 2. Transforming the data to its vectors form \n",
    "x_train_neutral_words  = vectorizer_neutral_words.transform(x_train['NeutralWords'].values.reshape(1,-1))\n",
    "x_train_neutral_words = x_train_neutral_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Train Data\",x_train_compound_words.shape)\n",
    "\n",
    "## 3. Fitting the data : For fitting the data we will use Normalizer which we have already imported\n",
    "vectorizer_neutral_words = Normalizer()  \n",
    "vectorizer_neutral_words.fit(x_test['NeutralWords'].values.reshape(1,-1))\n",
    "\n",
    "## 4. Transforming the data to its vectors form \n",
    "x_test_neutral_words  = vectorizer_compound_words.transform(x_test['NeutralWords'].values.reshape(1,-1))\n",
    "x_test_neutral_words = x_test_compound_words.reshape(-1,1)\n",
    "print(\"Shape of matrix after Normalizing data on Test Data\",x_test_neutral_words.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPXDtTqtsVUZ"
   },
   "source": [
    "# After encoding our numerical features , categorical features next we have to do is stacking , Instacking we are doing two parts \n",
    "\n",
    "# 1. Stacking all the categorical , numerical values together with the tfidf vectorization \n",
    "# 2. Stacking all the categorical , numerical values together with the tfidf word2vec vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce56Cn3hN6Hn"
   },
   "source": [
    "# 7. For task 1 set 1 stack up all the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wu1ELpLVN6Hn"
   },
   "source": [
    "SET 1 - Categorical , numerical features + TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 629,
     "status": "ok",
     "timestamp": 1664003257359,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "G-CpK6RgN6Hn",
    "outputId": "f0f0ee0b-51bb-4f48-df62-3ecba0418c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train and test after stacking \n",
      "(73196, 5016)\n",
      "(36052, 5016)\n"
     ]
    }
   ],
   "source": [
    "# we can concat using : https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "x_train_concat = hstack((x_train_tfidf,x_train_school_state , x_train_teacher_prefix , x_train_clean_categories , x_train_clean_subcategories , x_train_project_grade_category , x_train_price , x_train_teacher_number_of_previously_posted_projects , x_train_positive_words , x_train_negitive_words , x_train_compound_words , x_train_neutral_words))\n",
    "x_test_concat = hstack((x_test_tfidf,x_test_school_state , x_test_teacher_prefix , x_test_clean_categories , x_test_clean_subcategories , x_test_project_grade_category , x_test_price , x_test_teacher_number_of_previously_posted_projects ,  x_test_positive_words , x_test_negitive_words , x_test_compound_words , x_test_neutral_words))\n",
    "\n",
    "print(\"Shape of Train and test after stacking \")\n",
    "print(x_train_concat.shape)\n",
    "print(x_test_concat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKNgNWbYN6Ho"
   },
   "source": [
    "## 8. For task 1 set 2 stack up all the features (for stacking dense features you can use np.stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lAXjg6dtYhL"
   },
   "source": [
    "## For stacking we have used hstack function and we got the shape of resultant matrix as follow :\n",
    "## for  Stack 1 - we got x_train_concat as (73196 , 5105)  and x_test_concat as (36052 , 5105)\n",
    "## for Stack 2 - we got x_tran_concat1 as (73196 , 405) and x_test_concat1 as (36052 , 405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A87WcxyQt-4K"
   },
   "source": [
    "# **Next we have hyperparameter tuning , 3D plot , confusion Matrix , word cloud , Boxplot and pdf graph**\n",
    "\n",
    "# **All these things we will do in both of our Stacks , Stack 1 and Stack 2 seperatly , we will start from Stack 1 , do all the things , perform hyperparameter tuning , plotting confusion matrix , plotting 3D plot and get our results and get the conclusions from all the things**\n",
    "\n",
    "# ***Starting from Stack 1***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryy2pg4OSF4T"
   },
   "source": [
    "# **Performing hyperparameter tuning , 3d plot , confusion matrix on Stack1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOngtkL5N6Ho"
   },
   "source": [
    "## 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaLxF6kmyZpl"
   },
   "source": [
    "## 9.1 Hyperparameter tuning using GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1452821,
     "status": "ok",
     "timestamp": 1664005404309,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "sN41XCMVN6Hq",
    "outputId": "104de37f-f817-4cda-b0a4-c35f6e704455"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_concat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_concat' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [1, 5, 10, 50], 'min_samples_split': [5, 10, 100, 500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced') \n",
    "model = GridSearchCV(decision_tree, param_grid=params, cv=5, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "model.fit(x_train_concat,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7rp7NEMu57c"
   },
   "source": [
    "## I have used GridSearch cv for hyperparameter tuning here I have set the parameters as max_depth and min_sample_split that was already provided to us , we have defined the cross - validation value as 5 and defined all the other neccesary  parameters.\n",
    "\n",
    "## I have also called the decision tree classifier to define our decision trees , i have only used class_weight as the parameter because we have already defined our parameters t be used in grid search in  a dictonary called **params** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k_Q3B7vyhcg"
   },
   "source": [
    "## 9.2 Plotting 3D plot using the hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnYgQyR2wWH_"
   },
   "source": [
    "## Our Next Task is to plot a 3D plot using the hyperparmeters . for 3D plot we need the train AUC , cv AUC for the plotting purpose so we are dclculating the Train AUC , CV AUC using the mean score and std score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2HtGPxb7ClW"
   },
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "train_auc= model.cv_results_['mean_train_score']\n",
    "train_auc_std= model.cv_results_['std_train_score']\n",
    "cv_auc = model.cv_results_['mean_test_score'] \n",
    "cv_auc_std= model.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638,
     "status": "ok",
     "timestamp": 1664008733023,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "3uCi91c37Kvt",
    "outputId": "ce3f8bfc-6d44-4258-fe04-b1311548386f"
   },
   "outputs": [],
   "source": [
    "train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1664008735363,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "Widpx5Di7u26",
    "outputId": "1cde8314-7811-4c0e-8289-90a6cbdc5ef7"
   },
   "outputs": [],
   "source": [
    "train_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1664008735364,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "aUYB5bY37xbM",
    "outputId": "d7b750d2-e6b7-484a-920e-a1810f53646a"
   },
   "outputs": [],
   "source": [
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1664008735365,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "Zb0YOAgr70Eq",
    "outputId": "41823ce9-3591-4e98-a590-0c929ddf7793"
   },
   "outputs": [],
   "source": [
    "cv_auc_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twRlf898wvVZ"
   },
   "source": [
    "## As we got the values of trai auc , cv auc our next task is to plot 3D plots using the min_sample split as X axis , max_depth as y axis and AUC score which is train auc and cv auc as z axis .\n",
    "\n",
    "## Lets see how the 3D plot is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1YJUFb1iFKDL-PqBbdsKTncLVm4pqJqlQ"
    },
    "executionInfo": {
     "elapsed": 7148,
     "status": "ok",
     "timestamp": 1664008742491,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "1CNPEx3tzgXk",
    "outputId": "be025b60-4865-4b25-e211-5e02d84d48cb"
   },
   "outputs": [],
   "source": [
    "## for ploting the 3D plot the first thing we need do is import plotly  min_sample_split, Y-axis as max_depth, and Z-axis as AUC Score \n",
    "## for ploting 3D plot first we imported plotly \n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "offline.init_notebook_mode()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_samples_split = [5, 10, 100, 500]\n",
    "max_depth = [1, 5, 10, 50]\n",
    "\n",
    "x1 = min_samples_split\n",
    "y1 = max_depth\n",
    "z1 = train_auc\n",
    "z2 = cv_auc\n",
    "print(z1)\n",
    "# https://plot.ly/python/3d-axes/\n",
    "trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')\n",
    "trace2 = go.Scatter3d(x=x1,y=y1,z=z2, name = 'Cross validation')\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(scene = dict(\n",
    "        xaxis = dict(title='min_samples_split'),\n",
    "        yaxis = dict(title='max_depth'),\n",
    "        zaxis = dict(title='AUC'),))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig, filename='3d-scatter-colorscale')\n",
    "fig.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdaRgcrDvqFS"
   },
   "source": [
    "# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFzwqhD211Rw"
   },
   "source": [
    "## After pltting 3d plot our next Task is to find the best parameter , fit the model with our best parameters , plot the AUC , ROC Curve and get the values using the predict_proba function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjX6J25E2n4b"
   },
   "source": [
    "## The first thing we need to do is get the best parameters , to get the best parameters we are using best_score parameter and best_score and best_params attributes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1664008742492,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "uWvQZWJfN6Hq",
    "outputId": "6a31a097-5151-49e6-9327-27d10bf997d5"
   },
   "outputs": [],
   "source": [
    "## Next step is printing best scores for train data , test data , Overall best score , Best parameter \n",
    "## Refrence : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "train_accuracy = print('Train Accuracy : %.3f' %model.best_estimator_.score(x_train_concat , y_train))\n",
    "test_accuracy = print('Test Accuracy : %.3f' %model.best_estimator_.score(x_test_concat , y_test))\n",
    "best_accuracy = print('Best Accuracy through Grid Search  %.3f' %model.best_score_)\n",
    "best_parameter = print('Best Parameters : ',model.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhmx9gIZ3Bxw"
   },
   "source": [
    "## We can see that we got Best parameter as {'max_depth': 10, 'min_samples_split': 500} , also we got best accuracy of model through grid search as 0.650 and got train and test accuracys as 0.661 and 0.636 respectively "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxmypAg43if8"
   },
   "source": [
    "# **Next we have fitting our decision trees with the best parameter values , best parameter which we have got from our Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12564,
     "status": "ok",
     "timestamp": 1664008755044,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "bSiipcZFDrE6",
    "outputId": "ea0b7be5-1ec4-43fc-9b4e-4e2e9421ca1c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## for fitting our model the first thing is importing model from sklearn , next is defing the best parameters  then comes defining our model and at last comes fitting the model into x_concat and y_train \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier     ## importing model\n",
    "params = {'max_depth': [10], 'min_samples_split': [500]}    ## defining the parameters \n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= 10 , min_samples_split=500)   ## defining our model \n",
    "decision_tree.fit(x_train_concat , y_train)    ## fitting our model into train values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5P8uBq84xuN"
   },
   "source": [
    "# **Once we have fitted our model with the best parameters then comes the part in which we plot AUC ROC curve or we can say error plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 1126,
     "status": "ok",
     "timestamp": 1664008756131,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "vcnbNOc4vzAy",
    "outputId": "6b1547f8-60d7-4a4b-c019-21a952fa13b0"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## for plotting roc_curve there are some steps which starts from defining the train and test probability using predict proba function , then comes getting the fpr , tpr and threshold values for our data nd then finally plotting the graph \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc     ## importing the roc , auc curve from sklearn \n",
    "# Getting the predicted probability scores for test and train values\n",
    "\n",
    "## defining the train_pred and test_pred using predict_proba \n",
    "y_train_pred = decision_tree.predict_proba(x_train_concat)[:,1]   \n",
    "y_test_pred = decision_tree.predict_proba(x_test_concat)[:,1]\n",
    "\n",
    "## getting the tpr , fpr and threshold values \n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "## finally plotting graph \n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Plot\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3k46y2tiBz5r"
   },
   "source": [
    "## Conclusion and observation from AUC ROC curve:\n",
    "## As we know  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "## Also we know the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes, so as we see the train AUC has higher value then test AUC this means while trainig time the model is performing slightly better then while testing time although there is not way too much difference but still there is little difference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXSz2mXWE7R7"
   },
   "source": [
    "# 11. Plot confusion matrix based on best threshold value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwPmp0BhD8O4"
   },
   "source": [
    "## After we plot the AUC-ROC curve we move to next part which is confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "703We4BfEJTH"
   },
   "source": [
    "## For plotting the appropriate Confusion matrix first thing is defining a function which will determone the best probability for a perticular threshold \n",
    "\n",
    "## 11.1 - Defining a function for finding best probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvIVHeF-N6Hr"
   },
   "outputs": [],
   "source": [
    "# Pick the best threshold among the probability estimates, such that it has to yield maximum value for TPR*(1-FPR)\n",
    "\n",
    "def find_best_probability(probability , threshold , fpr , tpr):\n",
    "    ## we can find the threshold using the formula tpr(1-fpr)\n",
    "    ## for threshold to be maximum we are using np.argmax function which yeilds the maximum value\n",
    "    threshold = threshold[np.argmax(tpr*(1-fpr))]   \n",
    "    print(\"Value of fpr(1-tpr) is \" , threshold ,  \"for threshold\" , np.round(threshold,3))\n",
    "    \n",
    "    prediction = []\n",
    "    for j in probability:\n",
    "        if j>=threshold:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nczLXI1OEnQx"
   },
   "source": [
    "## Next is getting a classification report which shows the precision , recall , f1-score \n",
    "\n",
    "## 11.2 - Getting the classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1664008756870,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "FwBptig-FNWx",
    "outputId": "ec36f9b7-47ab-4bff-f1f2-e6a424d659f5"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_concat)\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_new, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23i-M5EME2fh"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "## As we know **Recall** is  the ability of a classification model to identify all data points in a relevant class , **Precision** is  the ability of a classification model to return only the data points in a class and **F1 score** is a single metric that combines recall and precision using the harmonic mean. \n",
    "\n",
    "## So we can say for **class 0 we have 22% of the data points which are in relevent class**\n",
    "\n",
    "## and in **class 1 we have around 89% of points which belong to relevent class.**\n",
    "\n",
    "## **This means class 1 have more weightage then class 0**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRg3ow-sGciP"
   },
   "source": [
    "## After getting the classification report score next comes determining the accuracy on test data .\n",
    "\n",
    "## 11.3 - Accuracy , Precision , Recall on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1664008756871,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "4_7wowikFXf9",
    "outputId": "017dd523-4a26-4337-fa15-a4543ad91b18"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets find the accuracy , precission , recall and f1 score on test dataset\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_concat)\n",
    "\n",
    "test_accuracy = print(' Accuracy on test dataset: %.3f' %accuracy_score(y_test , y_pred_new))\n",
    "test_Precision_score = print(' Precision  on test dataset: %.3f' %precision_score(y_test , y_pred_new))\n",
    "test_f1_score = print(' F1 score on test dataset: %.3f' %f1_score(y_test , y_pred_new))\n",
    "recall_score = print(' Recall score on test dataset: %.3f' %recall_score(y_test , y_pred_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoKjg_YnG2Cw"
   },
   "source": [
    "## Conclusion \n",
    "## By above values we can say that Accuracy on our test data is around 63% which is not too good but yes it can be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CM0DYEfHQZo"
   },
   "source": [
    "## Next Comes the Confusion Matrix \n",
    "## 11.4 - Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1664008757764,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "qiJdWvprFfhq",
    "outputId": "5225653f-7a34-431d-e35a-38c52cc6c718"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Train Dataset: \")\n",
    "\n",
    "# print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_train ,find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr))\n",
    "## For heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Reds')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDg4vLH1HdC2"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "After plotting the confusion Matrix on Train Dataset we observe that \n",
    "\n",
    "1. There are 7106 values whose actual value is 0 and predicted value is also 0 hence 10% of total values .\n",
    "\n",
    "2. Next comes values whose actual value is 0 and pedicted value is 1 which is 3977 values which is around 6% of total values .\n",
    "\n",
    "3. Next comes values whose actual value is 1 and predicted value is 0 there comes 21130 values which 29% of total values .\n",
    "\n",
    "4. At last we have 55% of values where actual value is also 1 and predicted value is also 1 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1664008757765,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "IsorYBU3GRIp",
    "outputId": "887d11e2-2a08-4dd1-a5cf-f030281ffe8e"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## 10% , 6% , 29% , 55%\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Test Dataset: \")\n",
    "\n",
    "##print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_test ,find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr))\n",
    "## Fir heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Greens')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRldhhBwVsA4"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "After plotting confusion matrix on test data we observe that:\n",
    "1. There are 3193 values whose actual class is 0 and predicted class is 0 \n",
    "2. There are 2266 values whose actual class is 0 and predicted class is 1\n",
    "3. There are 11002 values whose actual class is 1 and predicted class is 0 \n",
    "4. There are 19591 values whose actual class is 1 and predicted class is 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfomys7vKeG5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnrgMZsPow5U"
   },
   "source": [
    "# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBxpK-xZA4Bf"
   },
   "source": [
    "## 12.1 - Finding all the false positive data points and creating a dataframe of these false positive words ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1664008757767,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "gZJ8r8Otlx7K",
    "outputId": "4be7cee0-704b-40d8-c98f-31f199758c87"
   },
   "outputs": [],
   "source": [
    "## https://datascience.stackexchange.com/questions/97499/viewing-false-positive-rows-in-python\n",
    "predict = find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr)\n",
    "false_positive = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if (predict[i] == 1) and (y_test[i] == 0):\n",
    "        false_positive.append(i)\n",
    "len(false_positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3B-9eRHbkoK"
   },
   "source": [
    "## to find the false positive values what we are doing is we are getting all the values in the test data where our actual value is 0 and our predicted value is 1 , so we got around 2266 values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1664008757767,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "lWgMwaxUcBzQ",
    "outputId": "baa4cf21-18ea-4ca5-f4fe-fd4820a8e1d7"
   },
   "outputs": [],
   "source": [
    "coloumn = x_test.columns    ## getting the columns of x_test data\n",
    "x_test_false_positive = pd.DataFrame(columns = coloumn)    ##creating a dataset\n",
    "x_test_false_positive = x_test.iloc[false_positive]     ## inserting false positive values\n",
    "x_test_false_positive  ## showing the values   \n",
    "print(x_test_false_positive.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GsWyW9eca9y"
   },
   "source": [
    "## In above code we have created a dataframe which contain all the false positive values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDW5orSlbELK"
   },
   "source": [
    "## 12.2 Plotting the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1brfyh_-cywo"
   },
   "source": [
    "## Our next task is to plot the wordCloud ,\n",
    "We all know Word Cloud is a data visualization technique that is used for representing **text data in which the size of each word indicates its frequency or importance.** Significant textual data points can be highlighted using a word cloud.\n",
    "\n",
    "So for plotting the word cloud there are some basic steps to be followed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 7193,
     "status": "ok",
     "timestamp": 1664008764938,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ZhYRGDleqyIb",
    "outputId": "3920df43-8f36-4e0a-8342-72d757a5cb49"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in x_test_false_positive['essay']:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtsyniOzedpI"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "We can see that the most important words in our essay are **student** , **classroom** , **school** and there are many other words .\n",
    "\n",
    "If we observe carefully we can see that the important words are coloured with dark colors and as the importance of the word decreases the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOZ-w78qAwCC"
   },
   "source": [
    "\n",
    "\n",
    "## 12.2 Plot the box plot with the `price` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1664008764939,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "sBroRI5etepH",
    "outputId": "c7dc872e-7696-462d-c8c3-0a5824dcea22"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot('price',  data=x_test_false_positive)\n",
    "plt.title(\"Box Plot of 'price' on false positive data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxn8xHpmCKd-"
   },
   "source": [
    "## 12.3 Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1664008764940,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "FsRAdlLPDEL0",
    "outputId": "1562623f-9555-456c-e7ce-a91e1b570679"
   },
   "outputs": [],
   "source": [
    "# initializing random values\n",
    "data = x_test_false_positive['teacher_number_of_previously_posted_projects']\n",
    "  \n",
    "# getting data of the histogram\n",
    "count, bins_count = np.histogram(data, bins=10)\n",
    "  \n",
    "# finding the PDF of the histogram using count values\n",
    "pdf = count / sum(count)\n",
    "\n",
    "# printing the value of pdf and bins\n",
    "print(pdf)  \n",
    "print(\"\\n\")\n",
    "print(bins_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# plotting PDF\n",
    "plt.plot(bins_count[1:], pdf, color=\"red\")\n",
    "plt.xlabel('teacher_number_of_previously_posted_projects')\n",
    "\n",
    "plt.title('PDF of false positive data points and teacher_number_of_previously_posted_projects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsmG2paLOVIr"
   },
   "source": [
    "\n",
    "# 13. Write your observations about the wordcloud and pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4xb_7JYnJTx"
   },
   "source": [
    "## Observation for WordCloud :\n",
    "In case of wordcloud we saw that as the importance of the word was increasing the colour of the word was highliting or we can say that the word was highlighting as the importance of that word was increasing .\n",
    "\n",
    "## Observation of PDF of false positive data points and teacher_number_of_previously_posted_projects\n",
    "\n",
    "In case of PDF graph we observe that there is a sudden drop in te graph at 0.05 as the false positive rate and 50 as the teacher_number_of_previously_posted_projects after that there is a slight slope which then converted to contat value 0 (false positive data) as the teacher_number_of previously_posted_projects increase .\n",
    "This means till the teacher submitted first 50 projects there were false positive or incorrect classification but after 50 values , value of false positive data changed slightly and after 100 projects were submitted there were no false positive value ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iubkXAnKVqpQ"
   },
   "source": [
    "# **Performing hyperparameter tuning , 3d plot , confusion matrix on Stack2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTigVLopVqpR"
   },
   "source": [
    "## 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKI7iOz9kOD8"
   },
   "source": [
    "## I have used GridSearch cv for hyperparameter tuning here I have set the parameters as max_depth and min_sample_split that was already provided to us , we have defined the cross - validation value as 5 and defined all the other neccesary  parameters.\n",
    "\n",
    "## I have also called the decision tree classifier to define our decision trees , i have only used class_weight as the parameter because we have already defined our parameters t be used in grid search in  a dictonary called **params** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQPPO-EFVqpR"
   },
   "source": [
    "## 9.1 Hyperparameter tuning using GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 171019,
     "status": "ok",
     "timestamp": 1664011132793,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "RKgkt48NVqpR",
    "outputId": "031b8048-ef19-4aa4-f666-426bc88a1c39"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [1, 5, 10, 50], 'min_samples_split': [5, 10, 100, 500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced') \n",
    "model1 = GridSearchCV(decision_tree, param_grid=params, cv=5, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "model1.fit(x_train_concat1,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdBHWC07VqpR"
   },
   "source": [
    "## 9.2 Plotting 3D plot using the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OJPe_3_VqpR"
   },
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "train_auc= model1.cv_results_['mean_train_score']\n",
    "train_auc_std= model1.cv_results_['std_train_score']\n",
    "cv_auc = model1.cv_results_['mean_test_score'] \n",
    "cv_auc_std= model1.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1664011132797,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "TcvfrUYlVqpS",
    "outputId": "fa9ef891-a5da-4ae0-cd79-33ae9b07ef61"
   },
   "outputs": [],
   "source": [
    "train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1664011132798,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "nGVQFLbQVqpS",
    "outputId": "0132684c-bd27-45c2-bf5a-0a828df5dc37"
   },
   "outputs": [],
   "source": [
    "train_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1664011132799,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "g-kuSFDIVqpS",
    "outputId": "a581f948-36bf-4311-be2e-936700f1d8b6"
   },
   "outputs": [],
   "source": [
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1664011132800,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "Qp8gNBHHVqpS",
    "outputId": "9fa4d861-6a0e-4267-dbb4-f626eeb90e4f"
   },
   "outputs": [],
   "source": [
    "cv_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1svh24Ai1d7WLLMqsmuvheWHabG0KMv0l"
    },
    "executionInfo": {
     "elapsed": 13339,
     "status": "ok",
     "timestamp": 1664011146131,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "62KLZI64VqpS",
    "outputId": "525d7a6e-4a59-4304-ce44-f244c92f9400"
   },
   "outputs": [],
   "source": [
    "## for ploting the 3D plot the first thing we need do is import plotly  min_sample_split, Y-axis as max_depth, and Z-axis as AUC Score \n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "offline.init_notebook_mode()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_samples_split = [5, 10, 100, 500]\n",
    "max_depth = [1, 5, 10, 50]\n",
    "\n",
    "x1 = min_samples_split\n",
    "y1 = max_depth\n",
    "z1 = train_auc\n",
    "z2 = cv_auc\n",
    "print(z1)\n",
    "# https://plot.ly/python/3d-axes/\n",
    "trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')\n",
    "trace2 = go.Scatter3d(x=x1,y=y1,z=z2, name = 'Cross validation')\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(scene = dict(\n",
    "        xaxis = dict(title='min_samples_split'),\n",
    "        yaxis = dict(title='max_depth'),\n",
    "        zaxis = dict(title='AUC'),))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig, filename='3d-scatter-colorscale')\n",
    "fig.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPGxmh0NVqpS"
   },
   "source": [
    "# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1664011146479,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "T3GbO3JsVqpT",
    "outputId": "d6097c15-2144-4fcc-c194-9b66780259cc"
   },
   "outputs": [],
   "source": [
    "## Next step is printing best scores for train data , test data , Overall best score , Best parameter \n",
    "train_accuracy = print('Train Accuracy : %.3f' %model1.best_estimator_.score(x_train_concat1 , y_train))\n",
    "test_accuracy = print('Test Accuracy : %.3f' %model1.best_estimator_.score(x_test_concat1 , y_test))\n",
    "best_accuracy = print('Best Accuracy through Grid Search  %.3f' %model1.best_score_)\n",
    "best_alpha  = print('Best Parameters : ',model1.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13934,
     "status": "ok",
     "timestamp": 1664011160064,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "EQ5DkFACVqpT",
    "outputId": "cbc736d8-cafe-4278-a9e9-b498a15a935d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [5], 'min_samples_split': [500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= 5 , min_samples_split=500) \n",
    "decision_tree.fit(x_train_concat1 , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QAhbRzLZlxx7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_concat1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_concat1' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Getting the predicted probability scores for test and train values\n",
    "y_train_pred = decision_tree.predict_proba(x_train_concat1)[:,1]   \n",
    "y_test_pred = decision_tree.predict_proba(x_test_concat1)[:,1]\n",
    "\n",
    "\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"ROC\")\n",
    "plt.title(\"ROC  Plot\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FORHHx1eDsmd"
   },
   "source": [
    "## Conclusion and observation from AUC ROC curve:\n",
    "## As we know  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "## Also we know the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes, so as we see the train AUC has higher value then test AUC this means while trainig time the model is performing slightly better then while testing time although there is not way too much difference but still there is little difference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loY12KvybdkU"
   },
   "source": [
    "# 11. Plot confusion matrix based on best threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtBLjsIxN6Hr"
   },
   "outputs": [],
   "source": [
    "# Pick the best threshold among the probability estimates, such that it has to yield maximum value for TPR*(1-FPR)\n",
    "\n",
    "def find_best_probability(probability , threshold , fpr , tpr):\n",
    "    ## we can find the threshold using the formula tpr(1-fpr)\n",
    "    ## for threshold to be maximum we are using np.argmax function which yeilds the maximum value\n",
    "    threshold = threshold[np.argmax(tpr*(1-fpr))]   \n",
    "    print(\"Value of fpr(1-tpr) is \" , threshold ,  \"for threshold\" , np.round(threshold,3))\n",
    "    \n",
    "    prediction = []\n",
    "    for j in probability:\n",
    "        if j>=threshold:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO75eXwQN6Hr"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_concat1)\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_new, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ka8V1ogVN6Hs"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets find the accuracy , precission , recall and f1 score on test dataset\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_concat1)\n",
    "\n",
    "test_accuracy = print(' Accuracy on test dataset: %.3f' %accuracy_score(y_test , y_pred_new))\n",
    "test_Precision_score = print(' Precision  on test dataset: %.3f' %precision_score(y_test , y_pred_new))\n",
    "test_f1_score = print(' F1 score on test dataset: %.3f' %f1_score(y_test , y_pred_new))\n",
    "recall_score = print(' Recall score on test dataset: %.3f' %recall_score(y_test , y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ply3vcJ9bvZD"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Train Dataset: \")\n",
    "\n",
    "# print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_train ,find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr))\n",
    "## For heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Reds')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnO8hIJsbvTz"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Test Dataset: \")\n",
    "\n",
    "##print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_test ,find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr))\n",
    "## Fir heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Greens')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrkbAO26cKdY"
   },
   "source": [
    "# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHtffvEHcM1Z"
   },
   "source": [
    "## 12.1 Plotting the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these false positive data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VohlSUHZeWcd"
   },
   "source": [
    "## Our next task is to plot the wordCloud ,\n",
    "We all know Word Cloud is a data visualization technique that is used for representing **text data in which the size of each word indicates its frequency or importance.** Significant textual data points can be highlighted using a word cloud.\n",
    "\n",
    "So for plotting the word cloud there are some basic steps to be followed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1664011161265,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "xOYkM0qvbvMm",
    "outputId": "e35cc3d1-0545-494a-a839-a114d3194ee0"
   },
   "outputs": [],
   "source": [
    "## https://datascience.stackexchange.com/questions/97499/viewing-false-positive-rows-in-python\n",
    "predict = find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr)\n",
    "false_positive = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if (predict[i] == 1) and (y_test[i] == 0):\n",
    "        false_positive.append(i)\n",
    "len(false_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcanupqBcZaL"
   },
   "outputs": [],
   "source": [
    "coloumn = x_test.columns    ## getting the columns of x_test data\n",
    "x_test_false_positive = pd.DataFrame(columns = coloumn)    ##creating a dataset\n",
    "x_test_false_positive = x_test.iloc[false_positive]     ## inserting false positive values\n",
    "x_test_false_positive  ## showing the values    \n",
    "print(x_test_false_positive.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 7560,
     "status": "ok",
     "timestamp": 1664011170197,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "oKa6kn88cZXm",
    "outputId": "d4128957-44b0-49cd-bb33-ef82e6df79cc"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in x_test_false_positive['essay']:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAtyucuacpLt"
   },
   "source": [
    "\n",
    "\n",
    "## 12.2 Plot the box plot with the `price` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1664011170199,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "ss2LhySbcZUC",
    "outputId": "4c8e8688-bbb9-448a-b5c5-52263f9da7e4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot('price',  data=x_test_false_positive)\n",
    "plt.title(\"Box Plot of 'price' on false positive data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSverVqmcv4Q"
   },
   "source": [
    "## 12.3 Plot the pdf with the teacher_number_of_previously_posted_projects of these false positive data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1664011170201,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "WX4jwRjHcZRj",
    "outputId": "4c82280b-ba2a-4278-d0a8-963c055d161f"
   },
   "outputs": [],
   "source": [
    "# initializing random values\n",
    "data = x_test_false_positive['teacher_number_of_previously_posted_projects']\n",
    "  \n",
    "# getting data of the histogram\n",
    "count, bins_count = np.histogram(data, bins=10)\n",
    "  \n",
    "# finding the PDF of the histogram using count values\n",
    "pdf = count / sum(count)\n",
    "\n",
    "# printing the value of pdf and bins\n",
    "print(pdf)  \n",
    "print(\"\\n\")\n",
    "print(bins_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# plotting PDF\n",
    "plt.plot(bins_count[1:], pdf, color=\"red\")\n",
    "plt.xlabel('teacher_number_of_previously_posted_projects')\n",
    "\n",
    "plt.title('PDF of false positive data points and teacher_number_of_previously_posted_projects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDOON1emdB1A"
   },
   "source": [
    "\n",
    "# 13. Write your observations about the wordcloud and pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGVJLvMEcZN6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SwMC-ORcZLL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mJJGwJQcZIn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmHih9-YiqLx"
   },
   "source": [
    "# <font color='red'> <b>Task - 2</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zk2-az7QiqLz"
   },
   "outputs": [],
   "source": [
    "# 1. write your code in following steps for task 2\n",
    "# 2. select all non zero features\n",
    "# 3. Update your dataset i.e. X_train,X_test and X_cv so that it contains all rows and only non zero features\n",
    "# 4. perform hyperparameter tuning and plot either heatmap or 3d plot.\n",
    "# 5. Fit the best model. Plot ROC AUC curve and confusion matrix similar to model 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-F84_pUIN8j"
   },
   "source": [
    "For this task consider set-1 features.\n",
    "\n",
    "Select all the features which are having non-zero feature importance.You can get the feature importance using 'feature_importances_` (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), discard the all other remaining features and then apply any of the model of you choice i.e. (Dession tree, Logistic Regression, Linear SVM).\n",
    "You need to do hyperparameter tuning corresponding to the model you selected and procedure in step 2 and step 3\n",
    "Note: when you want to find the feature importance make sure you don't use max_depth parameter keep it None.\n",
    "\n",
    "You need to summarize the results at the end of the notebook, summarize it in the table format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1664011170207,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "N44iBzrmiqL1",
    "outputId": "62b1b212-da0b-4520-ec8f-f84ea7ac1759"
   },
   "outputs": [],
   "source": [
    "# we can concat using : https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "x_train_concat = hstack((x_train_tfidf,x_train_school_state , x_train_teacher_prefix , x_train_clean_categories , x_train_clean_subcategories , x_train_project_grade_category , x_train_price , x_train_teacher_number_of_previously_posted_projects , x_train_positive_words , x_train_negitive_words , x_train_compound_words , x_train_neutral_words))\n",
    "x_test_concat = hstack((x_test_tfidf,x_test_school_state , x_test_teacher_prefix , x_test_clean_categories , x_test_clean_subcategories , x_test_project_grade_category , x_test_price , x_test_teacher_number_of_previously_posted_projects ,  x_test_positive_words , x_test_negitive_words , x_test_compound_words , x_test_neutral_words))\n",
    "\n",
    "print(\"Shape of Train and test after stacking \")\n",
    "print(x_train_concat.shape)\n",
    "print(x_test_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1664011170792,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "bxdzfhcbsqq2",
    "outputId": "e46120c7-a0c1-42d3-e84a-82ff72767694"
   },
   "outputs": [],
   "source": [
    "# we can concat using : https://stackoverflow.com/a/19710648/4084039\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "x_train_concat1 = hstack((tfidf_w2v_vectors_train,x_train_school_state , x_train_teacher_prefix , x_train_clean_categories , x_train_clean_subcategories , x_train_project_grade_category , x_train_price , x_train_teacher_number_of_previously_posted_projects , x_train_positive_words , x_train_negitive_words , x_train_compound_words , x_train_neutral_words))\n",
    "x_test_concat1 = hstack((tfidf_w2v_vectors_test,x_test_school_state , x_test_teacher_prefix , x_test_clean_categories , x_test_clean_subcategories , x_test_project_grade_category , x_test_price , x_test_teacher_number_of_previously_posted_projects ,  x_test_positive_words , x_test_negitive_words , x_test_compound_words , x_test_neutral_words))\n",
    "\n",
    "print(\"Shape of Train and test after stacking \")\n",
    "print(x_train_concat1.shape)\n",
    "print(x_test_concat1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvfTMChJfita"
   },
   "source": [
    "## Converting coo.coo_matrix into dense matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjOeIbB0eTkw"
   },
   "outputs": [],
   "source": [
    "x_train_concat_dense = x_train_concat.tocsr()\n",
    "x_test_concat_dense = x_test_concat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUvM1SF5strf"
   },
   "outputs": [],
   "source": [
    "x_train_concat1_dense = x_train_concat1.tocsr()\n",
    "x_test_concat1_dense = x_test_concat1.tocsr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RN0l-getEBS"
   },
   "source": [
    "# Task 2 Part 1 - TFIDF VECTORIZED NON ZERO VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDZ_4a6Hs5aQ"
   },
   "source": [
    "##Fitting Decision tree on concat(only tfidf vectorisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98845,
     "status": "ok",
     "timestamp": 1664011269636,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "fSVKa3RviqL3",
    "outputId": "15c671e7-c93a-4759-d55e-301aff0751bd"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= None , min_samples_split= 500) \n",
    "decision_tree.fit(x_train_concat_dense, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3Nu0h6YM4vo"
   },
   "outputs": [],
   "source": [
    "features = decision_tree.feature_importances_\n",
    "features\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr5mioHBNidv"
   },
   "outputs": [],
   "source": [
    "non_zero_feature = []\n",
    "for i in  range(len(features)):\n",
    "    # print(i)\n",
    "    # print(features[i])     ## when we print the features of i we can see that there are both zeros and non zeroes values \n",
    "    if features[i] > 0 :\n",
    "        # print(features[i])\n",
    "        # print(type(features[i]))\n",
    "        # print(features[i])\n",
    "        non_zero_feature.append(i)\n",
    "# print(len(non_zero_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK9vNQsfU5xt"
   },
   "outputs": [],
   "source": [
    "# print((non_zero_feature))\n",
    "# print(len(non_zero_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TBgZuT1Ru79"
   },
   "source": [
    "## So Number of non zero features is 1060 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pALMU_fQWTvA"
   },
   "outputs": [],
   "source": [
    "type(x_train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVaOD_PTOQ2G"
   },
   "outputs": [],
   "source": [
    "x_train_feature = x_train_concat_dense[:,non_zero_feature]\n",
    "x_test_feature = x_test_concat_dense[:,non_zero_feature]\n",
    "\n",
    "print(\"Shape of train featue matrix :\" , x_train_feature.shape)\n",
    "print(\"Shape of test featue matrix :\" , x_test_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bA3_x20zb3iS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izThIvLXhedQ"
   },
   "source": [
    "# **Performing hyperparameter tuning , 3d plot , confusion matrix on Stack2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIMngdzmhedR"
   },
   "source": [
    "## 9. Perform hyperparameter tuning and plot either heatmap or 3d plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAHs52tXhedS"
   },
   "source": [
    "## 9.1 Hyperparameter tuning using GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w53UyuWAhedS"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [1, 5, 10, 50], 'min_samples_split': [5, 10, 100, 500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced') \n",
    "model1 = GridSearchCV(decision_tree, param_grid=params, cv=5, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "model1.fit(x_train_feature,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZWXoCOJhedT"
   },
   "source": [
    "## 9.2 Plotting 3D plot using the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPOwqG8WhedT"
   },
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "train_auc= model1.cv_results_['mean_train_score']\n",
    "train_auc_std= model1.cv_results_['std_train_score']\n",
    "cv_auc = model1.cv_results_['mean_test_score'] \n",
    "cv_auc_std= model1.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEA_j3SmhedU"
   },
   "outputs": [],
   "source": [
    "train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lebrYu_shedV"
   },
   "outputs": [],
   "source": [
    "train_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgH0LSkWhedW"
   },
   "outputs": [],
   "source": [
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sueXY7bshedW"
   },
   "outputs": [],
   "source": [
    "cv_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFvhnWDFhedX"
   },
   "outputs": [],
   "source": [
    "## for ploting the 3D plot the first thing we need do is import plotly  min_sample_split, Y-axis as max_depth, and Z-axis as AUC Score \n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "offline.init_notebook_mode()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_samples_split = [5, 10, 100, 500]\n",
    "max_depth = [1, 5, 10, 50]\n",
    "\n",
    "x1 = min_samples_split\n",
    "y1 = max_depth\n",
    "z1 = train_auc\n",
    "z2 = cv_auc\n",
    "print(z1)\n",
    "# https://plot.ly/python/3d-axes/\n",
    "trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')\n",
    "trace2 = go.Scatter3d(x=x1,y=y1,z=z2, name = 'Cross validation')\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(scene = dict(\n",
    "        xaxis = dict(title='min_samples_split'),\n",
    "        yaxis = dict(title='max_depth'),\n",
    "        zaxis = dict(title='AUC'),))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig, filename='3d-scatter-colorscale')\n",
    "fig.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAaNrlUYhedY"
   },
   "source": [
    "# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eicsX_eahedY"
   },
   "outputs": [],
   "source": [
    "## Next step is printing best scores for train data , test data , Overall best score , Best parameter \n",
    "train_accuracy = print('Train Accuracy : %.3f' %model1.best_estimator_.score(x_train_feature , y_train))\n",
    "test_accuracy = print('Test Accuracy : %.3f' %model1.best_estimator_.score(x_test_feature , y_test))\n",
    "best_accuracy = print('Best Accuracy through Grid Search  %.3f' %model1.best_score_)\n",
    "best_alpha  = print('Best Parameters : ',model1.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6P0aUCXhedZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [10], 'min_samples_split': [500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= 10 , min_samples_split=500) \n",
    "decision_tree.fit(x_train_feature , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZuTlIWihedZ"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Getting the predicted probability scores for test and train values\n",
    "y_train_pred = decision_tree.predict_proba(x_train_feature)[:,1]   \n",
    "y_test_pred = decision_tree.predict_proba(x_test_feature)[:,1]\n",
    "\n",
    "\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"AUC\")\n",
    "plt.ylabel(\"ROC\")\n",
    "plt.title(\"ROC  Plot\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RboasOLRheda"
   },
   "source": [
    "## Conclusion and observation from AUC ROC curve:\n",
    "## As we know  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "## Also we know the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes, so as we see the train AUC has higher value then test AUC this means while trainig time the model is performing slightly better then while testing time although there is not way too much difference but still there is little difference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11TFt1RJhedb"
   },
   "source": [
    "# 11. Plot confusion matrix based on best threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBmgXr0Vhedc"
   },
   "outputs": [],
   "source": [
    "# Pick the best threshold among the probability estimates, such that it has to yield maximum value for TPR*(1-FPR)\n",
    "\n",
    "def find_best_probability(probability , threshold , fpr , tpr):\n",
    "    ## we can find the threshold using the formula tpr(1-fpr)\n",
    "    ## for threshold to be maximum we are using np.argmax function which yeilds the maximum value\n",
    "    threshold = threshold[np.argmax(tpr*(1-fpr))]   \n",
    "    print(\"Value of fpr(1-tpr) is \" , threshold ,  \"for threshold\" , np.round(threshold,3))\n",
    "    \n",
    "    prediction = []\n",
    "    for j in probability:\n",
    "        if j>=threshold:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8nRYD2Ahedd"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_feature)\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_new, target_names = target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEUG6qTzhede"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets find the accuracy , precission , recall and f1 score on test dataset\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_feature)\n",
    "\n",
    "test_accuracy = print(' Accuracy on test dataset: %.3f' %accuracy_score(y_test , y_pred_new))\n",
    "test_Precision_score = print(' Precision  on test dataset: %.3f' %precision_score(y_test , y_pred_new))\n",
    "test_f1_score = print(' F1 score on test dataset: %.3f' %f1_score(y_test , y_pred_new))\n",
    "recall_score = print(' Recall score on test dataset: %.3f' %recall_score(y_test , y_pred_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xFlvC-ihede"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Train Dataset: \")\n",
    "\n",
    "# print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_train ,find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr))\n",
    "## For heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Reds')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpheEYhuhede"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Test Dataset: \")\n",
    "\n",
    "##print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_test ,find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr))\n",
    "## Fir heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Greens')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTbQdmInhedf"
   },
   "source": [
    "# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E95T1YnWhedf"
   },
   "source": [
    "## 12.1 Plotting the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these false positive data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXD_-AHwhedg"
   },
   "source": [
    "## Our next task is to plot the wordCloud ,\n",
    "We all know Word Cloud is a data visualization technique that is used for representing **text data in which the size of each word indicates its frequency or importance.** Significant textual data points can be highlighted using a word cloud.\n",
    "\n",
    "So for plotting the word cloud there are some basic steps to be followed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1664012004411,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "qs-EorYIhedg",
    "outputId": "a52d4307-0bf2-4c3f-9283-4ad16cdaa8aa"
   },
   "outputs": [],
   "source": [
    "## https://datascience.stackexchange.com/questions/97499/viewing-false-positive-rows-in-python\n",
    "predict = find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr)\n",
    "false_positive = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if (predict[i] == 1) and (y_test[i] == 0):\n",
    "        false_positive.append(i)\n",
    "len(false_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p11v46kuhedg"
   },
   "outputs": [],
   "source": [
    "coloumn = x_test.columns    ## getting the columns of x_test data\n",
    "x_test_false_positive = pd.DataFrame(columns = coloumn)    ##creating a dataset\n",
    "x_test_false_positive = x_test.iloc[false_positive]     ## inserting false positive values\n",
    "x_test_false_positive  ## showing the values    \n",
    "print(x_test_false_positive.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M8fbPCQmhedh"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in x_test_false_positive['essay']:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMCTfXl8hedh"
   },
   "source": [
    "\n",
    "\n",
    "## 12.2 Plot the box plot with the `price` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyBu8S2rhedi"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot('price',  data=x_test_false_positive)\n",
    "plt.title(\"Box Plot of 'price' on false positive data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5fxFzI9hedi"
   },
   "source": [
    "## 12.3 Plot the pdf with the teacher_number_of_previously_posted_projects of these false positive data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JofOCCH5hedi"
   },
   "outputs": [],
   "source": [
    "# initializing random values\n",
    "data = x_test_false_positive['teacher_number_of_previously_posted_projects']\n",
    "  \n",
    "# getting data of the histogram\n",
    "count, bins_count = np.histogram(data, bins=10)\n",
    "  \n",
    "# finding the PDF of the histogram using count values\n",
    "pdf = count / sum(count)\n",
    "\n",
    "# printing the value of pdf and bins\n",
    "print(pdf)  \n",
    "print(\"\\n\")\n",
    "print(bins_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# plotting PDF\n",
    "plt.plot(bins_count[1:], pdf, color=\"red\")\n",
    "plt.xlabel('teacher_number_of_previously_posted_projects')\n",
    "\n",
    "plt.title('PDF of false positive data points and teacher_number_of_previously_posted_projects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CGAOnDXqLMQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wZwE0NHtbOX"
   },
   "source": [
    "## Fitting Decision tree on concat1( tfidf word2vec vectorisation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo6difM_tbOY"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= None , min_samples_split= 500) \n",
    "decision_tree.fit(x_train_concat1_dense, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wl-KRG8MtbOY"
   },
   "outputs": [],
   "source": [
    "features = decision_tree.feature_importances_\n",
    "features\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PR-H0MG8tbOY"
   },
   "outputs": [],
   "source": [
    "non_zero_feature = []\n",
    "for i in  range(len(features)):\n",
    "    # print(i)\n",
    "    # print(features[i])     ## when we print the features of i we can see that there are both zeros and non zeroes values \n",
    "    if features[i] > 0 :\n",
    "        # print(features[i])\n",
    "        # print(type(features[i]))\n",
    "        # print(features[i])\n",
    "        non_zero_feature.append(i)\n",
    "# print(len(non_zero_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lf178y0NtbOZ"
   },
   "outputs": [],
   "source": [
    "# print((non_zero_feature))\n",
    "# print(len(non_zero_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-ZBrIpJtbOZ"
   },
   "source": [
    "## So Number of non zero features is 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RUxAMbjtbOZ"
   },
   "outputs": [],
   "source": [
    "type(x_train_concat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUQhGpqTtbOa"
   },
   "outputs": [],
   "source": [
    "x_train_feature1 = x_train_concat1_dense[:,non_zero_feature]\n",
    "x_test_feature1 = x_test_concat1_dense[:,non_zero_feature]\n",
    "\n",
    "print(\"Shape of train featue matrix :\" , x_train_feature.shape)\n",
    "print(\"Shape of test featue matrix :\" , x_test_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcvscrt-tbOa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ug4QZOeGrjhf"
   },
   "source": [
    "## 9.1 Hyperparameter tuning using GridSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZght8jjrjhf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'max_depth': [1, 5, 10, 50], 'min_samples_split': [5, 10, 100, 500]}\n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced') \n",
    "model = GridSearchCV(decision_tree, param_grid=params, cv=5, scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "model.fit(x_train_feature1,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-VJ6VJjrjhf"
   },
   "source": [
    "## I have used GridSearch cv for hyperparameter tuning here I have set the parameters as max_depth and min_sample_split that was already provided to us , we have defined the cross - validation value as 5 and defined all the other neccesary  parameters.\n",
    "\n",
    "## I have also called the decision tree classifier to define our decision trees , i have only used class_weight as the parameter because we have already defined our parameters t be used in grid search in  a dictonary called **params** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9jr4BXPrjhf"
   },
   "source": [
    "## 9.2 Plotting 3D plot using the hyperparameters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpJQW0Ytrjhf"
   },
   "source": [
    "## Our Next Task is to plot a 3D plot using the hyperparmeters . for 3D plot we need the train AUC , cv AUC for the plotting purpose so we are dclculating the Train AUC , CV AUC using the mean score and std score values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8NuZDAMrjhg"
   },
   "outputs": [],
   "source": [
    "## https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "train_auc= model.cv_results_['mean_train_score']\n",
    "train_auc_std= model.cv_results_['std_train_score']\n",
    "cv_auc = model.cv_results_['mean_test_score'] \n",
    "cv_auc_std= model.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfg4i6n4rjhg"
   },
   "outputs": [],
   "source": [
    "train_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EwohvmCrjhg"
   },
   "outputs": [],
   "source": [
    "train_auc_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifyrmEk7rjhg"
   },
   "outputs": [],
   "source": [
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfICbWuRrjhg"
   },
   "outputs": [],
   "source": [
    "cv_auc_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWAFhTxurjhg"
   },
   "source": [
    "## As we got the values of trai auc , cv auc our next task is to plot 3D plots using the min_sample split as X axis , max_depth as y axis and AUC score which is train auc and cv auc as z axis .\n",
    "\n",
    "## Lets see how the 3D plot is created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBl5gPMarjhg"
   },
   "outputs": [],
   "source": [
    "## for ploting the 3D plot the first thing we need do is import plotly  min_sample_split, Y-axis as max_depth, and Z-axis as AUC Score \n",
    "import plotly.offline as offline\n",
    "import plotly.graph_objs as go\n",
    "offline.init_notebook_mode()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_samples_split = [5, 10, 100, 500]\n",
    "max_depth = [1, 5, 10, 50]\n",
    "\n",
    "x1 = min_samples_split\n",
    "y1 = max_depth\n",
    "z1 = train_auc\n",
    "z2 = cv_auc\n",
    "print(z1)\n",
    "# https://plot.ly/python/3d-axes/\n",
    "trace1 = go.Scatter3d(x=x1,y=y1,z=z1, name = 'train')\n",
    "trace2 = go.Scatter3d(x=x1,y=y1,z=z2, name = 'Cross validation')\n",
    "data = [trace1, trace2]\n",
    "\n",
    "layout = go.Layout(scene = dict(\n",
    "        xaxis = dict(title='min_samples_split'),\n",
    "        yaxis = dict(title='max_depth'),\n",
    "        zaxis = dict(title='AUC'),))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "offline.iplot(fig, filename='3d-scatter-colorscale')\n",
    "fig.show()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFCfFWezrjhh"
   },
   "source": [
    "# 10. Find the best parameters and fit the model. Plot ROC-AUC curve(using predict proba method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sILC5J5Jrjhh"
   },
   "source": [
    "## After pltting 3d plot our next Task is to find the best parameter , fit the model with our best parameters , plot the AUC , ROC Curve and get the values using the predict_proba function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNInz7Y7rjhh"
   },
   "source": [
    "## The first thing we need to do is get the best parameters , to get the best parameters we are using best_score parameter and best_score and best_params attributes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYAyySSgrjhh"
   },
   "outputs": [],
   "source": [
    "## Next step is printing best scores for train data , test data , Overall best score , Best parameter \n",
    "## Refrence : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "train_accuracy = print('Train Accuracy : %.3f' %model.best_estimator_.score(x_train_feature1 , y_train))\n",
    "test_accuracy = print('Test Accuracy : %.3f' %model.best_estimator_.score(x_test_feature1 , y_test))\n",
    "best_accuracy = print('Best Accuracy through Grid Search  %.3f' %model.best_score_)\n",
    "best_parameter = print('Best Parameters : ',model.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8RcuGudrjhh"
   },
   "source": [
    "## We can see that we got Best parameter as {'max_depth': 10, 'min_samples_split': 500} , also we got best accuracy of model through grid search as 0.650 and got train and test accuracys as 0.661 and 0.636 respectively "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBUpc5aarjhh"
   },
   "source": [
    "# **Next we have fitting our decision trees with the best parameter values , best parameter which we have got from our Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eeKirzMarjhh"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## for fitting our model the first thing is importing model from sklearn , next is defing the best parameters  then comes defining our model and at last comes fitting the model into x_concat and y_train \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.tree import DecisionTreeClassifier     ## importing model\n",
    "params = {'max_depth': [10], 'min_samples_split': [500]}    ## defining the parameters \n",
    "\n",
    "decision_tree= DecisionTreeClassifier(class_weight='balanced' , max_depth= 10 , min_samples_split=500)   ## defining our model \n",
    "decision_tree.fit(x_train_feature1 , y_train)    ## fitting our model into train values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kICxmE32rjhh"
   },
   "source": [
    "# **Once we have fitted our model with the best parameters then comes the part in which we plot AUC ROC curve or we can say error plot.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJiFRhjxrjhh"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## for plotting roc_curve there are some steps which starts from defining the train and test probability using predict proba function , then comes getting the fpr , tpr and threshold values for our data nd then finally plotting the graph \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\n",
    "from sklearn.metrics import roc_curve, auc     ## importing the roc , auc curve from sklearn \n",
    "# Getting the predicted probability scores for test and train values\n",
    "\n",
    "## defining the train_pred and test_pred using predict_proba \n",
    "y_train_pred = decision_tree.predict_proba(x_train_feature1)[:,1]   \n",
    "y_test_pred = decision_tree.predict_proba(x_test_feature1)[:,1]\n",
    "\n",
    "## getting the tpr , fpr and threshold values \n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_pred)\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "## finally plotting graph \n",
    "plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.legend()\n",
    "plt.xlabel(\"fpr\")\n",
    "plt.ylabel(\"tpr\")\n",
    "plt.title(\"ROC Plot\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMyD5XYTrjhi"
   },
   "source": [
    "## Conclusion and observation from AUC ROC curve:\n",
    "## As we know  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n",
    "\n",
    "## Also we know the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes, so as we see the train AUC has higher value then test AUC this means while trainig time the model is performing slightly better then while testing time although there is not way too much difference but still there is little difference ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZ9U45Xprjhi"
   },
   "source": [
    "# 11. Plot confusion matrix based on best threshold value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0irWf81srjhi"
   },
   "source": [
    "## After we plot the AUC-ROC curve we move to next part which is confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6ACwRLzrjhi"
   },
   "source": [
    "## For plotting the appropriate Confusion matrix first thing is defining a function which will determone the best probability for a perticular threshold \n",
    "\n",
    "## 11.1 - Defining a function for finding best probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPAPnLRkrjhi"
   },
   "outputs": [],
   "source": [
    "# Pick the best threshold among the probability estimates, such that it has to yield maximum value for TPR*(1-FPR)\n",
    "\n",
    "def find_best_probability(probability , threshold , fpr , tpr):\n",
    "    ## we can find the threshold using the formula tpr(1-fpr)\n",
    "    ## for threshold to be maximum we are using np.argmax function which yeilds the maximum value\n",
    "    threshold = threshold[np.argmax(tpr*(1-fpr))]   \n",
    "    print(\"Value of fpr(1-tpr) is \" , threshold ,  \"for threshold\" , np.round(threshold,3))\n",
    "    \n",
    "    prediction = []\n",
    "    for j in probability:\n",
    "        if j>=threshold:\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jADN66Zxrjhi"
   },
   "source": [
    "## Next is getting a classification report which shows the precision , recall , f1-score \n",
    "\n",
    "## 11.2 - Getting the classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hN3DI8NJrjhi"
   },
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_feature1)\n",
    "target_names = ['class 0', 'class 1']\n",
    "print(classification_report(y_test, y_pred_new, target_names = target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcREzrVfrjhi"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "## As we know **Recall** is  the ability of a classification model to identify all data points in a relevant class , **Precision** is  the ability of a classification model to return only the data points in a class and **F1 score** is a single metric that combines recall and precision using the harmonic mean. \n",
    "\n",
    "## So we can say for **class 0 we have 22% of the data points which are in relevent class**\n",
    "\n",
    "## and in **class 1 we have around 89% of points which belong to relevent class.**\n",
    "\n",
    "## **This means class 1 have more weightage then class 0**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oby_F57rjhj"
   },
   "source": [
    "## After getting the classification report score next comes determining the accuracy on test data .\n",
    "\n",
    "## 11.3 - Accuracy , Precision , Recall on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnhkCstKrjhj"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## lets find the accuracy , precission , recall and f1 score on test dataset\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "y_pred_new = decision_tree.predict(x_test_feature1)\n",
    "\n",
    "test_accuracy = print(' Accuracy on test dataset: %.3f' %accuracy_score(y_test , y_pred_new))\n",
    "test_Precision_score = print(' Precision  on test dataset: %.3f' %precision_score(y_test , y_pred_new))\n",
    "test_f1_score = print(' F1 score on test dataset: %.3f' %f1_score(y_test , y_pred_new))\n",
    "recall_score = print(' Recall score on test dataset: %.3f' %recall_score(y_test , y_pred_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18OmtNMHrjhj"
   },
   "source": [
    "## Conclusion \n",
    "## By above values we can say that Accuracy on our test data is around 63% which is not too good but yes it can be considered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYegCRqBrjhj"
   },
   "source": [
    "## Next Comes the Confusion Matrix \n",
    "11.4 - Confusion Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALRknCGXrjhj"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Train Dataset: \")\n",
    "\n",
    "# print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_train ,find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr))\n",
    "## For heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Reds')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoLXFagIrjhj"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "After plotting the confusion Matrix on Train Dataset we observe that \n",
    "\n",
    "1. There are 7106 values whose actual value is 0 and predicted value is also 0 hence 10% of total values .\n",
    "\n",
    "2. Next comes values whose actual value is 0 and pedicted value is 1 which is 3977 values which is around 6% of total values .\n",
    "\n",
    "3. Next comes values whose actual value is 1 and predicted value is 0 there comes 21130 values which 29% of total values .\n",
    "\n",
    "4. At last we have 55% of values where actual value is also 1 and predicted value is also 1 .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eo2HtIXrjhj"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices(each for train and test data) afer encoding the predicted class labels, on the basis of the best threshod probability estimate.\n",
    "## 10% , 6% , 29% , 55%\n",
    "## https://coderzcolumn.com/tutorials/machine-learning/model-evaluation-scoring-metrics-scikit-learn-sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Confusion Matrix of Test Dataset: \")\n",
    "\n",
    "##print(confusion_matrix(y_train, find_best_probability(y_train_pred, train_thresholds, train_fpr, train_tpr)))\n",
    "\n",
    "confusion_metric = metrics.confusion_matrix(y_test ,find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr))\n",
    "## Fir heat maphttps://stackoverflow.com/questions/61748441/how-to-fix-the-values-displayed-in-a-confusion-matrix-in-exponential-form-to-nor\n",
    "## If annot is True data value is written inside each cell and cmao is used for colour coordination of the confusion matrix \n",
    "sns.heatmap(confusion_metric , annot=True , fmt='d' , cmap = 'Greens')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actal Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2Ob9qZcrjhj"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "After plotting confusion matrix on test data we observe that:\n",
    "1. There are 3193 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-YjGIRfrjhj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ncdd8Zorjhk"
   },
   "source": [
    "# 12. Find all the false positive data points and plot wordcloud of essay text and pdf of teacher_number_of_previously_posted_projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd0Z6hxgrjhk"
   },
   "source": [
    "## 12.1 - Finding all the false positive data points and creating a dataframe of these false positive words ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxN-zYorrjhk"
   },
   "outputs": [],
   "source": [
    "## https://datascience.stackexchange.com/questions/97499/viewing-false-positive-rows-in-python\n",
    "predict = find_best_probability(y_test_pred, test_thresholds, test_fpr, test_tpr)\n",
    "false_positive = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if (predict[i] == 1) and (y_test[i] == 0):\n",
    "        false_positive.append(i)\n",
    "len(false_positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7vAOCitrjhk"
   },
   "source": [
    "## to find the false positive values what we are doing is we are getting all the values in the test data where our actual value is 0 and our predicted value is 1 , so we got around 2266 values ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1664013580490,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "Ck4zNcH0rjhk",
    "outputId": "ee82dd3f-73f9-4557-e7f0-f54cc90cc038"
   },
   "outputs": [],
   "source": [
    "coloumn = x_test.columns    ## getting the columns of x_test data\n",
    "x_test_false_positive = pd.DataFrame(columns = coloumn)    ##creating a dataset\n",
    "x_test_false_positive = x_test.iloc[false_positive]     ## inserting false positive values\n",
    "x_test_false_positive  ## showing the values   \n",
    "print(x_test_false_positive.shape)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmjj0VbQrjhk"
   },
   "source": [
    "## In above code we have created a dataframe which contain all the false positive values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uH-YPIp7rjhk"
   },
   "source": [
    "## 12.2 Plotting the WordCloud(https://www.geeksforgeeks.org/generating-word-cloud-python/) with the words of essay text of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEEjDf1Mrjhk"
   },
   "source": [
    "## Our next task is to plot the wordCloud ,\n",
    "We all know Word Cloud is a data visualization technique that is used for representing **text data in which the size of each word indicates its frequency or importance.** Significant textual data points can be highlighted using a word cloud.\n",
    "\n",
    "So for plotting the word cloud there are some basic steps to be followed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4CpSaiKrjhk"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    " \n",
    "# iterate through the csv file\n",
    "for val in x_test_false_positive['essay']:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    val = str(val)\n",
    " \n",
    "    # split the value\n",
    "    tokens = val.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzAnwxx8rjhk"
   },
   "source": [
    "## Conclusion and Observation :\n",
    "We can see that the most important words in our essay are **student** , **classroom** , **school** and there are many other words .\n",
    "\n",
    "If we observe carefully we can see that the important words are coloured with dark colors and as the importance of the word decreases the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18cUS3lMrjhl"
   },
   "source": [
    "\n",
    "\n",
    "## 12.2 Plot the box plot with the `price` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apEMDJDlrjhl"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot('price',  data=x_test_false_positive)\n",
    "plt.title(\"Box Plot of 'price' on false positive data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vZ1Tufgrjhl"
   },
   "source": [
    "## 12.3 Plot the pdf with the `teacher_number_of_previously_posted_projects` of these `false positive data points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1664013588297,
     "user": {
      "displayName": "Shrishti Pandey",
      "userId": "10041048004831906322"
     },
     "user_tz": -330
    },
    "id": "303Vd2Tlrjhl",
    "outputId": "e57ee659-82eb-4581-909e-0a935c577cf9"
   },
   "outputs": [],
   "source": [
    "# initializing random values\n",
    "data = x_test_false_positive['teacher_number_of_previously_posted_projects']\n",
    "  \n",
    "# getting data of the histogram\n",
    "count, bins_count = np.histogram(data, bins=10)\n",
    "  \n",
    "# finding the PDF of the histogram using count values\n",
    "pdf = count / sum(count)\n",
    "\n",
    "# printing the value of pdf and bins\n",
    "print(pdf)  \n",
    "print(\"\\n\")\n",
    "print(bins_count)\n",
    "print(\"\\n\")\n",
    "\n",
    "# plotting PDF\n",
    "plt.plot(bins_count[1:], pdf, color=\"red\")\n",
    "plt.xlabel('teacher_number_of_previously_posted_projects')\n",
    "\n",
    "plt.title('PDF of false positive data points and teacher_number_of_previously_posted_projects')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ng_chEmYrjhl"
   },
   "source": [
    "\n",
    "# 13. Write your observations about the wordcloud and pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqEyrfxXrjhl"
   },
   "source": [
    "## Observation for WordCloud :\n",
    "In case of wordcloud we saw that as the importance of the word was increasing the colour of the word was highliting or we can say that the word was highlighting as the importance of that word was increasing .\n",
    "\n",
    "## Observation of PDF of false positive data points and teacher_number_of_previously_posted_projects\n",
    "\n",
    "In case of PDF graph we observe that there is a sudden drop in te graph at 0.05 as the false positive rate and 50 as the teacher_number_of_previously_posted_projects after that there is a slight slope which then converted to contat value 0 (false positive data) as the teacher_number_of previously_posted_projects increase .\n",
    "This means till the teacher submitted first 50 projects there were false positive or incorrect classification but after 50 values , value of false positive data changed slightly and after 100 projects were submitted there were no false positive value ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
